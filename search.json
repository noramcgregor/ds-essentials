[
  {
    "objectID": "atr.html",
    "href": "atr.html",
    "title": "Automatic Text Recognition (OCR/HTR)",
    "section": "",
    "text": "Cultural heritage organisations have been digitising their historical collections for several decades, outputting large-scale sets of digitised collection items - printed books, newspapers, manuscripts, maps, and many other content types - as simple JPG, PNG, or TIFF images. Though these collections are made available for users to read online, there is often an important layer of accessibility that is missing as the text appearing in these images is not always made searchable, editable, or analysable.\nAutomatic Text Recognition (ATR) refers to the process of using software to convert images of text, such as scanned or photographed documents, photos, or printed pages, into machine-readable text. ATR is primarily associated with Optical Character Recognition (OCR) (for print materials) and Handwritten Text Recognition (HTR) technologies (for handwritten materials) that enable computers to identify and extract text from various sources. These technologies are critical for digitising historical and cultural documents, making them searchable, accessible, and preservable for future generations.\nThough ATR technologies have been around in some form or another for nearly as long as we’ve been digitising, results can be patchy for cultural heritage collections. This may be down to costs as these technologies are typically considered an add-on to the basic photographic or scanning services and institutions or projects don’t have the funds available to cover that aspect of digitisation. Or, the traditional OCR technology at any given time simply isn’t developed enough to provide very accurate results to handle the anomalies of, for instance, quite worn or warped manuscript pages. Or, in the case of handwritten texts, particularly low-resource languages, the existing technology simply cannot cope and text outputs are illegible. The advent of machine learning technologies applied to the challenge of Automatic Text Recognition of handwritten or print materials, and with it, the ability to train a software to recognise specific text based on what it has already seen, however, provides huge opportunities to transform accessibility to the text contained within all manner of historical digitised images.\nSo what is the difference between the decades-old OCR technology and the more recent, AI/Machine Learning-based software? Traditional OCR systems rely on a set of predefined rules and templates to recognise characters. These rules are often based on the geometric properties of the text, such as the shapes and patterns of characters. On the other hand, Machine Learning-based OCR, or in other words, Automatic Text Recognition that leverages Artificial Intelligence, uses neural networks to learn to recognise characters from large datasets. These systems can be trained to automatically learn to identify the features that distinguish different characters and predict with some probability what that character could be. AI-OCR can handle a wide variety of fonts, sizes, and styles, and is robust to noisy, distorted, or low-quality images. The models can generalise well to different types of documents and text layouts; and they can continuously improve by retraining on new data, which allows them to adapt to new types of text and writing styles over time.\nWhen diving deeper into AI-OCR, you may encounter the term ‘Ground Truth’. Ground Truth refers to the accurate, manually created and verified data used as a standard or benchmark to both train and evaluate the performance of OCR/HTR systems. Ground truth data consists of the correct transcription of a document, including precise details about characters, words, and layout. It is used to ‘teach’ software how to recognise and transcribe text, as well as measure the accuracy of OCR/HTR outputs by comparing the machine-recognised text against this gold-standard reference.\nAutomatic Text Recognition typically involves multiple stages or elements, which include image pre-processing, binarisation, layout analysis, and text recognition.\n\nImage Pre-processing: Pre-processing is the first step and focuses on enhancing the quality of the input image to improve recognition accuracy. This may include tasks such as noise reduction, skew correction (to align slanted images), and contrast adjustment. These operations are crucial when dealing with old or degraded documents, where imperfections in the image can hinder the subsequent stages.\nBinarisation: Binarisation converts a grayscale or colour image into a binary image, typically using black for the text and white for the background. This simplifies the image, making it easier for the OCR/HTR system to distinguish between the text and non-text elements. This step is especially important for historical documents, where stains, fading, or other degradation can confuse the OCR/HTR engine.\n\n Examples of binarisation and its effects on legibility, created by Peter Smith who worked to improve Chinese HTR processes\n\nLayout Analysis: In layout analysis, the system identifies the structure of the document, distinguishing between various elements such as paragraphs, columns, headings, footnotes, and images. It is essential for documents with complex formatting (e.g. newspapers or tables) to ensure that the text is correctly segmented and processed. This step may also involve detecting text regions in multi-layout documents or distinguishing between handwritten and printed text.\nText Recognition: The core of the OCR/HTR process is text recognition, where the system identifies individual characters, words, and sentences within the segmented text regions. Modern OCR/HTR engines use pattern recognition techniques or machine learning algorithms, such as neural networks, to improve accuracy. Some systems also perform language modelling, where the recognised text is checked against a dictionary or corpus to ensure contextual correctness, particularly useful for older languages or scripts.\nPost-Processing: OCR/HTR often produces errors, especially when dealing with degraded, old, or complex manuscripts. To improve accuracy, post-processing correction is crucial. One effective method for refining OCR/HTR outputs is crowdsourcing text correction, where volunteers, often through online platforms, manually review and correct transcribed text. This method leverages the knowledge and dedication of the public to handle the nuances of historical documents that automated systems struggle with, such as obscure spellings or unusual handwriting styles. Recent experiments in automating text recognition have leveraged Large Language Models (LLMs) to enhance the accuracy of OCR/HTR systems. LLMs, such as GPT-based models, are particularly adept at understanding context and handling ambiguities in textual data, which makes them valuable for recognising and correcting errors in historical documents. LLMs can also be fine-tuned to specific historical corpora, allowing them to better interpret unique vocabulary, syntax, or stylistic variations. However, it is important to be cautious, as LLMs are prone to hallucination - generating plausible but inaccurate text - which can introduce new errors during automated corrections. And, it should be mentioned that once you have perfectly post-processed OCR/HTR results, you can use those as Ground Truth to retrain models and improve them!",
    "crumbs": [
      "Topic Guides",
      "Automatic Text Recognition (OCR/HTR)"
    ]
  },
  {
    "objectID": "atr.html#introduction",
    "href": "atr.html#introduction",
    "title": "Automatic Text Recognition (OCR/HTR)",
    "section": "",
    "text": "Cultural heritage organisations have been digitising their historical collections for several decades, outputting large-scale sets of digitised collection items - printed books, newspapers, manuscripts, maps, and many other content types - as simple JPG, PNG, or TIFF images. Though these collections are made available for users to read online, there is often an important layer of accessibility that is missing as the text appearing in these images is not always made searchable, editable, or analysable.\nAutomatic Text Recognition (ATR) refers to the process of using software to convert images of text, such as scanned or photographed documents, photos, or printed pages, into machine-readable text. ATR is primarily associated with Optical Character Recognition (OCR) (for print materials) and Handwritten Text Recognition (HTR) technologies (for handwritten materials) that enable computers to identify and extract text from various sources. These technologies are critical for digitising historical and cultural documents, making them searchable, accessible, and preservable for future generations.\nThough ATR technologies have been around in some form or another for nearly as long as we’ve been digitising, results can be patchy for cultural heritage collections. This may be down to costs as these technologies are typically considered an add-on to the basic photographic or scanning services and institutions or projects don’t have the funds available to cover that aspect of digitisation. Or, the traditional OCR technology at any given time simply isn’t developed enough to provide very accurate results to handle the anomalies of, for instance, quite worn or warped manuscript pages. Or, in the case of handwritten texts, particularly low-resource languages, the existing technology simply cannot cope and text outputs are illegible. The advent of machine learning technologies applied to the challenge of Automatic Text Recognition of handwritten or print materials, and with it, the ability to train a software to recognise specific text based on what it has already seen, however, provides huge opportunities to transform accessibility to the text contained within all manner of historical digitised images.\nSo what is the difference between the decades-old OCR technology and the more recent, AI/Machine Learning-based software? Traditional OCR systems rely on a set of predefined rules and templates to recognise characters. These rules are often based on the geometric properties of the text, such as the shapes and patterns of characters. On the other hand, Machine Learning-based OCR, or in other words, Automatic Text Recognition that leverages Artificial Intelligence, uses neural networks to learn to recognise characters from large datasets. These systems can be trained to automatically learn to identify the features that distinguish different characters and predict with some probability what that character could be. AI-OCR can handle a wide variety of fonts, sizes, and styles, and is robust to noisy, distorted, or low-quality images. The models can generalise well to different types of documents and text layouts; and they can continuously improve by retraining on new data, which allows them to adapt to new types of text and writing styles over time.\nWhen diving deeper into AI-OCR, you may encounter the term ‘Ground Truth’. Ground Truth refers to the accurate, manually created and verified data used as a standard or benchmark to both train and evaluate the performance of OCR/HTR systems. Ground truth data consists of the correct transcription of a document, including precise details about characters, words, and layout. It is used to ‘teach’ software how to recognise and transcribe text, as well as measure the accuracy of OCR/HTR outputs by comparing the machine-recognised text against this gold-standard reference.\nAutomatic Text Recognition typically involves multiple stages or elements, which include image pre-processing, binarisation, layout analysis, and text recognition.\n\nImage Pre-processing: Pre-processing is the first step and focuses on enhancing the quality of the input image to improve recognition accuracy. This may include tasks such as noise reduction, skew correction (to align slanted images), and contrast adjustment. These operations are crucial when dealing with old or degraded documents, where imperfections in the image can hinder the subsequent stages.\nBinarisation: Binarisation converts a grayscale or colour image into a binary image, typically using black for the text and white for the background. This simplifies the image, making it easier for the OCR/HTR system to distinguish between the text and non-text elements. This step is especially important for historical documents, where stains, fading, or other degradation can confuse the OCR/HTR engine.\n\n Examples of binarisation and its effects on legibility, created by Peter Smith who worked to improve Chinese HTR processes\n\nLayout Analysis: In layout analysis, the system identifies the structure of the document, distinguishing between various elements such as paragraphs, columns, headings, footnotes, and images. It is essential for documents with complex formatting (e.g. newspapers or tables) to ensure that the text is correctly segmented and processed. This step may also involve detecting text regions in multi-layout documents or distinguishing between handwritten and printed text.\nText Recognition: The core of the OCR/HTR process is text recognition, where the system identifies individual characters, words, and sentences within the segmented text regions. Modern OCR/HTR engines use pattern recognition techniques or machine learning algorithms, such as neural networks, to improve accuracy. Some systems also perform language modelling, where the recognised text is checked against a dictionary or corpus to ensure contextual correctness, particularly useful for older languages or scripts.\nPost-Processing: OCR/HTR often produces errors, especially when dealing with degraded, old, or complex manuscripts. To improve accuracy, post-processing correction is crucial. One effective method for refining OCR/HTR outputs is crowdsourcing text correction, where volunteers, often through online platforms, manually review and correct transcribed text. This method leverages the knowledge and dedication of the public to handle the nuances of historical documents that automated systems struggle with, such as obscure spellings or unusual handwriting styles. Recent experiments in automating text recognition have leveraged Large Language Models (LLMs) to enhance the accuracy of OCR/HTR systems. LLMs, such as GPT-based models, are particularly adept at understanding context and handling ambiguities in textual data, which makes them valuable for recognising and correcting errors in historical documents. LLMs can also be fine-tuned to specific historical corpora, allowing them to better interpret unique vocabulary, syntax, or stylistic variations. However, it is important to be cautious, as LLMs are prone to hallucination - generating plausible but inaccurate text - which can introduce new errors during automated corrections. And, it should be mentioned that once you have perfectly post-processed OCR/HTR results, you can use those as Ground Truth to retrain models and improve them!",
    "crumbs": [
      "Topic Guides",
      "Automatic Text Recognition (OCR/HTR)"
    ]
  },
  {
    "objectID": "atr.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "atr.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Automatic Text Recognition (OCR/HTR)",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nKeeping up with, and even contributing to, developments in Automatic Text Recognition technology is vital for heritage organisations and especially for the library sector, where large volumes of historical and cultural documents are preserved. ATR technologies provide several key benefits:\n\nAccessibility: ATR technologies are essential for libraries working to make their text content available for users. Once digitised and converted into searchable text, documents become accessible to a global audience. Researchers and the general public can search and access texts remotely, making previously hidden information available. Creating searchable documents allows users to quickly locate specific terms or phrases within vast collections. This enhances the research process and saves time.\nContent Enrichment: ATR can help enrich library records by enhancing their metadata; therefore assisting libraries in delivering a better service to their users. Different entities could be extracted from the text, such as author or place of publication, as well as subjects and descriptions. These could be used to enhance catalogue records for the benefit of library users.\nDigital Research: By converting historical texts into machine-readable formats, ATR technologies support digital humanities projects, where large-scale analysis, such as text mining or linguistic research, can uncover new insights into history, culture, and language development. The ability to extract text from digitised items is fundamental for any downstream tasks and enables unlocking content for large-scale analysis, e.g. text mining, Natural language processing (NLP), Named Entity Recognition (NER), sentiment analysis or topic modelling.​\n\n\nCase Studies\nIn collaboration with PRImA Research Lab, the British Library ran several OCR/HTR competitions with the aim of encouraging the development of state-of-the-art in text recognition software, facilitating dialogue around the challenges and opportunities of ATR, and creating openly licensed ground truth datasets. Competitions around early Bengali books and Quarterly Lists were run as part of the Two Centuries of Indian Print project, and the project used Transkribus to create OCR transcriptions for the Bengali books, in collaboration with the School of Cultural Texts and Records at Jadavpur University. Additional competitions were focused on finding solutions to automatically transcribe historical Arabic scientific manuscripts, using materials digitised by the Library and published on the Qatar Digital Library (QDL).\n Screenshot from Transkribus, used as the ATR engine in the Two Centuries of Indian Print project\nThe Open Islamicate Texts Initiative (OpenITI) is a collaborative project involving researchers from Aga Khan University’s Institute for the Study of Muslim Civilisations in London, the Roshan Institute for Persian Studies at the University of Maryland, College Park, and Universität Hamburg. Its goal is to establish the digital infrastructure needed for the study of Islamicate cultures. OpenITI focuses on building digital resources for Islamicate studies by enhancing optical character recognition (OCR) and handwritten text recognition (HTR) for Arabic-script texts, creating standardised OCR and HTR outputs and text encoding, and developing platforms for collaborative work on Islamicate text corpora and digital editions.\nThe Legacies of Curatorial Voice in the Descriptions of Incunabula Collections at the British Library project was part of Digital Curator Dr Rossitza Atanassova’s AHRC-RLUK funded Professional Practice Fellowship Project (2022–23). It aimed to explore innovative methods for working with digitised catalogues, enhancing the discoverability and usability of the collections they document. The research centred on the Catalogue of Books Printed in the 15th Century Now at the British Museum (BMC), published between 1908 and 2007, which describes over 12,700 volumes in the British Library’s incunabula collection. It used Transkribus as the ATR software and also as a search and publishing web platform. The project could then apply computational techniques and corpus analysis of the catalogue data thanks to the availability of OCRed text and provides fresh insights into this resource!",
    "crumbs": [
      "Topic Guides",
      "Automatic Text Recognition (OCR/HTR)"
    ]
  },
  {
    "objectID": "atr.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "atr.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Automatic Text Recognition (OCR/HTR)",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nHere are some excellent resources to get you started:\nAutomatic Text Recognition: Harmonising ATR workflows: This DARIAH-EU-supported resource is a great place to start. This website features a set of video tutorials on ATR in English, French and German, and also includes really useful blog posts, articles and links.\nCheck out the Transkribus YouTube channel for a set of beginner-friendly tutorial videos, learning how to get started with Transkribus to effectively digitise and preserve historical documents.\nIf you’re interested in eScriptorium, have a look at their thorough documentation and tutorials on YouTube created by the OpenITI project. These are split into five parts: Part I, Part II, Part III, Part IV, and Part V.\nThis step-by-step guide will teach you how to use the Tesseract open-source software, and it also recommends some software that helps prepare documents for Tesseract use.\nFor more advanced practitioners, there’s an introductory course by Dr William Mattingly teaching you how to automate OCR in Python. It includes using OpenCV (an open-source library specialising in computer vision and machine learning tasks) and Pytesseract, an OCR tool in Python. Helpfully, this course functions alongside a YouTube series of tutorials on OCR in Python.\nAnother one for the more confident practitioners is this Programming Historian OCR with Google Vision API and Tesseract tutorial by Isabelle Gribomont, published in March 2023.\nThis Programming Historian OCR and Machine Translation course by Andrew Akhlaghi (published in January 2021) uses Tesseract for OCR and takes the results to another level - translation.",
    "crumbs": [
      "Topic Guides",
      "Automatic Text Recognition (OCR/HTR)"
    ]
  },
  {
    "objectID": "atr.html#recommended-readingviewing",
    "href": "atr.html#recommended-readingviewing",
    "title": "Automatic Text Recognition (OCR/HTR)",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nThis is a great blog post by Chris Woodform, simply entitled Optical character recognition (OCR). It’s a good place to start! It explains what OCR is and how it works, and even looks at the history of this technology.\nAwesome OCR is a resource created by Konstantin Baierer from the OCR-D project. It hasn’t been updated for a while, however, it includes really useful and comprehensive lists of software tools, libraries and literature. It also includes a list of ground truth datasets, so well worth taking a look.\nThe IMPACT Centre of Competence  is a useful OCR resource. In their words, IMPACT is a ‘not for profit organisation with the mission to make the digitisation of text “better, faster, cheaper” and to further advance the state-of-the-art in the field of document imaging, language technology and the processing of historical text.’ You can find datasets, training materials, blogs and more!\n\n\n\nAn Introduction to Ground Truth Production with the IMPACT Project\n\n\nAn excellent guide to newspaper OCR and data analysis is this Short Guide to Historical Newspaper Data, Using R by Yann Ryan.\nMany excellent data analysis tools such as NLP can also be found on this GitHub page, which collated materials used for the Text to Tech workshop at the Digital Humanities Oxford Summer School, by Kaspar von Beelen, Mariona Coll Ardanuy and Federico Nanni (and this is another brilliant introduction into NLP!).\nThere are many papers on OCR/HTR but here’s a small selection:\n\nKhan, R., Gupta, N., Sinhababu, A., & Chakravarty, R. (2023). Impact of Conversational and Generative AI Systems on Libraries: A Use Case Large Language Model (LLM). Science & Technology Libraries, 1–15. https://doi.org/10.1080/0194262X.2023.2254814\nNeudecker, C., Baierer, K., Federbusch, M., Boenig, M., Würzner, K. M., Hartmann, V., & Herrmann, E. (2019). OCR-D: An end-to-end open source OCR framework for historical printed documents. In Proceedings of the 3rd international conference on digital access to textual cultural heritage, 53-58. https://dl.acm.org/doi/10.1145/3322905.3322917\nNguyen, T.T.H., Jatowt, A., Coustaty, M., & Doucet, A. (2021). Survey of Post-OCR Processing Approaches. ACM Computing Surveys (CSUR), 54(6), 1-37. https://doi.org/10.1145/3453476\nSmith, D.A. & Cordell, R. (2018). A Research Agenda for Historical and Multilingual Optical Character Recognition.\nVan Strien, D., Beelen, K., Ardanuy, M., Hosseini, K., McGillivray, B., & Colavizza, G. (2020). Assessing the impact of OCR quality on downstream NLP tasks. SCITEPRESS - Science and Technology Publications. https://doi.org/10.17863/CAM.52068\n\nSome journals to look out for include, for example, the International Journal on Document Analysis and Recognition (IJDAR), Pattern Recognition, or the IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI).\nThe team behind the Hypotheses ‘Automatic Text Recognition: Harmonising ATR workflows’ resource created this insightful road map to help you get started with Automatic Text Recognition. The road map is divided into three sections, and asks you to consider the following questions:\nI - General Information\n\nWhat type of texts/text collections do you work with?\nHow can you integrate Automated Text Recognition in your workflow?\n\nII - Technical information\n\nWhy do you want to use ATR?\nWhat is your objective?\nDo you have technical and financial resources?\n\nIII - Setting up ATR in your project\n\nChoose your tool\nIs there transcription data that can be reused (e.g. training data)?\nIs there an appropriate generic model?\nWhat are your transcription rules?\nDo you plan to share your ATR training data? What are your ATR predictions?\nWhere can you share data? In which format?\n\nThis document also includes links to tutorials and documentation, a selection of articles, ATR tools and where to find ATR ground truth datasets.\nWhen it comes to ground truth datasets, a good place to look is the HTR-United resource, which includes training datasets used for both transcription or segmentation models, for different periods and styles of writing.\nOCR-D is a good place to visit too, and it has a ground truth repository available here.",
    "crumbs": [
      "Topic Guides",
      "Automatic Text Recognition (OCR/HTR)"
    ]
  },
  {
    "objectID": "atr.html#finding-communities-of-practice",
    "href": "atr.html#finding-communities-of-practice",
    "title": "Automatic Text Recognition (OCR/HTR)",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nSome tools and platforms have solid communities around them which you can get in touch with - or be a part of. Transkribus, for example, has an active Facebook page and a user group, where Transkribus users can ask questions and get support from the Transkribus team and from each other.\nSeveral conference series discuss advances in OCR/HTR, and would be a good place to meet colleagues and hear about interesting ATR projects. For example, the International Conference on Document Analysis and Recognition (ICDAR) and the International Conference on Frontiers in Handwriting Recognition (ICFHR) have been well established. Alternatively, many Digital Humanities conferences will have papers on this topic, e.g. the ADHO DH conference serie or AI4LAM Fantastic Futures conferences.",
    "crumbs": [
      "Topic Guides",
      "Automatic Text Recognition (OCR/HTR)"
    ]
  },
  {
    "objectID": "openresearch.html",
    "href": "openresearch.html",
    "title": "Supporting Open Research (Open Science)",
    "section": "",
    "text": "Open Research - used interchangeably with Open Science - is an umbrella term that sets values and practises to make research easily discoverable, transparent, equitable, available and re-usable across all disciplines and across the entire research lifecycle. It aims to remove barriers to knowledge and make access as inclusive as possible.\nThe aim of Open Research is:\n\nTo promote a collaborative, sustainable and inclusive research culture\nTo incentivise innovation and creativity\nTo support reproducibility of research outputs (where relevant)\n\n\n\nUniversities and researchers are recognising that Open Research has benefits extending far beyond each individual institution. The Open Research Group at University of Cambridge articulates this nicely in their guidance to researchers:\n\nThe general public has free access to quality information that matters in their lives\nPractitioners and policy makers can put the findings of research into practice more quickly and easily\nPublic funds result in knowledge that can be shared as a public good\nEconomic benefits derive from reducing attrition between research and commercial applications\nStudents in a variety of context face no barriers to accessing materials that help them\nAccess to knowledge is more equitably distributed around the world\nFindings from research are more transparent and trustworthy\n\n\n\n\nThe Open Research Group at University of Cambridge note that individual researchers also benefit from sharing their research:\n\nMore visibility as outputs are not restricted by paywalls and other barriers\nGreater impact as more people read and apply their work\nMore credibility through making the process of research more transparent\nCompliance with funders’ requirements and career opportunities\n\nOpen Research is more than just making research findings public. It’s a philosophy that encourages transparency, accessibility, and collaboration throughout the entire research process.\n\n\n\nFunders, institutions, research groups and individual researchers from around the world are actively working to overcome technical, cultural, ethical, legal and financial challenges to make open research the norm. This includes developing robust infrastructure, fostering a culture that values openness, ensuring ethical practices, clarifying legal frameworks, and establishing sustainable funding models.",
    "crumbs": [
      "Topic Guides",
      "Supporting Open Research (Open Science)"
    ]
  },
  {
    "objectID": "openresearch.html#introduction",
    "href": "openresearch.html#introduction",
    "title": "Supporting Open Research (Open Science)",
    "section": "",
    "text": "Open Research - used interchangeably with Open Science - is an umbrella term that sets values and practises to make research easily discoverable, transparent, equitable, available and re-usable across all disciplines and across the entire research lifecycle. It aims to remove barriers to knowledge and make access as inclusive as possible.\nThe aim of Open Research is:\n\nTo promote a collaborative, sustainable and inclusive research culture\nTo incentivise innovation and creativity\nTo support reproducibility of research outputs (where relevant)\n\n\n\nUniversities and researchers are recognising that Open Research has benefits extending far beyond each individual institution. The Open Research Group at University of Cambridge articulates this nicely in their guidance to researchers:\n\nThe general public has free access to quality information that matters in their lives\nPractitioners and policy makers can put the findings of research into practice more quickly and easily\nPublic funds result in knowledge that can be shared as a public good\nEconomic benefits derive from reducing attrition between research and commercial applications\nStudents in a variety of context face no barriers to accessing materials that help them\nAccess to knowledge is more equitably distributed around the world\nFindings from research are more transparent and trustworthy\n\n\n\n\nThe Open Research Group at University of Cambridge note that individual researchers also benefit from sharing their research:\n\nMore visibility as outputs are not restricted by paywalls and other barriers\nGreater impact as more people read and apply their work\nMore credibility through making the process of research more transparent\nCompliance with funders’ requirements and career opportunities\n\nOpen Research is more than just making research findings public. It’s a philosophy that encourages transparency, accessibility, and collaboration throughout the entire research process.\n\n\n\nFunders, institutions, research groups and individual researchers from around the world are actively working to overcome technical, cultural, ethical, legal and financial challenges to make open research the norm. This includes developing robust infrastructure, fostering a culture that values openness, ensuring ethical practices, clarifying legal frameworks, and establishing sustainable funding models.",
    "crumbs": [
      "Topic Guides",
      "Supporting Open Research (Open Science)"
    ]
  },
  {
    "objectID": "openresearch.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "openresearch.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Supporting Open Research (Open Science)",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nThe role of libraries on Open Research has been discussed for a while and endorsed publicly by international organisations and stakeholders such as the European commission (European Commission, 2012) and OECD (OECD, 2015).\nOECD defines libraries as enablers “Libraries have adapted their role and are now active in the preservation, curation, publication and dissemination of digital scientific materials, in the form of publications, data and other research-related content. Libraries and repositories constitute the physical infrastructure that allows scientists to share use and reuse the outcome of their work, and they have been essential in the creation of the Open Science movement”\nLibrary staff have an important role in encouraging their institution to make open research a priority, and deliver practical support to their researchers.\nLIBER’s Open Science Roadmap produced by the LIBER Open Access Working Group is an exceptional resource and outlines the “specific actions libraries can take to champion Open Science, both within and beyond their own institutions”. It provides specific recommendations on how libraries can (and must) work across many different areas to fully advocate, raise awareness of and support Open Science such as:\n\nScholarly Publishing\nFAIR Data\nResearch Infrastructure & the EOSC\nOpen Science Skills\n\nThe Roadmap also provides a number of helpful case studies, well worth a read, of champions in the Open Research space:\n\nKarlsruhe Institute of Technology\nNational Library of Finland\nRuder Bošković Institute Library\nSpanish National Research Council\nSvetozar Markovic University Library\nUniversity of Barcelona\nUniversity College London\nUniversity Library of Southern Denmark\n\nThese can be a good source of inspiration and are concrete examples to help your institution practically support Open Research.\nOne of the most important ways librarians can and do support Open Research is through adopting, promoting and supporting Fair Data Principles.\n\nFindable: Research data, software and publications should be easy to discover using clear and consistent identification methods.\nAccessible: Data, software and publications should be readily available to anyone with minimal barriers, often through open access repositories.\nInteroperable: Data should be presented in a standardised format that allows for seamless integration and analysis with other datasets.\nReusable: Data, software and publications should be accompanied by clear documentation and licensing, allowing others to understand and build upon them.\n\nLIBER’s Research Data Management Working Group produced a helpful factsheet, Implementing FAIR Data Principles: The Role of Libraries, specifically to help libraries understand how to get started incorporating the FAIR Data Principles in their work:\n\nPromote the FAIR principles to local research and IT staff;\nIncorporate the FAIR principles in your Data Management Plans and your digital preservation practices and policies;\nSeek opportunities to curate, enrich, capture and preserve research data that will aid in making data findable, accessible, interoperable and reusable. Good starting points are collections of individual researchers, or a data collection of a research group;\nTrain subject and data librarians on disciplinary metadata, vocabularies and tools to make data FAIR;\nEncourage researchers to deposit data with archives that embody the FAIR principles;\nEvaluate the data collections and data management practices at your institution against the FAIR principles.\n\nHere are a few more practical suggestions for ways in which librarians can support the adoption of Open Research principals at their insitutions:\n1. Make it easier for staff and researchers to find all the info they need about Open Research\nYour institution should have a central place for providing information to staff and researchers about embarking on open research, and where to seek practical support. If it doesn’t your library website can be a natural home for such information!\n2. Provide practical support for researchers\nResearchers may be keen to adopt open research principals, but without a lack of institutional support and practical training they may be more easily deterred. Librarians have an important role in encouraging their institution to make open research a priority, and deliver practical support to their researchers.\nProviding advice and training to support researchers across the research life cycle is a key way for librarians to support open research.\nPlanning a Research Project\n\nExploring pre-registered studies and protocols\nUsing existing open datasets to inspire new research questions and applications.\nOpen Peer Review practices\nChoosing the right data repository\nDevelop a Research Data Management plan and outlining data management plans for during and after the project\n\nDuring the Research Project\n\nDeveloping a pre-registration document\nExploring the use of Open Code and Software\nHow best to document data collection process (The Turing Way: A handbook for reproducible, ethical and collaborative research: open notebooks)\nHow to share preliminary results through the use of Preprints\nHelp researchers to manage their personal identifiers (ORCID\n\nAfter the end of the Project\n\nHow to share results in an open and suitable long-term format (e.g. UK Data Service: Recommended File Formats & Library of Congress Recommended Formats)\nHow to upload (anonymised if appropriate) research data to a trusted open access data repository\nHow to Publish research findings in open access journals and/or deposit them in open access repositories with a permissive reuse licence\n\n3. Work to understand the barriers The blogpost “How can librarians support open research?” recommends librarians work closely with researchers to understand their specific barriers to publishing openly and share data more easily, and collaborate with other faculties to understand if their researchers are facing the same challenges. This collaborative communication could help find the right solutions for your researchers’ needs.\n4. Make use of bibliometrics Another way to help encourage the adoption of Open Research practices is to use bibliometrics to incentivise open practices. The authors of “How can librarians support open research?” notes that colleagues who work with bibliometrics/citation analysis can help develop their institution’s understanding of the impact of their research and shared data, especially if more outputs across the research lifecycle are being openly published. Library can help institutions to recognise, showcase, and reward high impact data sets which have been shared openly and reused by other researchers.\n5. Contribute to and/or create an institution platform Library staff are often responsible for maintaining and enhancing the records deposited on the institutional or national repository, so having a full understanding of open research principals will be essential in some roles.\nSome institutions may find that creating institutional platforms, such as a repository capable of hosting (and making open) data sets and other useful resources to share across faculties can drive open research at your institution. It is important though to be aware though that such repositories may often come with some limitations which means they should only be considered as part of a wider plan for supporting open research.",
    "crumbs": [
      "Topic Guides",
      "Supporting Open Research (Open Science)"
    ]
  },
  {
    "objectID": "openresearch.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "openresearch.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Supporting Open Research (Open Science)",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nSkills4EOSC is an EU funded project that started in 2022 and offers “a comprehensive training program designed to equip researchers, data stewards, and other stakeholders with essential skills for navigating the evolving landscape of Open Science and the European Open Science Cloud (EOSC)”“. Some training is asynchronous and you can enroll yourself at any time such as Learning path for (data) librarians: Technical skills are the bridge to reproducible research.\nThe Foster Open Science Project produced a number of training materials which are helpful for librarians wanting to get started in supporting open research.\nThough aimed at researchers in biological and biomedical sciences, this Beta Data Carpentries lesson provides a really accessible introduction for anyone interested in FAIR (Findable, Accessible, Interoperable, Reusable) principles for data re-use, and how to practically apply then throughout a projects’ life cycle.",
    "crumbs": [
      "Topic Guides",
      "Supporting Open Research (Open Science)"
    ]
  },
  {
    "objectID": "openresearch.html#recommended-readingviewing",
    "href": "openresearch.html#recommended-readingviewing",
    "title": "Supporting Open Research (Open Science)",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nAgain, we can’t recommend enough LIBER’s Open Science Roadmap produced by the LIBER Open Access Working Group as an all around guide for libraries getting started in Open Research.\nUKRN Open Research Resources is also an excellent resource for getting to grips with supporting open research. They provide helpful animations, primers, videos and training materials as well as discipline specific case studies of open research including Library and Information Science related examples.\nUNESCO Recommendation on Open Science (2021)\nThe Community Sourced Open Research glossary is a good place to start to find out more about the terminology connected to Open Research, which can be sometimes overwhelming and confusing\nIf you prefer podcasts, we can recommend:\n\nEverything Hertz (in particular podcast 57 & 176) & Orion Open Science will give you a perspective or Open research from different fields\nThe first few episodes of ReproducibilitiTea are an excellent introduction to Open research",
    "crumbs": [
      "Topic Guides",
      "Supporting Open Research (Open Science)"
    ]
  },
  {
    "objectID": "openresearch.html#finding-communities-of-practice",
    "href": "openresearch.html#finding-communities-of-practice",
    "title": "Supporting Open Research (Open Science)",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nLearning about Open Research can be overwhelming at first especially if you are new to it! LIBER’s Open Access Working Group is a great community to join as they are playing an important role in the Open Research Europe (ORE) Project - LIBER Europe and have many members well versed in this topic!",
    "crumbs": [
      "Topic Guides",
      "Supporting Open Research (Open Science)"
    ]
  },
  {
    "objectID": "networks.html",
    "href": "networks.html",
    "title": "USEFUL NETWORKS",
    "section": "",
    "text": "Working groups are open to staff at participating LIBER Member institutions:\n\nLIBER Data Science in Libraries\nLIBER Digital Scholarship & Digital Cultural Heritage\nOr have a look at the other LIBER Working Groups - LIBER Europe\n\n\n\n\n\nAI4LAM\nCode4Lib\nElectronic Literature Organisation\nFlickr Commons\nGlam Labs International\nIIIF/UV Open Collective\nIMPACT Centre of Competence\nMuseums Computer Group\nTranskribus\nWikidata Community)\n\n\n\n\n\n\n\nRLUK Digital Scholarship Network\n\n\n\n\n\n\n\nThink we’re missing something?\n\n\n\n\n\nWe’d love to hear it! Suggest edits by opening a new Issue or adding to the discussion on existing Issues on the project Github. If you’re new to GitHub don’t worry, we have a Topic Guide for that: GitHub: How to navigate and contribute to Git-based projects! Or just drop us a line!"
  },
  {
    "objectID": "networks.html#liber-working-groups",
    "href": "networks.html#liber-working-groups",
    "title": "USEFUL NETWORKS",
    "section": "",
    "text": "Working groups are open to staff at participating LIBER Member institutions:\n\nLIBER Data Science in Libraries\nLIBER Digital Scholarship & Digital Cultural Heritage\nOr have a look at the other LIBER Working Groups - LIBER Europe"
  },
  {
    "objectID": "networks.html#international-networks",
    "href": "networks.html#international-networks",
    "title": "USEFUL NETWORKS",
    "section": "",
    "text": "AI4LAM\nCode4Lib\nElectronic Literature Organisation\nFlickr Commons\nGlam Labs International\nIIIF/UV Open Collective\nIMPACT Centre of Competence\nMuseums Computer Group\nTranskribus\nWikidata Community)"
  },
  {
    "objectID": "networks.html#national-networks-european",
    "href": "networks.html#national-networks-european",
    "title": "USEFUL NETWORKS",
    "section": "",
    "text": "RLUK Digital Scholarship Network\n\n\n\n\n\n\n\nThink we’re missing something?\n\n\n\n\n\nWe’d love to hear it! Suggest edits by opening a new Issue or adding to the discussion on existing Issues on the project Github. If you’re new to GitHub don’t worry, we have a Topic Guide for that: GitHub: How to navigate and contribute to Git-based projects! Or just drop us a line!"
  },
  {
    "objectID": "api.html",
    "href": "api.html",
    "title": "APIs: A starter guide for Librarians",
    "section": "",
    "text": "APIs (Application Programming Interfaces) can sound intimidating, but the ideas behind them are not unfamiliar. An application programming interface (API) allows a connection between computers or between computer programs; it is a type of software interface offering a service to other pieces of software. A common analogy that can be helpful in understanding the process of how APIs work is the restaurant analogy.\nImagine going to a restaurant (server). You first consult the menu provided (API documentation) to understand what is available for you to order (request). After consulting the menu (documentation) you have a good idea of what is likely to result in a successful order from the kitchen. The waiter is a bit like the restaurant’s API. Behind the waiter is a whole world of kitchen staff, bar staff, wholesalers, suppliers, shareholders and so on. You don’t need to worry about any of them, you probably won’t even ever speak to them. All your food orders (requests) and any extra instructions such as “chips or mash?” (parameters) will be passed to the kitchen (server) by the waiter (API). The kitchen (server) prepares your order (request) and passes it to the waiter (API) to deliver to your meal (response).\n\n\n\nVisualisation of the “restaurant analogy” for an API.\n\n\nJust as in our analogy above, sites will offer an API, and anyone can use it without having to know anything about the internal workings of that system. You don’t need to see the source code, the technologies they used to build it, or know the name of the person who keeps it running at weekends. All you need to know is how to make a request to the API, and for this, APIs will normally have a site that will document what you can request and how. A good example is OpenLibrary’s API pages. The API documentation will list what requests you can make. It will also describe what additional information you need to provide when making a request, for example the name of a dish in the restaurant example; the additional pieces of information are referred to as parameters. The documentation may also state any security considerations, such as needing authentication (an API Key) or a log in before using the method. See Lists API | Open Library for examples.\nThis Topic Guide will focus on REST-based APIs as an example as it’s the most likely you’ll encounter in your day to day library work. Just be aware that other API types exist, such as:\n\nSOAP\nGraphQL\nRPC\n\nYou can find some information on those here: https://www.postman.com/what-is-an-api/#api-architectural-styles\nThe main thing to keep in mind is the idea that an API enables the exchange of data and information between two systems and provides a protocol between both sides on how to do this. The user gets a documented way to find out how to interact with a system and understand everything that comes back from it. The provider of the service also knows exactly how their system is being interacted with. Again, thinking about this idea in the context of a restaurant, a restaurant might find itself being hugely successful, so makes the decision to move to larger or multiple premises and upgrade its kitchen. If they keep the same menu, then the effect of this on you is minimal – you can still order your favourite dish, despite all the changes!\n\n\n\n\nDifferent types of requests are made to an API depending on what you want to do and whether you want to read from the API or send some information back. In a REST API we call these different types of requests “HTTP request methods”, and they are designed as a simple way to describe what you are trying to do. Have a look at Mozilla’s pages on methods for more in-depth details.\nThe main HTTP request methods are described in the table below:\n\n\n\n\n\n\n\nHTTP Request Method\nDescription\n\n\n\n\nGET\nReads information from the server, but does not change what is on the server. Example: requesting a list of available images or an individual image.\n\n\nPOST\nSend information to be handled by the server, possibly adding or changing what is stored on the server. Example: posting a comment on social media.\n\n\nPUT\nUpload a new resource or replace an entire existing resource, to a specific location on the server. Example: adding a new item to a database.\n\n\nPATCH\nLike PUT, but modifies just part of the existing resource rather than the entire thing.\n\n\nDELETE\nRemoves a specific resource from the server. Example: delete an item from a database.\n\n\n\nTo make these more understandable, let’s substitute in an editor of books in place of all this technology. Our editor character is equivalent to the user of our API, and the author they are working with is equivalent to the API itself. The mail is the equivalent of the Internet and the draft pages of the book are the resources handled by the API.\nOur editor starts the day by GETting a draft version of a book from the author, dropped off by the local postman. As the editor works through the book, they might make some changes to existing text, like a PATCH request does; they might rewrite and replace an entire section, like a PUT request does; or they might suggest adding a new section like a POST request. If they were not happy with a part of the book, they might even request to DELETE it.\n\n\n\nIn our restaurant example, sending our requests to the right person, in this case the waiter or waitress, is essential. We do not want to accidentally ask another customer for our food! When using APIs it is essential to also make sure that we are sending our requests to the right place. However, when using APIs this can be a little more complicated as we must construct a web address that corresponds with what we want to do.\nAPI web addresses are made up of several parts. Take this URL as an example:\nhttps://api.example.org/v1/desserts/56\nWhen you consult the documentation for an API, you might see that a request is documented something like this:\nGET /v1/desserts/{id}\nYou will notice that the first part of the address, https://api.example.org, is omitted. This part of the web address routes to the whole API. Think of it as an address for a company, and everything that follows is different departments.\nYou may need to be prepared for this first part of the address to change, e.g. a company might have a “test” version of the API that you use for development and then they may have a “production” version with live data on it. In some cases, they may only let you have access to the production API after reviewing your work to ensure it will not cause their systems any problems and data will be kept safe.\nThe initial “/” character between “GET” and “v1” means\nThe rest we can break down like this:\n\n\n\n\n\n\n\n—\n—\n\n\n\n\nGET\nThe type of request we are making (see above). In this case we are reading information, not changing it or deleting it.\n\n\n/\nIf you see a forward slash (/) at the start of an address fragment, it means that the rest of the address is relative to the root of the website. Think of this as giving directions to somewhere in an office block from the reception. Without it, the address would be relative, i.e. in our office block this would be the equivalent of giving directions to someone relative to whichever room they happen to be in at that moment.\n\n\nv1\nThe version number of the API. Having versions of the API allows providers to provide new versions of APIs, that may work in different ways, without breaking existing API requests.\n\n\ndesserts\nThis part of the URL tells you the type of resource you will be working with - in this case dessert listings on our menu.\n\n\n{id}\nA unique identifier for the resource. This could be a number such as 56 or a string such as “sticky-toffee-pudding”. The curly brackets indicate that this is a parameter provided by the user.\n\n\n\nAssuming your request was successful, the response from the server could be any sort of file type, depending on the resources it handles. If the endpoint deals with metadata, then you might get back a JSON or XML encoded document. These are both ways of representing data in a way that can be easily understood by a computer program.\nFor instance, a response from our fictional restaurant menu API above might look like this:\n{“id”: 56, “title”: “Rhubarb Crumble”, “description”: “A modern take on a classic pudding, served with custard or ice cream”, “gluten_free”: false, “vegetarian”: true, “vegan”: true, “price”: 5.99}\n\n\n\n\nSometimes, APIs might have functionality that supports returning a group of resources based on a filter. In this case we are providing a field name and the value it must have, in order for that resource to be included in our group.\nLet’s take an example where we need a list of vegan desserts that are also gluten free. If you look at the example above you will see that this means we only want resources where the “vegetarian” field is “true” and the “gluten_free” field is also true.\nParameters are added to a request by adding a question mark (?) symbol, followed by the name of the parameter, an equals sign (=) and the value. Additional parameters can be added by adding an ampersand (&) symbol and the name=value pair as before. In our example, we would end up with a URL like this:\nhttps://api.example.org/v1/desserts?vegan=true&gluten_free=true\nThere will be some practical examples that will step you through how to make different API calls like this in the next section.\n\n\nWhen you make a request to an API you will get a three-digit number returned. This is called the HTTP status code. These codes can be broadly classified as follows:\n2XX: The request was understood, the right authorisation was present, and the server could process your request\n3XX: The web address you are trying to use is now at a different web address\n4XX: You have made an error, this could be caused by the server not understanding your request, or an authorisation problem\n5XX: There was an error on the server side. This could be a temporary or permanent problem, it could be caused by things like a fault in the server-side code, or a database being down.",
    "crumbs": [
      "Topic Guides",
      "APIs: A starter guide for Librarians"
    ]
  },
  {
    "objectID": "api.html#introduction",
    "href": "api.html#introduction",
    "title": "APIs: A starter guide for Librarians",
    "section": "",
    "text": "APIs (Application Programming Interfaces) can sound intimidating, but the ideas behind them are not unfamiliar. An application programming interface (API) allows a connection between computers or between computer programs; it is a type of software interface offering a service to other pieces of software. A common analogy that can be helpful in understanding the process of how APIs work is the restaurant analogy.\nImagine going to a restaurant (server). You first consult the menu provided (API documentation) to understand what is available for you to order (request). After consulting the menu (documentation) you have a good idea of what is likely to result in a successful order from the kitchen. The waiter is a bit like the restaurant’s API. Behind the waiter is a whole world of kitchen staff, bar staff, wholesalers, suppliers, shareholders and so on. You don’t need to worry about any of them, you probably won’t even ever speak to them. All your food orders (requests) and any extra instructions such as “chips or mash?” (parameters) will be passed to the kitchen (server) by the waiter (API). The kitchen (server) prepares your order (request) and passes it to the waiter (API) to deliver to your meal (response).\n\n\n\nVisualisation of the “restaurant analogy” for an API.\n\n\nJust as in our analogy above, sites will offer an API, and anyone can use it without having to know anything about the internal workings of that system. You don’t need to see the source code, the technologies they used to build it, or know the name of the person who keeps it running at weekends. All you need to know is how to make a request to the API, and for this, APIs will normally have a site that will document what you can request and how. A good example is OpenLibrary’s API pages. The API documentation will list what requests you can make. It will also describe what additional information you need to provide when making a request, for example the name of a dish in the restaurant example; the additional pieces of information are referred to as parameters. The documentation may also state any security considerations, such as needing authentication (an API Key) or a log in before using the method. See Lists API | Open Library for examples.\nThis Topic Guide will focus on REST-based APIs as an example as it’s the most likely you’ll encounter in your day to day library work. Just be aware that other API types exist, such as:\n\nSOAP\nGraphQL\nRPC\n\nYou can find some information on those here: https://www.postman.com/what-is-an-api/#api-architectural-styles\nThe main thing to keep in mind is the idea that an API enables the exchange of data and information between two systems and provides a protocol between both sides on how to do this. The user gets a documented way to find out how to interact with a system and understand everything that comes back from it. The provider of the service also knows exactly how their system is being interacted with. Again, thinking about this idea in the context of a restaurant, a restaurant might find itself being hugely successful, so makes the decision to move to larger or multiple premises and upgrade its kitchen. If they keep the same menu, then the effect of this on you is minimal – you can still order your favourite dish, despite all the changes!\n\n\n\n\nDifferent types of requests are made to an API depending on what you want to do and whether you want to read from the API or send some information back. In a REST API we call these different types of requests “HTTP request methods”, and they are designed as a simple way to describe what you are trying to do. Have a look at Mozilla’s pages on methods for more in-depth details.\nThe main HTTP request methods are described in the table below:\n\n\n\n\n\n\n\nHTTP Request Method\nDescription\n\n\n\n\nGET\nReads information from the server, but does not change what is on the server. Example: requesting a list of available images or an individual image.\n\n\nPOST\nSend information to be handled by the server, possibly adding or changing what is stored on the server. Example: posting a comment on social media.\n\n\nPUT\nUpload a new resource or replace an entire existing resource, to a specific location on the server. Example: adding a new item to a database.\n\n\nPATCH\nLike PUT, but modifies just part of the existing resource rather than the entire thing.\n\n\nDELETE\nRemoves a specific resource from the server. Example: delete an item from a database.\n\n\n\nTo make these more understandable, let’s substitute in an editor of books in place of all this technology. Our editor character is equivalent to the user of our API, and the author they are working with is equivalent to the API itself. The mail is the equivalent of the Internet and the draft pages of the book are the resources handled by the API.\nOur editor starts the day by GETting a draft version of a book from the author, dropped off by the local postman. As the editor works through the book, they might make some changes to existing text, like a PATCH request does; they might rewrite and replace an entire section, like a PUT request does; or they might suggest adding a new section like a POST request. If they were not happy with a part of the book, they might even request to DELETE it.\n\n\n\nIn our restaurant example, sending our requests to the right person, in this case the waiter or waitress, is essential. We do not want to accidentally ask another customer for our food! When using APIs it is essential to also make sure that we are sending our requests to the right place. However, when using APIs this can be a little more complicated as we must construct a web address that corresponds with what we want to do.\nAPI web addresses are made up of several parts. Take this URL as an example:\nhttps://api.example.org/v1/desserts/56\nWhen you consult the documentation for an API, you might see that a request is documented something like this:\nGET /v1/desserts/{id}\nYou will notice that the first part of the address, https://api.example.org, is omitted. This part of the web address routes to the whole API. Think of it as an address for a company, and everything that follows is different departments.\nYou may need to be prepared for this first part of the address to change, e.g. a company might have a “test” version of the API that you use for development and then they may have a “production” version with live data on it. In some cases, they may only let you have access to the production API after reviewing your work to ensure it will not cause their systems any problems and data will be kept safe.\nThe initial “/” character between “GET” and “v1” means\nThe rest we can break down like this:\n\n\n\n\n\n\n\n—\n—\n\n\n\n\nGET\nThe type of request we are making (see above). In this case we are reading information, not changing it or deleting it.\n\n\n/\nIf you see a forward slash (/) at the start of an address fragment, it means that the rest of the address is relative to the root of the website. Think of this as giving directions to somewhere in an office block from the reception. Without it, the address would be relative, i.e. in our office block this would be the equivalent of giving directions to someone relative to whichever room they happen to be in at that moment.\n\n\nv1\nThe version number of the API. Having versions of the API allows providers to provide new versions of APIs, that may work in different ways, without breaking existing API requests.\n\n\ndesserts\nThis part of the URL tells you the type of resource you will be working with - in this case dessert listings on our menu.\n\n\n{id}\nA unique identifier for the resource. This could be a number such as 56 or a string such as “sticky-toffee-pudding”. The curly brackets indicate that this is a parameter provided by the user.\n\n\n\nAssuming your request was successful, the response from the server could be any sort of file type, depending on the resources it handles. If the endpoint deals with metadata, then you might get back a JSON or XML encoded document. These are both ways of representing data in a way that can be easily understood by a computer program.\nFor instance, a response from our fictional restaurant menu API above might look like this:\n{“id”: 56, “title”: “Rhubarb Crumble”, “description”: “A modern take on a classic pudding, served with custard or ice cream”, “gluten_free”: false, “vegetarian”: true, “vegan”: true, “price”: 5.99}\n\n\n\n\nSometimes, APIs might have functionality that supports returning a group of resources based on a filter. In this case we are providing a field name and the value it must have, in order for that resource to be included in our group.\nLet’s take an example where we need a list of vegan desserts that are also gluten free. If you look at the example above you will see that this means we only want resources where the “vegetarian” field is “true” and the “gluten_free” field is also true.\nParameters are added to a request by adding a question mark (?) symbol, followed by the name of the parameter, an equals sign (=) and the value. Additional parameters can be added by adding an ampersand (&) symbol and the name=value pair as before. In our example, we would end up with a URL like this:\nhttps://api.example.org/v1/desserts?vegan=true&gluten_free=true\nThere will be some practical examples that will step you through how to make different API calls like this in the next section.\n\n\nWhen you make a request to an API you will get a three-digit number returned. This is called the HTTP status code. These codes can be broadly classified as follows:\n2XX: The request was understood, the right authorisation was present, and the server could process your request\n3XX: The web address you are trying to use is now at a different web address\n4XX: You have made an error, this could be caused by the server not understanding your request, or an authorisation problem\n5XX: There was an error on the server side. This could be a temporary or permanent problem, it could be caused by things like a fault in the server-side code, or a database being down.",
    "crumbs": [
      "Topic Guides",
      "APIs: A starter guide for Librarians"
    ]
  },
  {
    "objectID": "api.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "api.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "APIs: A starter guide for Librarians",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nWhy do Libraries and cultural heritage organisations provide and/or use APIs? APIs for Librarians gives a really nice view of the various ways librarians may want to use APIs big and small for various purposes, from displaying Word of the Day on your website to providing access to Internet Archive material. Overcoming disintermediation: a call for librarians to learn to use web service APIs outlines what APIs can enable for our institutions and users. (Free-to-read version available here). Below are some additional use cases for providing and using APIs in a cultural heritage context:\n\nIntegrate\nThe technological beating heart of a modern library is the Library Management System (LMS). This is the electronic brain of a library, but to be able to do its job it must link to many other systems, usually through APIs. This is called Systems Integration.\nAn example of this is obtaining details about a book that has been bought. The LMS can talk to the publisher’s website through an API to get all the details needed to catalogue the item without the need for someone to manually type it in.\n\n\nEncourage Reuse\nAPIs are a great way to help people interact with cultural heritage collections. They allow people to develop new applications that could access your API, maybe in conjunction with many others, to search for types of items across many collections. They enable users, both inside and outside organisations to analyse information, create visualisations and integrate collections data with other systems and workflows.\nIn the case of digital resources, they can make it easier to reuse and share those resources. For example, the IIIF standard enables images from a cultural heritage institution’s collection to be easily embedded into learning materials produced by a completely different organisation.\n\n\nAuthentication\nMost APIs will require authentication in some way in order to use them, ensuring data providers retain some confidence and control over how the data provided is interacted with. This is in place of logging into the website.\nThe main types of authentication that you will use are:\n\nAPI Key – this is a long alphanumeric bit of text that uniquely identifies you to the system.\nOAuth – this is a bit like an API key, but every so often it gets exchanged for a new one. This is more secure as we are not using one card for an extended period of time, but can be more complicated to set up.\n\n\n\nExtend\nAn organisation’s requirements for what it needs from a computer will change over time as technology and society develops. Having an API is a good way to ensure that you will always be able to extend your system to deal with new requirements. Extending functionality in this way is much easier and less risky than modifying a system that has been written as one monolithic unit.\nAn example here could be if you had to generate a new report from your system. It may already provide some reporting but up until now you have had to take the information from an old report and manipulate it in Excel. This can be error prone and time consuming. Instead, you can use the API to get the information you need and write a program to turn this information into a report. Using an API also has the advantage that you can use an existing system without modifying it or interrupting its uptime. Your code can be entirely separate. (Case Study OpenAthens Reporting API helps librarians demonstrate value)\n\n\nExtract\nEventually, a computer system will come to the end of its life and need replacing, or maybe a system is being replaced because it did not live up to expectations. In this scenario, being able to get all your data out of that system is critical. An API is an excellent way to do this. It may even be possible to write a program that uses the API of your old system to supply the API of your new system with information, saving everybody a lot of time!\n\n\nInsurance\nSometimes, circumstances change around a system. It might be that requirements change, or an organisation finds itself doing something new. An API is a kind of insurance policy in these circumstances. It potentially allows you to extend the functionality of a system to meet new challenges. These might be something you need to do continuously, or just once. For example, you might want to bulk update the classification of some books. With an API you could write a program that would go through your collection, look for matching criteria and update accordingly. By always having access to your own data via an API, you have the insurance of being able to react flexibly if circumstances change.",
    "crumbs": [
      "Topic Guides",
      "APIs: A starter guide for Librarians"
    ]
  },
  {
    "objectID": "api.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "api.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "APIs: A starter guide for Librarians",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nLet’s try a few real-world examples of getting information from an API as this is the easiest way to understand the process.\n\nExercise 1: Using an API for the very first time\n1. Introduction\nIn this exercise, adapted from Using an API: a hands on exercise (a tutorial created for British Library staff by Owen Stephens), you are going to use a Google Spreadsheet to retrieve records from the Flickr API and display the results.\nThe API you are going to use simply allows you to submit some search terms and get a list of results in a format called RSS. You are going to use a Spreadsheet to submit a search to the API, and display the results.\n2. Understanding the API\nThe API you are going to use is an interface to Flickr, a photo sharing website where many cultural heritage institutions have hosted some of their collection images. Flickr has a very powerful API with lots of functions, but for simplicity in this exercise you are just going to use the Flickr RSS feeds, rather than the full API which is more complex and requires you to register.\nBefore you can start working with the API, you need to understand how it works. To do this, we are going to look at an example URL:\nhttps://api.flickr.com/services/feeds/photos_public.gne?tags=food&format=rss\nThe first part of the URL is the address of the API. Everything after the ‘?’ are ‘parameters’ which form the input to the API. There are two parameters listed and they each consist of the parameter name, followed by an ‘=’ sign, then a value.\nThe URL and parameters breakdown like this:\n\n\n\nURL Part\nExplanation\n\n\n\n\nhttps://api.flickr.com/services/feeds/photos_public.gne\nThe address of the API\n\n\ntags=food\nThe “tags” parameter is a list of tags (separated by commas) to be used to filter the list of images returned by the API. In this case, just a single tag “food” is used.\n\n\nformat=rss\nHow the results will be presented, in this case using the RSS feed format.\n\n\n\n3. Using the API\nTo use the API, you are going to use a Google Spreadsheet. Go to https://drive.google.com and login to your Google account. Create a Google Spreadsheet\nThe first thing to do is build the API call (the query you are going to submit to the API).\nFirst some labels:\nIn cell A1 enter the text ‘API Address’\nIn cell A2 enter the text ‘Tags’\nIn cell A3 enter the text ‘Format’\nIn cell A4 enter ‘API Call’\nIn cell A5 enter ‘Results’\nNow, based on the information we were able to obtain by understanding the API we can fill values into column B as follows:\nIn cell B1 enter the address of the API\nIn cell B2 enter a simple, one-word tag\nIn cell B3 enter the text ‘rss’ (omitting the inverted commas)\nThe first three rows of the spreadsheet should look something like this (with whatever tag you’ve chosen to search for in B2):\n\n\n\nExcel sheet with the relevant API information.\n\n\nYou now have all the parameters we need to build the API call. To do this you want to create a URL very similar to the one you looked at above. You can do this using a handy spreadsheet function/formula called ‘Concatenate’ which allows you to combine the contents of a number of spreadsheet cells with other text.\nIn Cell B4 type the following formula:\n=CONCATENATE(B1,“?”,“tags=”,B2,“&format=”,B3)\nThis joins the contents of cells B1, B2 with the text included in inverted commas in formula. Once you have entered this formula and pressed enter, your spreadsheet should look like:\n\n\n\nExcel sheet with the relevant API information.\n\n\nThe final step is to send this query, and retrieve and display the results. This is where the fact that the API returns results as an RSS feed comes in extremely useful. Google Spreadsheets has a special function for retrieving and displaying RSS feeds.\nTo use this, in Cell B5 type the following formula:\n=importFeed(B4)\nGoogle Spreadsheets knows what an RSS feed is, and understands that it will contain one or more ‘items’ with a ‘title’ and a ‘link’. It will do the rest for us. Hit enter, and see the results.\nExcel sheet with the relevant API information.\nCongratulations! You have built an API query, and displayed the results.\nYou have:\n* Explored an API for Flickr\n* Seen how you can ‘call’ the API by adding some parameters to a URL\n* Understood how the API returns results in RSS format\n* Used this knowledge to build a Google Spreadsheet which searches for a tag on Flickr and displays the results\n4. Going Further\nFurther parameters that this API accepts are:\n\nid\nids\ntagmode\nformat\nlang\n\nThese are documented at https://www.flickr.com/services/feeds/docs/photos_public/. When adding parameters to a URL, you use the ampersand (‘&’ sign) between each parameter. For example:\nhttps://api.flickr.com/services/feeds/photos_public.gne?tags=food&id=23577728@N07\nThis searches for all photos tagged with ‘food’ from a specific user with id = 23577728@N07\nBy adding a row to the spreadsheet for this parameter, and modifying the ‘concatenate’ statement that builds the API Call, can you make the spreadsheet only return images with a specific tag in the British Library Flickr collection? The Flickr ID for the British Library is 12403504@N02\n\n\nExercise 2: Find out information for a location using Google APIs\n1. Go to https://developers.google.com/maps/documentation/places/web-service/text-search in your browser  2. Click on “Web Service” as the platform 3. You may be asked to log in using your Google account. 4. Click on the “API” button in the right-hand section. 5. Click on the “Fullscreen” icon along the top of this section, it is the icon that looks like edges of a square. 6. In the APIs Explorer window, click on “HTTP” – this lets you see the API request in terms of the web requests that are made. 7. The “Request Body” has a single parameter called “textQuery” which you can use to search for a place you would like to know more about. 8. Try a place you know, for example “British Library” as the value. 9. Click “Execute”. 10. You may be prompted to login and authorise the API request to continue.\nAll being well you will see a response in the right-hand side. “200” will be prominently displayed, you might remember this code from the “How did the request go?” section earlier. Underneath, you will see a structured response with everything Google knows about that place. Notice that a lot of information is mixed in together, for example what type of location it is, coordinates on a map and opening hours.\n\n\nOther online tutorials for beginners to try:\n\nJoshua Dull has created a useful workshop in the style of the Library Carpentries, called APIs for Libraries\nOpenLibrary has some APIs you can experiment with directly in your browser: https://openlibrary.org/developers/api and I can particularly recommend having a play around in their sandbox which gives a great illustration of the code involved in API requests and API responses: swagger/docs | Open Library\nThe Programming Historian has a lot of great API tutorials you can try which will walk you through using APIs for different purposes such as Fetching and Parsing Data from the Web with OpenRefine\n\n\n\nOnce you’re feeling a little more confident, these tutorials will help you go further:\n\nWikimedia offers some incredibly useful APIs of particular interest to libraries who contribute to or have collection items there. They have an entire tutorial to help beginners: Getting started with Wikimedia APIs - API Portal\nDataquest has a nice tutorial on using the Python language to call APIs: How to Use an API in Python\nYou can even use APIs to integrate with the latest AI tools. Here are some examples from OpenAI: Developer quickstart: Take your first steps with the OpenAI API.\nOnce you are a bit more confident with how APIs work, why not try designing one? For example, how would you go about designing an API for a publisher to allow them to keep track of books, editions and authors? This guide should help: RESTful web API design.",
    "crumbs": [
      "Topic Guides",
      "APIs: A starter guide for Librarians"
    ]
  },
  {
    "objectID": "api.html#recommended-readingviewing",
    "href": "api.html#recommended-readingviewing",
    "title": "APIs: A starter guide for Librarians",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nMany great resources are available on the web to tell you more about APIs:\n\nIntroduction to APIs | DARIAH-Campus\nWhat is an API? from IBM https://www.ibm.com/think/topics/api\nWhat is a REST API? from Postman: https://www.youtube.com/watch?v=PfujVETI-i4\nWhat is a REST API? from IBM: https://www.youtube.com/watch?v=lsMQRaeKNDk\nUK Government list of APIs: https://www.api.gov.uk/#uk-public-sector-apis\nTrove API introduction - GLAM Workbench\n\nSome highly relevant APIs for libraries and/or researchers: - OpenAlex is a free API that provides similar information to Scopus and Web of Science. - CrossRef provides a free API to search and access metadata for all of their DOIs.",
    "crumbs": [
      "Topic Guides",
      "APIs: A starter guide for Librarians"
    ]
  },
  {
    "objectID": "api.html#finding-communities-of-practice",
    "href": "api.html#finding-communities-of-practice",
    "title": "APIs: A starter guide for Librarians",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nThis guide is intended only as a first step for starting your journey with APIs and as you start experimenting and getting hands-on experience you will undoubtedly have further questions or need for support from those who have more experience with APIs. Here are some suggestions: - Many organisations providing APIs will have support or contact info for questions and troubleshooting so look out for those. - You can also join a community such as the AI4LAM community where you can connect with lots of professionals working at the intersection of technology and cultural heritage. - Stack Overflow is a popular discussion board where you can post your software related questions. - Look for local in-person or online Hackathons which are often focused on using a specific API.",
    "crumbs": [
      "Topic Guides",
      "APIs: A starter guide for Librarians"
    ]
  },
  {
    "objectID": "digitisation.html",
    "href": "digitisation.html",
    "title": "Digitisation: Imaging, 3D and RTI",
    "section": "",
    "text": "Over recent decades research library collections, operations, and audiences have moved from a largely analogue to a mixed analogue/digital environment. So, it will come as no surprise that advances in digital imaging have put digitisation in the position of being one of the most prominent demonstrations of this digital shift. Cultural heritage institutions of all sizes are likely to have some level of digitisation and imaging workflows and standards in place, either through in-house imaging labs or outsourcing through mass digitisation projects like Google Books to keep up with this shift. Many seek to take advantage of uniqueness in their collection by exploiting advanced imaging methods such as 3D modelling, Multispectral Imaging and Reflectance Transformation Imaging (RTI).\nOf course, now that almost everyone has a camera available to them via their phone, it may appear easy for anyone to digitise collection items quickly, or even make their own 3D model, but navigating this complex landscape of technologies, terminologies, methodologies, gadgets and gizmos can be quite daunting for anyone. Those working in cultural heritage institutions also need to consider not just how the technologies work, but how they can be employed within budget, integrated at scale into larger workflows, support preservation alongside access, and be deployed sensitively. This guide aims to help library professionals to understand some of the questions and considerations they need to keep in mind while exploring and having a play with some of these new advanced imaging methods for collection digitisation.\n\n\nIdentifying the right imaging method for different objects, and then the specific technology or combination of them to use can be confusing, and is often restricted by what resources you might have available. Knowing and understanding why you are digitising something from the outset, and what it will be used for, will also inform the decision process, so spend some time thinking about that - the wrong choices can be costly and time consuming. That said, in my experience many of the technologies advance so quickly that whatever you choose, no matter how thoughtfully, will likely be out of date by the time you’ve finished - so my advice is to accept this inevitably, don’t let it stop you, start with a sound approach, do the best with what you can, and know who, and when, to ask for advice!\nHere are brief introductions to some of the advanced imaging methods we have at our disposal:\n\n\n\nMost imaging methods build on a foundation of high quality 2D imaging. You’ll have heard phone manufacturers boasting about how many megapixels their camera has. But this isn’t always an indicator of quality, there are lots of other factors, the more important factor being the size and quality of the imaging sensor. So while using your phone camera may seem a cost-effective and easy solution for digitising collection items, it is unlikely to provide the high-resolution and preservation quality you’re after long term. The guide ‘Remote Capture: Digitising Documentary Heritage in Challenging Locations’ explains this quite well in their chapter on Equipment and skills for digitising in the field. You can see some great examples that demonstrate why high-quality imaging is a must in my case-study.\n\n\n\nAn illustration showing example digital image sensor sizes.\n\n\n\n\n\n3D data can be generated through different methods and technologies. These include photogrammetry, which uses images to obtain accurate measurable information of real-world objects and the terrain, and LIDAR which stands for ‘light detection and ranging’ and is a remote sensing method that uses pulses of light to measure distances and angles as they bounce back into the sensor. The data produced from these methods can then be used to build 3D models of objects or the terrain. 3D models help a user build an understanding of the form of an object, and can even be useful in conveying book construction for example. It works well on most objects, but can run into difficulties with reflective materials like polished glass and metal. Check out some examples in the case-study.\n3D models of cultural heritage objects can then be used in AR/VR/XR environments. You might have come across all three of these terms and been slightly confused as to what the difference is. Augmented reality (AR) is an experience where reality is enhanced in some way using technology. Virtual reality (VR) is usually an entirely simulated experience in which you are immersed. Extended reality (XR) is an umbrella term that encompasses AR, VR or even mixed reality (MR), in which both actual and virtual worlds are merged.\n\n\n\nThis is a method of examining an object under different wavelengths of light – you’ve probably heard of infrared and ultraviolet, but there’s a huge spectrum that can be applied in order to reveal underwriting or investigate faded text and even the structure of the medium, revealing things like paper manufacturing watermarks. Whilst multispectral images can be hard for the average user to interpret, specialists can use them to transcribe long lost texts. For an example of this, you can see a recently rediscovered Merlin Fragment in the IIIF viewer below, or check out the digital edition of the Codex Zacynthius.\n\n\n\n\n\nReflectance Transformation Imaging (RTI) is a technique that creates hyper-real digital images with which the viewer can interact. It creates texture mapping by capturing multiple images of the subject from a fixed point whilst the light source varies in position. This is really useful for revealing the textured detail in surfaces of objects such as coins, engravings or pressed plant material - there are some examples of a herbarium sheet in my case-study. This diagram shows a basic RTI set-up, you can learn more about the technique and process from the Cultural Heritage Imaging’s website about RTI.\n\n\n\nAn illustration showing an example RTI setup, by Andy Corrigan.\n\n\n\n\n\n\nLighting is often as important, if not more so, than the camera you are using, so it pays to learn a bit about that too. For example, a camera sensor is more sensitive to colour temperature and contrast than the human eye. The guide Remote Capture: Digitising Documentary Heritage in Challenging Locations explains a bit more in their section about lighting and flash.\n\nEthics & Cultural Sensitivities Another thing to remember when selecting and preparing to digitise collections is the impact of our own narrative into the process. What we choose to select, our interactions, and the choices we make for providing access to objects, become entwined and embedded through the process of digitisation, and this should be thought through as well. The Digital Preservation Coalition has published a useful guidance note ‘Exploring ethical considerations for providing access to digital heritage collections’, and there is also some interesting discussion in Fafinski’s article ‘Facsimile narratives: Researching the past in the age of digital reproduction’.\nFacsimile, surrogate, object or edition? Opinions often vary on how/what we consider digitised objects to be and what we call them. You may have come across the terms “digital surrogate”, “digital facsimile” or “digital edition” for example. The digitisation process can remove or reduce some aspects, but can add or increase others. Due to this, the outputs of digitisation are now more widely considered to constitute a distinct thing from their real-world original.\nCopyright and licensing is a complex issue, but an essential consideration when digitising anything. You might want to start by looking at the LIBER DS Topic Guide on Copyright.\nHosting Your institution might have one or more hosting solutions on which you can store your digitisation and make it available over the web and integrate it into existing systems. If not, then IIIF might be worth exploring, and a great place to start is the LIBER DS Topic Guide on IIIF. But whilst regular image hosting options are common, more complex digital objects can be a challenge. If you are creating 3D models, RTI or other specialist imaging, you will need to consider a number of options. Many cultural institutions have been hosting their 3D models through a platform called Sketchfab, but at the time of writing, the platform has recently been absorbed into a bigger platform, and its future is in doubt, providing a good example of how challenging it can be relying on third party services. Other options to consider include MorphoSource, a data repository that is more focussed on research and academia, or tools such as Model-Viewer and A-Frame that can be used to build your own virtual experiences. The IIIF community are currently working on support for 3D objects, so keep an eye on developments there!\n\nDigital Preservation is also important to consider - digitisation can be expensive, so don’t risk losing your valuable assets. A great place to start is the Digital Preservation Coalition.",
    "crumbs": [
      "Topic Guides",
      "Digitisation: Imaging, 3D and RTI"
    ]
  },
  {
    "objectID": "digitisation.html#introduction",
    "href": "digitisation.html#introduction",
    "title": "Digitisation: Imaging, 3D and RTI",
    "section": "",
    "text": "Over recent decades research library collections, operations, and audiences have moved from a largely analogue to a mixed analogue/digital environment. So, it will come as no surprise that advances in digital imaging have put digitisation in the position of being one of the most prominent demonstrations of this digital shift. Cultural heritage institutions of all sizes are likely to have some level of digitisation and imaging workflows and standards in place, either through in-house imaging labs or outsourcing through mass digitisation projects like Google Books to keep up with this shift. Many seek to take advantage of uniqueness in their collection by exploiting advanced imaging methods such as 3D modelling, Multispectral Imaging and Reflectance Transformation Imaging (RTI).\nOf course, now that almost everyone has a camera available to them via their phone, it may appear easy for anyone to digitise collection items quickly, or even make their own 3D model, but navigating this complex landscape of technologies, terminologies, methodologies, gadgets and gizmos can be quite daunting for anyone. Those working in cultural heritage institutions also need to consider not just how the technologies work, but how they can be employed within budget, integrated at scale into larger workflows, support preservation alongside access, and be deployed sensitively. This guide aims to help library professionals to understand some of the questions and considerations they need to keep in mind while exploring and having a play with some of these new advanced imaging methods for collection digitisation.\n\n\nIdentifying the right imaging method for different objects, and then the specific technology or combination of them to use can be confusing, and is often restricted by what resources you might have available. Knowing and understanding why you are digitising something from the outset, and what it will be used for, will also inform the decision process, so spend some time thinking about that - the wrong choices can be costly and time consuming. That said, in my experience many of the technologies advance so quickly that whatever you choose, no matter how thoughtfully, will likely be out of date by the time you’ve finished - so my advice is to accept this inevitably, don’t let it stop you, start with a sound approach, do the best with what you can, and know who, and when, to ask for advice!\nHere are brief introductions to some of the advanced imaging methods we have at our disposal:\n\n\n\nMost imaging methods build on a foundation of high quality 2D imaging. You’ll have heard phone manufacturers boasting about how many megapixels their camera has. But this isn’t always an indicator of quality, there are lots of other factors, the more important factor being the size and quality of the imaging sensor. So while using your phone camera may seem a cost-effective and easy solution for digitising collection items, it is unlikely to provide the high-resolution and preservation quality you’re after long term. The guide ‘Remote Capture: Digitising Documentary Heritage in Challenging Locations’ explains this quite well in their chapter on Equipment and skills for digitising in the field. You can see some great examples that demonstrate why high-quality imaging is a must in my case-study.\n\n\n\nAn illustration showing example digital image sensor sizes.\n\n\n\n\n\n3D data can be generated through different methods and technologies. These include photogrammetry, which uses images to obtain accurate measurable information of real-world objects and the terrain, and LIDAR which stands for ‘light detection and ranging’ and is a remote sensing method that uses pulses of light to measure distances and angles as they bounce back into the sensor. The data produced from these methods can then be used to build 3D models of objects or the terrain. 3D models help a user build an understanding of the form of an object, and can even be useful in conveying book construction for example. It works well on most objects, but can run into difficulties with reflective materials like polished glass and metal. Check out some examples in the case-study.\n3D models of cultural heritage objects can then be used in AR/VR/XR environments. You might have come across all three of these terms and been slightly confused as to what the difference is. Augmented reality (AR) is an experience where reality is enhanced in some way using technology. Virtual reality (VR) is usually an entirely simulated experience in which you are immersed. Extended reality (XR) is an umbrella term that encompasses AR, VR or even mixed reality (MR), in which both actual and virtual worlds are merged.\n\n\n\nThis is a method of examining an object under different wavelengths of light – you’ve probably heard of infrared and ultraviolet, but there’s a huge spectrum that can be applied in order to reveal underwriting or investigate faded text and even the structure of the medium, revealing things like paper manufacturing watermarks. Whilst multispectral images can be hard for the average user to interpret, specialists can use them to transcribe long lost texts. For an example of this, you can see a recently rediscovered Merlin Fragment in the IIIF viewer below, or check out the digital edition of the Codex Zacynthius.\n\n\n\n\n\nReflectance Transformation Imaging (RTI) is a technique that creates hyper-real digital images with which the viewer can interact. It creates texture mapping by capturing multiple images of the subject from a fixed point whilst the light source varies in position. This is really useful for revealing the textured detail in surfaces of objects such as coins, engravings or pressed plant material - there are some examples of a herbarium sheet in my case-study. This diagram shows a basic RTI set-up, you can learn more about the technique and process from the Cultural Heritage Imaging’s website about RTI.\n\n\n\nAn illustration showing an example RTI setup, by Andy Corrigan.\n\n\n\n\n\n\nLighting is often as important, if not more so, than the camera you are using, so it pays to learn a bit about that too. For example, a camera sensor is more sensitive to colour temperature and contrast than the human eye. The guide Remote Capture: Digitising Documentary Heritage in Challenging Locations explains a bit more in their section about lighting and flash.\n\nEthics & Cultural Sensitivities Another thing to remember when selecting and preparing to digitise collections is the impact of our own narrative into the process. What we choose to select, our interactions, and the choices we make for providing access to objects, become entwined and embedded through the process of digitisation, and this should be thought through as well. The Digital Preservation Coalition has published a useful guidance note ‘Exploring ethical considerations for providing access to digital heritage collections’, and there is also some interesting discussion in Fafinski’s article ‘Facsimile narratives: Researching the past in the age of digital reproduction’.\nFacsimile, surrogate, object or edition? Opinions often vary on how/what we consider digitised objects to be and what we call them. You may have come across the terms “digital surrogate”, “digital facsimile” or “digital edition” for example. The digitisation process can remove or reduce some aspects, but can add or increase others. Due to this, the outputs of digitisation are now more widely considered to constitute a distinct thing from their real-world original.\nCopyright and licensing is a complex issue, but an essential consideration when digitising anything. You might want to start by looking at the LIBER DS Topic Guide on Copyright.\nHosting Your institution might have one or more hosting solutions on which you can store your digitisation and make it available over the web and integrate it into existing systems. If not, then IIIF might be worth exploring, and a great place to start is the LIBER DS Topic Guide on IIIF. But whilst regular image hosting options are common, more complex digital objects can be a challenge. If you are creating 3D models, RTI or other specialist imaging, you will need to consider a number of options. Many cultural institutions have been hosting their 3D models through a platform called Sketchfab, but at the time of writing, the platform has recently been absorbed into a bigger platform, and its future is in doubt, providing a good example of how challenging it can be relying on third party services. Other options to consider include MorphoSource, a data repository that is more focussed on research and academia, or tools such as Model-Viewer and A-Frame that can be used to build your own virtual experiences. The IIIF community are currently working on support for 3D objects, so keep an eye on developments there!\n\nDigital Preservation is also important to consider - digitisation can be expensive, so don’t risk losing your valuable assets. A great place to start is the Digital Preservation Coalition.",
    "crumbs": [
      "Topic Guides",
      "Digitisation: Imaging, 3D and RTI"
    ]
  },
  {
    "objectID": "digitisation.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "digitisation.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Digitisation: Imaging, 3D and RTI",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nOne of the biggest benefits to digitising content is often thought of as one of access. Sharing things that have been locked away in our libraries and archives, sometimes for many hundreds of years, to anyone, anywhere in the world with an internet connection is a very powerful thing. This isn’t the only reason or advantage to digitisation - some might be surprising or even more compelling.\nAcknowledgment that the act of digitisation is also an important and valuable part of the research process is steadily growing. For example, over the last few years in the UK, recognition amongst university and research institutions of initiatives such as the Technician Commitment and the Hidden Ref has been increasingly impactful.\n\nCambridge University Library\nBy way of a case study, I wanted to share some stories relating to digitisation here at Cambridge Digital Library. Hopefully these demonstrate just some of the potential and value of digitisation.\nWe are lucky here at Cambridge that a long history of larger scale digitisation projects means we have been able to steadily increase the amount of equipment, skills, and experience we have and the services we can offer researchers. But this activity is more than a service, we work in direct collaboration with researchers, curators, conservators, publishers and exhibitions teams to help them achieve the results they need.\n\n\nFlat digitisation\nAt Cambridge, our focus is very much on delivering the highest possible quality results we can. This is good practice when thinking about the longevity of your outputs, digital preservations needs thinking about at every stage of course, but a major advantage of this approach is that the higher quality the result, the more likely you are to make new discoveries and learn new things about the objects in our care. We have many examples of things that have only been spotted because of the high quality imaging we produce. Take a look at some of the Benefits of Digitisation in this interactive story:\n\n\n\n\n3D and other multidimensional media\nWhilst we might normally expect the written word to occur on a page, the world is more complex than that and even paper isn’t as “flat” as you might expect. Some of the most ancient texts in our collections are Oracle Bones, that are about 3000 years old. They are objects that have undergone a process far more complex than simply marking a flat surface with a pen. They’re materiality is intrinsic to the meaning of the text on their surfaces. But even though something this ancient has been studied for hundreds of years, they are so fragile that most studies are undertaken from impressions of the text (a process which flattens the text for reading and reproduction on the printed page), so it is not often the object is taken out of its carefully constructed archival housing. It was not until the curator saw the 3D model of this oracle bone in the video below, that they noticed there was a surface of it on which there were markings that had never been spotted before!\n\n\nSo, as we can see, 3D modelling can facilitate deeper study and reduce the need to handle fragile objects. But 3D printing them can also be a great way to bring collections to life and enable increased handling via replicas. This is a great way to engage anyone with handling collection items, but can be a particularly pertinent way to bring them to life for people with visual impairments, as we can see in The tale of the ‘Old Horse’ for example:\n\n\nBut 3D models don’t work for everything. In another project we have been collaborating across the other collections at the University of Cambridge to experiment with digitally re-uniting the wide variety of collections relating to Charles Darwin that the University has split over various museums and archives. One particular challenge with this project was presented in the form of herbarium sheets. Plant specimens that Darwin collected on the Beagle Voyage, some of which are now extinct. Whilst the writing on the sheets is well captured by a flat digital image, the plant parts themselves have form and texture, an understanding of which cannot be perceived so well on a flatly lit image. We experimented with 3D modelling them, but ironically they are too flat – the processes involved in 3D modelling didn’t cope well with the flat paper surfaces. Whilst it would be possible to rectify this digitally, it would take a great deal of time, rendering the process extremely inefficient. So we are experimenting with RTI, which allows us to capture texture in a much more engaging way.\nViewing RTI files currently requires specific software, although developments are underway to facilitate the experience through a web browser. The video below demonstrates a few examples that showcase what the experience is like:\n\n\nYou might even find that occasionally people want to see inside something! CT Scanning certainly isn’t an everyday technique that we might associate with research libraries, but you never know. The CT scan shown in the video below is of a fish specimen collected by Charles Darwin on the Beagle Voyage nearly 200 years ago. Preserved in alcohol in a jar, it’s not very easy to inspect or study the real thing:",
    "crumbs": [
      "Topic Guides",
      "Digitisation: Imaging, 3D and RTI"
    ]
  },
  {
    "objectID": "digitisation.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "digitisation.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Digitisation: Imaging, 3D and RTI",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nIf you’ve got half an hour and a colleague or friend to spare, why not take your first steps into a virtual world and have a go at making a 3D model.\nThe “Big Me, Little Me” exercise has been developed by the team at StoryFutures, and is a great way to start thinking about 3D modelling and the virtual world it creates.\n\n!Warning! - You’ll probably end up wanting to spend longer!\n:-) The question you always need to ask yourself though, is should you?\n\n\nStep 1: Download the Scaniverse App to your mobile device.\n\nStep 2: Scan your friend or colleague. Here are some tips:\n\nMove slowly and as steadily as possible - it also helps if your subject is comfortable and can stay as still as possible.\n\nTry to follow a regular pattern as you move around your subject. Moving in a spiral shape around them from top to bottom can work well.\n\nPay extra attention to heads and faces - avoid starting or stopping with the face as this can result in a visible ‘seam’.\n\n\nStep 3: Process your scan - this might take a few minutes, maybe have a cup of tea.\n\nStep 4: Once your scan is processed, click the “AR View” button. This allows you to play around with the scan in augmented reality, so you can adjust the scale and position. This is the fun bit - you can shrink the scan and get your colleague to adopt a funny pose with a mini version of themselves!\n\nStep 5: Now you can take a screenshot and send it round the team to make everyone smile!",
    "crumbs": [
      "Topic Guides",
      "Digitisation: Imaging, 3D and RTI"
    ]
  },
  {
    "objectID": "digitisation.html#recommended-readingviewing",
    "href": "digitisation.html#recommended-readingviewing",
    "title": "Digitisation: Imaging, 3D and RTI",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nThere’s loads out there to read-up on that covers everything from the very technical aspects and guidance to the more philosophical side of things - inspiration is everywhere and creative approaches have a habit of seeding into fruitful outcomes!\n\nMaterial Awareness : Exploring the Entanglement of Library Digitization and Digital Textual Scholarship. Martinez, Merisa. (2024). PhD dissertation. Högskolan i Borås.\n\nWhy Do We Digitize? The Case for Slow Digitization. Prescott, Andrew and Hughes, Lorna. (2018). Archive Journal.\n\nA Field Guide to Digital Surrogates: Evaluating and Contextualizing a Rapidly Changing Resource. Stanford, Emma. (2020). In Kathryn Brown (ed.) ‘The Routledge Companion to Digital Humanities and Art History’ (1st ed.). Routledge. Pp 203-214.\n\nDigital humanities and digitised cultural heritage. Terras, Melissa. (2022). In J O’Sullivan (ed.), ‘The Bloomsbury Handbook to the Digital Humanities’ (1st ed.). Bloomsbury Handbooks, Bloomsbury Academic. pp. 255-266.\n\nFacsimile narratives: Researching the past in the age of digital reproduction. Fafinski, Mateusz. (2021). In ‘Digital Scholarship in the Humanities’, Vol. 37. No. 1, 2022.\n\nTechnical Guidance:\n\nRemote Capture: Digitising Documentary Heritage in Challenging Locations. Edited by Jody Butterworth, Andrew Pearson, Patrick Sutherland & Adam Farquhar. (2018). Open Book Publishers.\n\nReflectance Transformation Imaging (RTI). Cultural Heritage Imaging.\n\nFrom Shelf to Europeana: Digitization Workflow Handbook. Europeana.\n\nBasic principles and tips for 3D digitisation of cultural heritage. Europeana (2020).\n\nLearning hub. Association for Historical & Fine Art Photography (AHFAP).\n\nManual for the photography of 3D objects. Rijksmuseum (2017).\n\nTechnical Guidelines for Digitizing Cultural Heritage Materials (3rd ed.). Federal Agencies Digital Guidelines Initiative (FADGI), (2023).\n\nThe London Charter: For the Computer-Based Visualisation of Cultural Heritage. (2009).\nHighlight-Reflectance Transformation Imaging (H-RTI) for Cultural Heritage. Historic England (2018).",
    "crumbs": [
      "Topic Guides",
      "Digitisation: Imaging, 3D and RTI"
    ]
  },
  {
    "objectID": "digitisation.html#finding-communities-of-practice",
    "href": "digitisation.html#finding-communities-of-practice",
    "title": "Digitisation: Imaging, 3D and RTI",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nAs we all know, libraries are friendly places and often keen on collaborating. Why not reach out to some libraries with digitisation studios and see if you can go and visit them to learn a bit more about how they operate or what equipment they’re using first hand - it can really help to see a variety of different set-ups in person to start building up an idea of what might work for you. Or try and find a local workshop or summer school to attend that will give you some hands-on experience and the opportunity to meet others like you!",
    "crumbs": [
      "Topic Guides",
      "Digitisation: Imaging, 3D and RTI"
    ]
  },
  {
    "objectID": "lod.html",
    "href": "lod.html",
    "title": "Linked Open Data in Library Use Today",
    "section": "",
    "text": "The Semantic Web was first introduced in the 2000s by Tim Berners Lee as an extension of the current Web. Instead of providing information in the form of documents and unstructured text like in traditional webpages, the Semantic Web facilitates the publication of machine-readable data on the web through standards such as Resource Description Framework (RDF) and Web Ontology Language (OWL).\nWhat does publishing linked open data enable? Linked Open Data (LOD) is a method of publishing structured data about things using RDF to enable interlinking and semantic queries across datasets. The data is organised in “triples”, each consisting of a subject (for example Named Person), a predicate (for example Is Author Of), and an object (for example Book Title), identified by Uniform Resource Identifiers (URIs) to ensure global uniqueness and interoperability. It allows metadata to be connected and enriched, so that different representations of the same content can be found, and links made between related resources.\nHave a quick look at this video from Europeana explaining the high-level basic principle of LOD before we dive a bit deeper into how it practically works:\n Linked Open Data | Europeana PRO\n\n\nRDF triples are the fundamental building blocks of Linked Open Data. Triples follow the RDF standard and consist of three components:\n\nSubject: This is the entity or resource being described. It is usually represented by a URI that uniquely identifies the resource.\nPredicate: This represents the relationship or property of the subject. It is also identified by a URI and specifies the type of relationship between the subject and the object.\nObject: This is the value or resource that is related to the subject. The object can be another URI (representing another resource) or a literal value (such as a string or number), amongst others.\n\nAn example of a triple stating “Miguel de Cervantes is author of El Quijote” would look like this:\n\nSubject: &lt;http://www.wikidata.org/entity/Q5682&gt; (Wikidata URI reference to Miguel de Cervantes)\nPredicate: &lt;http://purl.org/dc/terms/creator&gt; (Dublin Core URI term for creator/author)\nObject: &lt;http://www.wikidata.org/entity/Q480&gt; (Wikidata URI reference to the book El Quijote)\n\nIn a 2020 survey of LIBER members, the LIBER Linked Open Data Working Group identified GeoNames, VIAF, ISNI, and Wikidata as the most frequently used datasets by libraries to enrich their catalogues:\n\n\n\nName of the Dataset\nDescription\n\n\n\n\nGeoNames\nContains over 25 million geographical names and consists of over 11 million unique features whereof 4.8 million populated places and 13 million alternate names.\n\n\nISNI\nLibrary of Congress’ Linked Data Services with over 50 vocabularies.\n\n\nVIAF\nOCLC’s Virtual Authority file (VIAF), an aggregation of multiple authority files from different countries and regions.\n\n\nWikidata\nWikidata is a free and open knowledge base that can be read and edited by both humans and machines. Wikidata acts as central storage for the structured data of its Wikimedia sister projects including Wikipedia, Wikisource and others.\n\n\n\nThere are many more datasets that can be used depending on your needs (see Linked Data Survey (oclc.org); some examples of advanced data models are Bibliographic Framework (BIBFRAME) and Library Reference Model (LRM). In addition, the lod-cloud provides more than one thousand LOD repositories classified by categories and based on different domains such as geography and government.\nSo let’s go back to our triple that describes the relationship between the resource “Miguel de Cervantes” and the book of “El Quijote”. When different triples share the same URI for a subject, predicate, or object, they create a connection. For example:\nTriple 1: Miguel de Cervantes is author of El Quijote\n\nSubject: &lt;http://www.wikidata.org/entity/Q5682&gt; (Wikidata identifier for the author Miguel de Cervantes)\nPredicate: &lt;https://schema.org/author&gt; (Schema.org term for creator/author)\nObject: &lt;http://www.wikidata.org/entity/Q480&gt; (Wikidata identifier for the work El Quijote)\n\nTriple 2: El Quijote is a work of Spanish Literature\n\nSubject: &lt;http://www.wikidata.org/entity/Q480&gt; (Wikidata identifier for the work El Quijote)\nPredicate: &lt;https://schema.org/about&gt; (Schema.org term for subject)\nObject: &lt;http://dbpedia.org/resource/Spanish_literature&gt; (DBpedia identifier for Spanish literature)\n\nHere, the object of the first triple (&lt;http://www.wikidata.org/entity/Q480&gt;) is the subject of the second triple, linking information about the book to information about its subject matter. So an example catalogue record combining many triples then might look like:\n&lt;http://example.org/catalogue/El_Quijote&gt; &gt;rdf:type schema:Book; schema:name “El_Quijote”; schema:author &lt;http://www.wikidata.org/entity/Q5682&gt;; schema:genre &lt;http://dbpedia.org/resource/Novel&gt;; schema:inLanguage &lt;http://id.loc.gov/vocabulary/iso639-1/es&gt;; schema:datePublished “1605”; schema:about &lt;http://dbpedia.org/resource/Spanish_literature&gt;; schema:about &lt;http://dbpedia.org/resource/Spanish_Golden_Age&gt;; schema:sameAs &lt;http://dbpedia.org/resource/Don_Quixote&gt;.\n&lt;http://www.wikidata.org/entity/Q5682&gt; &gt;rdf:type schema:Person; schema:name “Miguel de Cervantes”; schema:birthPlace &lt;http://dbpedia.org/resource/Alcala_de_Henares&gt;; schema:birthDate “1547-09-29”.\n&lt;http://dbpedia.org/resource/Alcala_de_Henares**&gt; &gt;rdf:type schema:Place; schema:name “Alcalá de Henares”; geo:country &lt;http://sws.geonames.org/2510769/&gt;.\n&lt;http://sws.geonames.org/2510769/&gt; &gt;rdf:type schema:Country; schema:name “Spain” .",
    "crumbs": [
      "Topic Guides",
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#introduction",
    "href": "lod.html#introduction",
    "title": "Linked Open Data in Library Use Today",
    "section": "",
    "text": "The Semantic Web was first introduced in the 2000s by Tim Berners Lee as an extension of the current Web. Instead of providing information in the form of documents and unstructured text like in traditional webpages, the Semantic Web facilitates the publication of machine-readable data on the web through standards such as Resource Description Framework (RDF) and Web Ontology Language (OWL).\nWhat does publishing linked open data enable? Linked Open Data (LOD) is a method of publishing structured data about things using RDF to enable interlinking and semantic queries across datasets. The data is organised in “triples”, each consisting of a subject (for example Named Person), a predicate (for example Is Author Of), and an object (for example Book Title), identified by Uniform Resource Identifiers (URIs) to ensure global uniqueness and interoperability. It allows metadata to be connected and enriched, so that different representations of the same content can be found, and links made between related resources.\nHave a quick look at this video from Europeana explaining the high-level basic principle of LOD before we dive a bit deeper into how it practically works:\n Linked Open Data | Europeana PRO\n\n\nRDF triples are the fundamental building blocks of Linked Open Data. Triples follow the RDF standard and consist of three components:\n\nSubject: This is the entity or resource being described. It is usually represented by a URI that uniquely identifies the resource.\nPredicate: This represents the relationship or property of the subject. It is also identified by a URI and specifies the type of relationship between the subject and the object.\nObject: This is the value or resource that is related to the subject. The object can be another URI (representing another resource) or a literal value (such as a string or number), amongst others.\n\nAn example of a triple stating “Miguel de Cervantes is author of El Quijote” would look like this:\n\nSubject: &lt;http://www.wikidata.org/entity/Q5682&gt; (Wikidata URI reference to Miguel de Cervantes)\nPredicate: &lt;http://purl.org/dc/terms/creator&gt; (Dublin Core URI term for creator/author)\nObject: &lt;http://www.wikidata.org/entity/Q480&gt; (Wikidata URI reference to the book El Quijote)\n\nIn a 2020 survey of LIBER members, the LIBER Linked Open Data Working Group identified GeoNames, VIAF, ISNI, and Wikidata as the most frequently used datasets by libraries to enrich their catalogues:\n\n\n\nName of the Dataset\nDescription\n\n\n\n\nGeoNames\nContains over 25 million geographical names and consists of over 11 million unique features whereof 4.8 million populated places and 13 million alternate names.\n\n\nISNI\nLibrary of Congress’ Linked Data Services with over 50 vocabularies.\n\n\nVIAF\nOCLC’s Virtual Authority file (VIAF), an aggregation of multiple authority files from different countries and regions.\n\n\nWikidata\nWikidata is a free and open knowledge base that can be read and edited by both humans and machines. Wikidata acts as central storage for the structured data of its Wikimedia sister projects including Wikipedia, Wikisource and others.\n\n\n\nThere are many more datasets that can be used depending on your needs (see Linked Data Survey (oclc.org); some examples of advanced data models are Bibliographic Framework (BIBFRAME) and Library Reference Model (LRM). In addition, the lod-cloud provides more than one thousand LOD repositories classified by categories and based on different domains such as geography and government.\nSo let’s go back to our triple that describes the relationship between the resource “Miguel de Cervantes” and the book of “El Quijote”. When different triples share the same URI for a subject, predicate, or object, they create a connection. For example:\nTriple 1: Miguel de Cervantes is author of El Quijote\n\nSubject: &lt;http://www.wikidata.org/entity/Q5682&gt; (Wikidata identifier for the author Miguel de Cervantes)\nPredicate: &lt;https://schema.org/author&gt; (Schema.org term for creator/author)\nObject: &lt;http://www.wikidata.org/entity/Q480&gt; (Wikidata identifier for the work El Quijote)\n\nTriple 2: El Quijote is a work of Spanish Literature\n\nSubject: &lt;http://www.wikidata.org/entity/Q480&gt; (Wikidata identifier for the work El Quijote)\nPredicate: &lt;https://schema.org/about&gt; (Schema.org term for subject)\nObject: &lt;http://dbpedia.org/resource/Spanish_literature&gt; (DBpedia identifier for Spanish literature)\n\nHere, the object of the first triple (&lt;http://www.wikidata.org/entity/Q480&gt;) is the subject of the second triple, linking information about the book to information about its subject matter. So an example catalogue record combining many triples then might look like:\n&lt;http://example.org/catalogue/El_Quijote&gt; &gt;rdf:type schema:Book; schema:name “El_Quijote”; schema:author &lt;http://www.wikidata.org/entity/Q5682&gt;; schema:genre &lt;http://dbpedia.org/resource/Novel&gt;; schema:inLanguage &lt;http://id.loc.gov/vocabulary/iso639-1/es&gt;; schema:datePublished “1605”; schema:about &lt;http://dbpedia.org/resource/Spanish_literature&gt;; schema:about &lt;http://dbpedia.org/resource/Spanish_Golden_Age&gt;; schema:sameAs &lt;http://dbpedia.org/resource/Don_Quixote&gt;.\n&lt;http://www.wikidata.org/entity/Q5682&gt; &gt;rdf:type schema:Person; schema:name “Miguel de Cervantes”; schema:birthPlace &lt;http://dbpedia.org/resource/Alcala_de_Henares&gt;; schema:birthDate “1547-09-29”.\n&lt;http://dbpedia.org/resource/Alcala_de_Henares**&gt; &gt;rdf:type schema:Place; schema:name “Alcalá de Henares”; geo:country &lt;http://sws.geonames.org/2510769/&gt;.\n&lt;http://sws.geonames.org/2510769/&gt; &gt;rdf:type schema:Country; schema:name “Spain” .",
    "crumbs": [
      "Topic Guides",
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "lod.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Linked Open Data in Library Use Today",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nGLAM institutions and in particular, libraries, have played a leading role in the publication of their data, primarily collections metadata, as LOD and using them including:\n\nBibliothèque nationale de France\nBiblioteca Virtual Miguel de Cervantes\nBritish Library\nEuropeana\nLibrary of Congress\nNational Library of Scotland\nNational Library of Spain\n\nAdditional examples from other related domains such as museums and Digital Humanities initiatives are the Rijksmuseum and Smithsonian American Art Museum, and Linked Open Data Infrastructure for Digital Humanities in Finland (LODI4DH).\nThe benefits of the publishing and use of the Semantic Web and LOD are:\n\nSemantic Enrichment: LOD helps libraries improve searchability and enables more precise queries by enriching existing catalogue records. Libraries have started to enrich their catalogues with external LOD repositories and vocabularies in order to provide additional contextual information that may be missing from your own catalogue (e.g., author nationalities (VIAF), geographic coordinates (GeoNames) relating to birth places of authors, or related subjects (Library of Congress Subject Headings). As in the example above a catalogue record for the book “El Quijote,” could be enriched with metadata about the author, language, publication date, related literary movements, and geographical information, all connected through LOD triples.\nInterconnectedness: LOD allows libraries to link their data with other rich datasets, creating a web of interconnected information. This enables users to discover related resources beyond their own library’s holdings. For example: a library could link their catalogue data with other LOD repositories, to enhance search results. Searching for “El Quijote” in the catalogue could return results not only from their own collection but also from other institutions that use LOD.\nIncreased Visibility: By publishing data as LOD, institutions can increase their visibility on the web as researchers, developers, and other institutions can easily find and reuse library data. For example: Adding information about a rare copy of El Quijote in your collection to Wikidata would aid its discovery through Wikipedia articles (Libraries and Wikidata: Using linked data to expand access to library collections worldwide – Wiki Education).\nInnovation: LOD encourages creative applications and tools. Developers can build new services, visualisations, and applications using linked library data. For example: LOD allows the creation of new types of visualisations, such as timelines, maps and graph charts that can be useful to gain insight, in some cases without the need to install additional software thanks to the use of APIs. Some examples include:\n\na tutorial in Spanish to create map visualisations based on Wikidata and using several data repositories (e.g., members of the International GLAM Labs Community) as content\nthe exploration of machine-readable visual configurations to browse LOD repositories provided by Cultural Heritage institutions, including libraries, in the form of Jupyter Notebooks\na map representing the geographic locations mentioned in the metadata provided by a corpus of historical documents and paintings.\n\n\nLinked Open Data can be queried and accessed with SPARQL, an RDF query language. However, though there are many benefits, it’s worth being aware that the use of APIs based on SPARQL can be complex for less technical users since they need to understand how the data is modelled as well as be able to type a query. In addition, data quality has become crucial and several initiatives are focused on the assessment of the data quality provided by the catalogues.\n\nCase Study: Manuscripts on Wikidata: the state of the art? | by Martin L Poulter | Medium\nThis example shows how to use Wikidata, a community-driven approach based on the Semantic Web and LOD that enables volunteers to edit the metadata, to describe manuscripts. It shows the expressivity of the vocabulary provided by Wikidata and the benefits of using Wikidata as a repository in terms of visibility and reuse.",
    "crumbs": [
      "Topic Guides",
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "lod.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Linked Open Data in Library Use Today",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nAs part of my National Research Librarian’s fellowship at National Library of Scotland exploring the adoption of Semantic Web technologies to transform, enrich and assess the Data Foundry’s digital collections, I created a collection of Jupyter Notebooks that enables users to:\n\nunderstand the benefits of the adoption of the Semantic Web;\ncreate an RDF repository from a traditional dataset;\nenrich a dataset with external repositories such as Wikidata;\nreproduce the analysis and visualisations based on the datasets created.\n\nI can also highly recommend starting with this Introduction to the Principles of Linked Open Data | Programming Historian tutorial which gives a great walk through creating linked open data and includes an activity for using SPARQL to query LOD.\nThe course about Linked Open Data in cultural heritage collections, developed at Leiden University also includes a tutorial about a number of tools that can be used to create and to publish LOD. More specifically, it contains discussions of the LDwizard and CLARIAH Data Legend tool ‘COW’.\nTo be able to retrieve and analyse Linked Open Data, you need to know how to build SPARQL queries. The following course can be helpful:\n\nIntroduction to SPARQL\n\nExamples of SPARQL queries used to collect and analyse data from heritage institutions can be found in the notebooks below:\n\nThe Europeana SPARQL endpoint\nWikidata\nShort Title Catalogue of the Netherlands\nThe Dutch Institute for Art History",
    "crumbs": [
      "Topic Guides",
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#recommended-readingviewing",
    "href": "lod.html#recommended-readingviewing",
    "title": "Linked Open Data in Library Use Today",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nIf you are interested in learning more about LOD in terms of how to transform traditional bibliographic information into the Semantic Web, check out my research work performed as part of a fellowship at the National Library of Scotland in order to publish digital collections as LOD.\nI can also recommend the excellent Best Practices for Library Linked Open Data (LOD) guide published by the LIBER Linked Open Data (LOD) Working Group in 2021 which outlines in detail six steps for publishing library linked data.\n\nSome examples of research articles to read providing additional details and information include:\n\nLinked Open Data: Impressions & Challenges Among Europe’s Research Libraries\nTowards a semantic approach in GLAM Labs: The case of the Data Foundry at the National Library of Scotland\nAn automatic data quality approach to assess semantic data from cultural heritage institutions\nA Shape Expression approach for assessing the quality of Linked Open Data in libraries\n\nYou can also find innovative ideas in the research articles published at the Semantic Web Journal.",
    "crumbs": [
      "Topic Guides",
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#finding-communities-of-practice",
    "href": "lod.html#finding-communities-of-practice",
    "title": "Linked Open Data in Library Use Today",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nThe LD4 Community is a community of practice for linked data in libraries.\nLinked Art is a community working together to create a shared model based on LOD to describe cultural heritage with a particular focus on art.\nCode4Lib is a community effort including a mailing list and a journal providing open articles based on the library domain and including LOD.",
    "crumbs": [
      "Topic Guides",
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "CONTACT",
    "section": "",
    "text": "CONTACT\nIf you’d like to ask a question of the project team you can please send an email to Nora McGregor, nora.mcgregor@bl.uk or any of Co-Chairs and select Members of the collaborating WGs listed here who act as the core project delivery team and editors of this resource.\nJodie Double, Editor DSDCH\nPéter Király, Editor DSLib\nNora McGregor, Editor DSDCH\nPeter Verhaar, Editor DSLib"
  },
  {
    "objectID": "copyright.html",
    "href": "copyright.html",
    "title": "Copyright & Licensing: Current context and considerations for researchers and libraries using AI in research today",
    "section": "",
    "text": "This guide aims to provide library professionals at European research institutions, particularly those supporting or undertaking activities that combines AI tools and methods and digital cultural heritage collections and data, with a brief overview of the current copyright and licensing context for such research today. \nIf it’s been awhile since you’ve had a look at copyright rules closely, let’s take a minute to cover the basics so we have a good foundation for understanding the more complex considerations of copyright in an AI context today.\nEqually, if you feel well versed in the basics already and are keen to dive straight into the AI context, feel free to hop on down to the Relevance to the Library section.\n\n\n“Copyright is a legal term used to describe the rights that creators have over their literary and artistic works. Works covered by copyright range from books, music, paintings, sculpture, and films, to computer programs, databases, advertisements, maps, and technical drawings.” WIPO\nCopyright is a form of intellectual property intended to protect original works as they are created and recorded. As soon as an original work is created, it is protected by copyright and there is no need to formally register it. In some countries registration may increase authors’ rights for compensation in case of infringement.\nThe purpose of copyright is to protect and reward creators for the works they have produced. Copyright consists of economic rights - the right of reproduction and the right to make a work available to the public- and moral rights - the right to be identified as the author and the right to prevent mistreatment.\nTo qualify for protection the work must be original in the sense that it is an author’s own intellectual creation, i.e. it reflects the author’s personality, the author was able to express their creative abilities in the production of the work by making free and creative choices.\n\n\nCopyright protects the artistic or literary form, but not the subject matter. The work that is created is protected, but not to the ideas or theories/ concepts represented. The ideas vs expression debate is a central part of the copyright discourse when looking to build on or reuse the work or ideas of another. If you are using the work itself this is likely to involve copyright issues, but if you are looking to use the ideas rather than the work this may not involve copyright issues at all.\n\n\n\nCopyright is finite and lasts for a set duration based on the type of work and where it was created.\nIn Europe and USA this period is 70 years after the author dies. According to the international Berne Convention the minimum protection period is the life of the author plus 50 years after their death. Copies or reproductions of works where copyright expired do not get protection in EU or USA:\nWhen copyright expires it becomes Public Domain however given the differences in duration across borders a work may be Public Domain in one country yet still under copyright in another.\nFor example, Canada has a copyright duration of 50 years after death for literary works whereas in France it’s 70. In Canada the work would not require permission for certain uses but in France those same uses would still require a licence or an exception.\nAnother example is the UK. “Some works are protected in the UK until 31st December 2039, even where the author died perhaps hundreds of years ago. This is also known as the ‘2039’ rule and applies to literary, dramatic and musical works, but not artistic works other than photographs and engravings. Other rules apply for photographs, films and sound recordings.”\nWhen a work enters the Public Domain it is free to be used by anyone in any way without the permission of the owner. When copyright expires also the moral rights of the creator expire, so works can be altered and used without crediting the author. However, many countries have given public authorities, such as the Ministry of Education/ Culture the possibility to protect the ‘educational and cultural value’ of works that are in the public domain, Italy for example.\n\n\n\nOwnership is important as it determines who has control over how and when a work is used, as well as who can access and use it. Owners, also known as rights holders, have the right to control who can access the work, how they can use it, if they can make copies, how it is performed or communicated to the public. They can licence these permissions to use to a licensee or they can sell these rights completely to another party. Ownership is a requirement for licensing, for example for licensing with Creative Commons licences.\nCopyright is an intangible asset which is separate to possession of a physical work such as a painting or book and transferring physical possession does not include any transfer of copyright.\nUsing a work without permission from the copyright owner, usually in the form of a licence, is a copyright infringement, unless an exception applies.\nCopyright ownership is granted to the creator of the work in the EU automatically by copyright legislation. However, this can be changed by contract. In the UK for example, the law says the creator is the owner but if a work was created in the course of employment, the employer will own the copyright unless the employment contract says otherwise. The US has a work-for-hire clause in the copyright legislation so that copyright is born to the employer if not otherwise agreed. Even ownership is not quite straightforward and local knowledge is important.\n\n\n\nOwners of copyright works are able to apply any licensing conditions they wish to their work, however research funders may make it a condition of a grant that certain outputs arising from funded research must be released openly. Where this is the case, they will communicate their preferred licensing choice.\nThe most common licences used are Creative Commons licences. Creative Commons licences are intended to pre-approve certain uses meaning a user can make use of a work within the terms of those licence conditions. If a user wishes to make other uses they will need to contact the owner in the usual way and negotiate separate permissions. If a user breaches the Creative Commons terms they can be sued for infringement by the owner.\nCreative Commons licences do not impact on the ownership status of the work- they are simply licences. Only the copyright owner of a work has the authority to grant a Creative Commons licence.\nCreative Commons licences can be very open permitting any reuse provided the creator and the licence are cited, or they can be closer to a traditional ‘all rights reserved’ approach which might enable access to a work but any reuse is limited in nature. The licence terms are binding so any user must mention the name of the author and the work as shown in the licence, and if there are additional terms such as Non-Commercial or Share-Alike these have to be complied with.\nCreative Commons has a comprehensive website with guidance but it is highly likely that research institutions and libraries will have other additional guidance on this too.\n\n\n\n\nCopyright protected materials are central to research activity whether that is in the form of text, music, images or code. Data itself is not protected by copyright, but databases can be protected as catalogue or sui generis database, as noted above.\nResearchers will produce copyright protected works when writing articles, monographs or code. Researchers will need protection for the right to be recognised as authors and also as owners of copyright. Research ethics requires contributors to be recognised in a research output, even if they are not owners or authors in a copyright sense. Good scientific research requires contributor roles to be defined and mentioned in research outputs and CRediT – Contributor Role Taxonomy (niso.org) describes 14 roles that can be used when defining contributor roles. Contributor roles should be agreed before sending a manuscript to a publisher.\nResearchers will use works created by others- either as a fundamental part of their research methodology or during publication. Where exceptions like those for non-commercial research or non-commercial data mining do not apply, ensuring that licences are in place that allow for the uses within the research activity is essential. The use of research data and the management of data related rights should be considered and outlined within the data management plan which must also take into account how the research data are to be archived and used, as well as how the authors of the data are attributed in compliance with research ethics.\n\n\nIn order to use the copyright protected work, either permission from the owner, agreements covering the uses, or legislation allowing the use is needed.\nCopyright legislation tries to strike a balance between protecting the freedom of sciences, expression, and information, and the protection of the copyright owner. This means that while the owner of the rights can control it, there are exceptions to those rights which enable some uses without the need for the owner’s permission. These are known as exceptions, or user rights.\nCopyright exceptions include for example certain educational uses, the provision of inter library loans by libraries, text and data mining for non-commercial scientific research purposes and commercial purposes, creating accessible copies of works for those with disabilities\nCopyright laws vary by country and, while EU law is largely harmonised, there are differences between member states concerning the copyright exceptions. Understanding local national law exceptions for research and education is important.\n\n\n\nUsing archival materials can raise some unusual situations which mean the normal copyright rules don’t apply. Below are a couple of examples of unusual copyright scenarios:\nWorks of art on public display The rules around works of art on public display vary by country. In some EU countries the so-called ’Freedom of Panorama’ rules dictate what can and cannot be done with such works for commercial purposes. Italy for example controls commercial uses of works of cultural importance regardless of their copyright status.\nContractual issues\nWhile works in the public domain may be free from copyright access to them and their use may be controlled via contract law. Many museums and galleries apply terms and conditions to the use of digital surrogates which apply irrespective of the copyright status.\nDatabase Rights\nThe EU sui generis database protection and the Nordic catalogue protection intend to protect the investment in a database. A database usually does not have the original artistic or literary form that protection of a database as an original work would require but it does involve creative choices in the structure layout and data fields being collected. While the data items themselves may not have copyright the overall database will be protected by the database right. Ownership of database rights may vary by law or contract. For example in the Nordic version these rights are given to the university as employer, not to the researcher as the employee.\nUnpublished works\nThere are specific rules surrounding the copyright of works that have never been ‘communicated to the public’ or they have never been publicly accessible.\nOrphan works\nWhere the owner of the rights in a copyright work is known but cannot be traced they are classed as orphan work. Within the EU the Orphan works Licensing Scheme is a EU wide licensing mechanism that gives assurance to users of these works subject to the payment of a fee.\nOut of commerce works\nWhere a work is no longer commercially exploited by the owner, or publisher for example, there are EU rules around how a user might use these items.\nEthical authorship\nCopyright authorship and who are authors in a scientific publication differ. When listing authors and those persons who have contributed to the research and to the publication should be mentioned, contributor roles should be defined, and a tool for this is the CRediT – Contributor Role Taxonomy (niso.org).\n\n\n\n\nLicensing agreements are a central type of legal contract concerning copyright works. Use of published content within institutions will be covered by licences. Access to ebooks, e-journals, databases etc are all covered by licences. It is important to know and understand the terms under which access to content is provided so that users can use the content without risk of breaching the licence terms.\nMany of the permitted uses within licences closely mirror the copyright exceptions mentioned above, however they provide a clarity and certainty that the exceptions may not.\nIn general, licences give institutions, their staff and students, permission to use the licensed content for specific purposes. These purposes are usually limited to education and non-commercial research activities, i.e. the students, researchers and educators can use the content for their purposes but HR or financial teams that are not directly engaged in research or teaching delivery cannot.\nThe terms of the licences will vary but they frequently allow for saving or printing of parts of the licensed works. Some licences will allow users to include extracts within teaching materials or even within publications. How much can be used will also vary.\nIt is important that these purposes and licence terms are clearly understood and that the use they expect to make of the content is expressly permitted within the licence terms. For example if an institution has an active research interest in data mining, licences that seek to prevent or restrict this activity will be problematic. Where licence terms are unclear this should be raised with the provider. Where they conflict with the legal exceptions in law this should also be queried.\nInstitutions may also rely on licences provided by collective licensing societies- official bodies that represent a group of authors, publishers or rights holders. The licences may cover things like photocopying and scanning of printed works, showing broadcast television programmes or playing recorded music. Again understanding the terms, the uses and obligations is important.",
    "crumbs": [
      "Topic Guides",
      "Copyright & Licensing: Current context and considerations for researchers and libraries using AI in research today"
    ]
  },
  {
    "objectID": "copyright.html#introduction",
    "href": "copyright.html#introduction",
    "title": "Copyright & Licensing: Current context and considerations for researchers and libraries using AI in research today",
    "section": "",
    "text": "This guide aims to provide library professionals at European research institutions, particularly those supporting or undertaking activities that combines AI tools and methods and digital cultural heritage collections and data, with a brief overview of the current copyright and licensing context for such research today. \nIf it’s been awhile since you’ve had a look at copyright rules closely, let’s take a minute to cover the basics so we have a good foundation for understanding the more complex considerations of copyright in an AI context today.\nEqually, if you feel well versed in the basics already and are keen to dive straight into the AI context, feel free to hop on down to the Relevance to the Library section.\n\n\n“Copyright is a legal term used to describe the rights that creators have over their literary and artistic works. Works covered by copyright range from books, music, paintings, sculpture, and films, to computer programs, databases, advertisements, maps, and technical drawings.” WIPO\nCopyright is a form of intellectual property intended to protect original works as they are created and recorded. As soon as an original work is created, it is protected by copyright and there is no need to formally register it. In some countries registration may increase authors’ rights for compensation in case of infringement.\nThe purpose of copyright is to protect and reward creators for the works they have produced. Copyright consists of economic rights - the right of reproduction and the right to make a work available to the public- and moral rights - the right to be identified as the author and the right to prevent mistreatment.\nTo qualify for protection the work must be original in the sense that it is an author’s own intellectual creation, i.e. it reflects the author’s personality, the author was able to express their creative abilities in the production of the work by making free and creative choices.\n\n\nCopyright protects the artistic or literary form, but not the subject matter. The work that is created is protected, but not to the ideas or theories/ concepts represented. The ideas vs expression debate is a central part of the copyright discourse when looking to build on or reuse the work or ideas of another. If you are using the work itself this is likely to involve copyright issues, but if you are looking to use the ideas rather than the work this may not involve copyright issues at all.\n\n\n\nCopyright is finite and lasts for a set duration based on the type of work and where it was created.\nIn Europe and USA this period is 70 years after the author dies. According to the international Berne Convention the minimum protection period is the life of the author plus 50 years after their death. Copies or reproductions of works where copyright expired do not get protection in EU or USA:\nWhen copyright expires it becomes Public Domain however given the differences in duration across borders a work may be Public Domain in one country yet still under copyright in another.\nFor example, Canada has a copyright duration of 50 years after death for literary works whereas in France it’s 70. In Canada the work would not require permission for certain uses but in France those same uses would still require a licence or an exception.\nAnother example is the UK. “Some works are protected in the UK until 31st December 2039, even where the author died perhaps hundreds of years ago. This is also known as the ‘2039’ rule and applies to literary, dramatic and musical works, but not artistic works other than photographs and engravings. Other rules apply for photographs, films and sound recordings.”\nWhen a work enters the Public Domain it is free to be used by anyone in any way without the permission of the owner. When copyright expires also the moral rights of the creator expire, so works can be altered and used without crediting the author. However, many countries have given public authorities, such as the Ministry of Education/ Culture the possibility to protect the ‘educational and cultural value’ of works that are in the public domain, Italy for example.\n\n\n\nOwnership is important as it determines who has control over how and when a work is used, as well as who can access and use it. Owners, also known as rights holders, have the right to control who can access the work, how they can use it, if they can make copies, how it is performed or communicated to the public. They can licence these permissions to use to a licensee or they can sell these rights completely to another party. Ownership is a requirement for licensing, for example for licensing with Creative Commons licences.\nCopyright is an intangible asset which is separate to possession of a physical work such as a painting or book and transferring physical possession does not include any transfer of copyright.\nUsing a work without permission from the copyright owner, usually in the form of a licence, is a copyright infringement, unless an exception applies.\nCopyright ownership is granted to the creator of the work in the EU automatically by copyright legislation. However, this can be changed by contract. In the UK for example, the law says the creator is the owner but if a work was created in the course of employment, the employer will own the copyright unless the employment contract says otherwise. The US has a work-for-hire clause in the copyright legislation so that copyright is born to the employer if not otherwise agreed. Even ownership is not quite straightforward and local knowledge is important.\n\n\n\nOwners of copyright works are able to apply any licensing conditions they wish to their work, however research funders may make it a condition of a grant that certain outputs arising from funded research must be released openly. Where this is the case, they will communicate their preferred licensing choice.\nThe most common licences used are Creative Commons licences. Creative Commons licences are intended to pre-approve certain uses meaning a user can make use of a work within the terms of those licence conditions. If a user wishes to make other uses they will need to contact the owner in the usual way and negotiate separate permissions. If a user breaches the Creative Commons terms they can be sued for infringement by the owner.\nCreative Commons licences do not impact on the ownership status of the work- they are simply licences. Only the copyright owner of a work has the authority to grant a Creative Commons licence.\nCreative Commons licences can be very open permitting any reuse provided the creator and the licence are cited, or they can be closer to a traditional ‘all rights reserved’ approach which might enable access to a work but any reuse is limited in nature. The licence terms are binding so any user must mention the name of the author and the work as shown in the licence, and if there are additional terms such as Non-Commercial or Share-Alike these have to be complied with.\nCreative Commons has a comprehensive website with guidance but it is highly likely that research institutions and libraries will have other additional guidance on this too.\n\n\n\n\nCopyright protected materials are central to research activity whether that is in the form of text, music, images or code. Data itself is not protected by copyright, but databases can be protected as catalogue or sui generis database, as noted above.\nResearchers will produce copyright protected works when writing articles, monographs or code. Researchers will need protection for the right to be recognised as authors and also as owners of copyright. Research ethics requires contributors to be recognised in a research output, even if they are not owners or authors in a copyright sense. Good scientific research requires contributor roles to be defined and mentioned in research outputs and CRediT – Contributor Role Taxonomy (niso.org) describes 14 roles that can be used when defining contributor roles. Contributor roles should be agreed before sending a manuscript to a publisher.\nResearchers will use works created by others- either as a fundamental part of their research methodology or during publication. Where exceptions like those for non-commercial research or non-commercial data mining do not apply, ensuring that licences are in place that allow for the uses within the research activity is essential. The use of research data and the management of data related rights should be considered and outlined within the data management plan which must also take into account how the research data are to be archived and used, as well as how the authors of the data are attributed in compliance with research ethics.\n\n\nIn order to use the copyright protected work, either permission from the owner, agreements covering the uses, or legislation allowing the use is needed.\nCopyright legislation tries to strike a balance between protecting the freedom of sciences, expression, and information, and the protection of the copyright owner. This means that while the owner of the rights can control it, there are exceptions to those rights which enable some uses without the need for the owner’s permission. These are known as exceptions, or user rights.\nCopyright exceptions include for example certain educational uses, the provision of inter library loans by libraries, text and data mining for non-commercial scientific research purposes and commercial purposes, creating accessible copies of works for those with disabilities\nCopyright laws vary by country and, while EU law is largely harmonised, there are differences between member states concerning the copyright exceptions. Understanding local national law exceptions for research and education is important.\n\n\n\nUsing archival materials can raise some unusual situations which mean the normal copyright rules don’t apply. Below are a couple of examples of unusual copyright scenarios:\nWorks of art on public display The rules around works of art on public display vary by country. In some EU countries the so-called ’Freedom of Panorama’ rules dictate what can and cannot be done with such works for commercial purposes. Italy for example controls commercial uses of works of cultural importance regardless of their copyright status.\nContractual issues\nWhile works in the public domain may be free from copyright access to them and their use may be controlled via contract law. Many museums and galleries apply terms and conditions to the use of digital surrogates which apply irrespective of the copyright status.\nDatabase Rights\nThe EU sui generis database protection and the Nordic catalogue protection intend to protect the investment in a database. A database usually does not have the original artistic or literary form that protection of a database as an original work would require but it does involve creative choices in the structure layout and data fields being collected. While the data items themselves may not have copyright the overall database will be protected by the database right. Ownership of database rights may vary by law or contract. For example in the Nordic version these rights are given to the university as employer, not to the researcher as the employee.\nUnpublished works\nThere are specific rules surrounding the copyright of works that have never been ‘communicated to the public’ or they have never been publicly accessible.\nOrphan works\nWhere the owner of the rights in a copyright work is known but cannot be traced they are classed as orphan work. Within the EU the Orphan works Licensing Scheme is a EU wide licensing mechanism that gives assurance to users of these works subject to the payment of a fee.\nOut of commerce works\nWhere a work is no longer commercially exploited by the owner, or publisher for example, there are EU rules around how a user might use these items.\nEthical authorship\nCopyright authorship and who are authors in a scientific publication differ. When listing authors and those persons who have contributed to the research and to the publication should be mentioned, contributor roles should be defined, and a tool for this is the CRediT – Contributor Role Taxonomy (niso.org).\n\n\n\n\nLicensing agreements are a central type of legal contract concerning copyright works. Use of published content within institutions will be covered by licences. Access to ebooks, e-journals, databases etc are all covered by licences. It is important to know and understand the terms under which access to content is provided so that users can use the content without risk of breaching the licence terms.\nMany of the permitted uses within licences closely mirror the copyright exceptions mentioned above, however they provide a clarity and certainty that the exceptions may not.\nIn general, licences give institutions, their staff and students, permission to use the licensed content for specific purposes. These purposes are usually limited to education and non-commercial research activities, i.e. the students, researchers and educators can use the content for their purposes but HR or financial teams that are not directly engaged in research or teaching delivery cannot.\nThe terms of the licences will vary but they frequently allow for saving or printing of parts of the licensed works. Some licences will allow users to include extracts within teaching materials or even within publications. How much can be used will also vary.\nIt is important that these purposes and licence terms are clearly understood and that the use they expect to make of the content is expressly permitted within the licence terms. For example if an institution has an active research interest in data mining, licences that seek to prevent or restrict this activity will be problematic. Where licence terms are unclear this should be raised with the provider. Where they conflict with the legal exceptions in law this should also be queried.\nInstitutions may also rely on licences provided by collective licensing societies- official bodies that represent a group of authors, publishers or rights holders. The licences may cover things like photocopying and scanning of printed works, showing broadcast television programmes or playing recorded music. Again understanding the terms, the uses and obligations is important.",
    "crumbs": [
      "Topic Guides",
      "Copyright & Licensing: Current context and considerations for researchers and libraries using AI in research today"
    ]
  },
  {
    "objectID": "copyright.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "copyright.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Copyright & Licensing: Current context and considerations for researchers and libraries using AI in research today",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nLibraries and cultural heritage institutions today don’t just support academics to undertake computational research by providing guidance and increasingly computational access to digital collections, they undertake digital research in their own right as part of library work, using existing models and developing new ones for analysing digital collections at scale for metadata improvement and enhancement and a whole host of other applications. For European institutions understanding TDM rights granted by the Directive on Copyright in the Digital Single Market (later the DSM or Directive (EU) 2019/790) and the EU AI Act is important when undertaking this work, as well as keeping up to date on your local contexts which aligns with these but in some cases may differ, such as in the UK.\nThis is an evolving area and our aim here is to provide a brief overview of the current copyright and licensing context for European researchers and institutions using AI tools and methods in research today.\n\nText and Data Mining (TDM)\nText and data mining (TDM) is a common research technique that allows researchers, and research organisations to analyse large volumes of data using modern computing power. Under EU law the Directive on Copyright in the Digital Single Market (later the DSM or Directive (EU) 2019/790) introduced TDM copyright exceptions. TDM is defined Article 2(2) as:\n\n‘text and data mining’ means any automated analytical technique aimed at analysing text and data in digital form in order to generate information which includes but is not limited to patterns, trends and correlations;\n\nTDM is a core part of machine learning and artificial intelligence (AI) technologies. It could include the harvesting and scrapping of online data sources, or digitising printing items so that they can be read by computers.\nDSM directive Article 3 allows for the use of copyright works in TDM activities for the purposes of non-commercial scientific research by research organisations and cultural heritage institutions provided they have lawful access to the content. This exception is mandatory and rights holders can not override it using agreements or technical measures when EU legislation is applicable. If the agreement is done with an organisation outside EU and governed by legislation other than EU legislation, researchers should contact their legal department for advice on how EU legislation exceptions can be applied to contractual obligations (see Regulation (EC) N o 593/2008 of the European Parliament and of the Council of 17 June 2008 on the law applicable to contractual obligations (Rome I). It is advisable to define in agreements with US/other non-EU companies that mandatory exceptions to copyright in national EU member state legislation apply to the agreement, despite being otherwise governed by US legislation.\nDSM directive Article 4 allows for text and data mining for any purpose by any organisation or person, provided they have lawful access. However, right holders are permitted to opt out of this broad exception as defined using a machine-readable opt-out, as defined in Article 4(3):3.\n\nthe exception or limitation provided for in paragraph 1 shall apply on condition that the use of works and other subject matter referred to in that paragraph has not been expressly reserved by their rightholders in an appropriate manner, such as m_achine-readable means in the case of content made publicly available online.\n\nWhat is the way to opt out in “appropriate manner, such as machine-readable means” is now to be discussed in a court case in Germany Machine readable or not? - notes on the hearing in LAION e.v. vs Kneschke - Kluwer Copyright Blog (kluweriplaw.com). District Court of Hamburg, Germany has on 27.9.2024 made a decision in the first European case that examines the relying on the the TDM exception for the purpose of training generative AI models\nAs the DSM is a directive, member states were able to implement certain elements as they see fit and this has led to some disjointed approaches across the EU with some countries taking different approaches. Ireland for example requires that an author is entitled to be informed that the copy has been made for text and data mining purposes and ask for details about the steps taken to ensure the security of the works copied (see Copyright and Related Rights Act, 2000 (as amended) sections 53A and 53B). Similar requirement of transparency regarding the materials used for AI training of generative AI models is a key point in the AI Act; the sections concerning generative AI are applicable from August 2025.\n\nExample 1\nThe library is a partner on a non-commercial collaborative research project at a university where a research team wants to engage in linguistic analysis, using computational methods, of EU newspaper articles the library has made available publicly online. In order to complete the research they need to extract all of the articles during a certain period to build a corpus of data sourced from different EU newspapers. Once the corpus is complete the researcher wants to use a computer program to perform the analysis. As the data is sourced from different newspaper websites, there is some data cleaning required.\nAll of the copying of the articles for the purposes of this research is permitted as is any data cleaning under the terms of the TDM exception above. The research team is permitted to carry out any steps necessary to obtain and format the data to enable them to complete the analysis using computational methods. If the researchers want to use printed articles those could be digitised and made machine-readable too.\n\n\nExample 2\nNow the corpus is complete the researcher wants to use a free online AI service to complete the analysis.\nThe model’s terms say that the user of the service declares owning all of the input data they provide and grants a licence to the model provider which allows the model provider to retain the input data and use it as a part of the training data. The DSM directive Article 3 exception or its national legislations do not allow this granting of licence to commercial companies to the input data, so the researcher can only use paid services, which the university has purchased, and which have terms that allow the input data to stay on the university VPN.\nWhile the mandatory exception would cover the collection and analysis of the data for scientific research purposes, it does not enable the researcher to own the data nor does the exception give the researcher the authority to grant permissions for non-scientific training data uses. If the researcher would give the material to the AI system provider, contrary to the scope of the exception legislation, then the AI system provider would not have acquired the data lawfully, so the general text and data mining exception would not apply.\n\n\n\nAI Act and Copyright\nThe EU AI Act legislates obligations for providers and deployers of artificial intelligence (AI) systems and AI models and includes articles concerning the copyright protected content as training data of AI models. The AI Act confirms that text and data mining exceptions support the use of content for training AI models. AI Act Article 53 contains obligations for providers of general-purpose AI models:\n\n\nArticle 53(c) the provider of a general purpose AI model must put in place a policy to comply with Union law on copyright and related rights, and in particular to identify and comply with, including through state-of-the-art technologies, a reservation of rights (opt-out) expressed in Article 4(3) of Directive (EU) 2019/790 (DSM directive).\nArticle 53(d) the provider of a general purpose AI model must draw up and make publicly available a sufficiently detailed summary about the content used for training of the general-purpose AI model, according to a template that will be provided by the EU AI Office.\n\n\nArticle 53 and other general-purpose AI sections of the AI Act are applied from August 2025 onwards.\nAI models that are used to create content, such as text or images, for example Midjourney or ChatGPT, are considered general-purpose AI models in the recitals of the AI Act.\n\n\nAI Act Article 3 (4) defines ‘provider’ as a natural or legal person, public authority, agency or other body that develops an AI system or a general-purpose AI model or that has an AI system or a general-purpose AI model developed and places it on the market or puts the AI system into service under its own name or trademark, whether for payment or free of charge.\n\n\nHowever, the AI Act is not directly applied to scientific research:\n\n\nArticle 2(6) The AI Act does not apply to AI systems or AI models, including their output, specifically developed and put into service for the sole purpose of scientific research and development.\n\n\nBut if the result of a research study is a general-purpose AI model, and it is placed on the market or put to use, other than for the sole purpose of scientific research, then the provider requirements apply, e.g. the provider needs to be able to list a sufficiently detailed summary of the content used for training of the general-purpose AI model. According to Article 2 (8) the AI Act does not apply to testing or development prior to placing it on the market, but it applies to real world testing of the general-purpose AI model.\nAI Act does not regulate artificial intelligence, which is a philosophical rather than legal term, but it regulates AI systems that are are defined in Article 3 (1):\n\n\n‘AI system’ means a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.\n\n\nThe autonomy of the AI systems makes it different from other tools that are controlled by the author and this autonomous aspect of generative AI has an effect when regarding the human authorship of the generative AI system output.\nAI Act Article 3 (33) defines that ‘input data’ means data provided to or directly acquired by an AI system on the basis of which the system produces an output. Both input data and output should be considered in the framework of copyright, so that neither are infringing copyright of third parties.\nWhen applying the exception rules allowing text and data mining, users of third party works as training data must bear in mind that the three step test limits all exception rules.\n\n\nDirective 2001/29 Article 5 (5) states that the exceptions and limitations of copyright can only be applied in certain special cases which do not conflict with a normal exploitation of the work or other subject-matter and do not unreasonably prejudice the legitimate interests of the rights holder. For example a generative AI model that would produce illustrations in the style of an illustrator could unreasonably prejudice the legitimate interests of that illustrator as rights holder.\n\n\n\n\nConsiderations for libraries and research projects using AI\nIf a research project intends to use copyright protected works as training data for AI models, researchers should consider how the text and data mining exception in EU legislation would allow the intended reproduction of works as training data. The recent court case from Hamburg District Court, considers scientific research to be a wide concept. If the research outputs are to be commercialised, consideration must also be given to the new EU AI Act and its requirement in Article 53 to document all copyright protected training data.\nIf researchers are using copyright protected works in existing third party AI tools, use should be balanced against the exceptions or licences covering the content they wish to use. Researchers should consider whether they are required to grant the third party tool permissions to use the content, and whether this is possible within the scope of the licence or exception. As the researcher may not own the content they wish to input, they may not have the authority to grant those permissions. This is especially important where third party tools develop their model using input data.\nIncreasingly, publishers are seeking to restrict the use of the licensed content as training data to train AI models or as input used in AI systems, while in other instances they are creating licencing agreements to allowing large technology companies access to scholarly content (see Generative AI Licensing Agreement Tracker - Ithaka S+R and An academic publisher has struck an AI data deal with Microsoft – without their authors’ knowledge). The ICOLC Statement on AI in Licensing offers a useful template to begin to push back to ensure researchers and institutions are able to use licensed content for non-commercial research at least. Of course institutions are heavily involved in the translation of research into commercial activity so the ICOLC statement is only of limited use, but it’s a start.\nConsideration should also be given to the impact of AI tools on library systems and the ingestion of content, openly licensed or otherwise, by generative AI tools. For instance, the KB restricts access to collections for training commercial AI | KB, National Library of the Netherlands and in January 2024 issued a Statement on commercial generative AI | KB, National Library of the Netherlands outlining their position “that commercial parties who crawl digital resources on websites on a large scale for training models, using applications such as ChatGPT, are not complying with the AI principles established by the KB in 2020.”\nThe harvesting of vast volumes of online content as training data for AI models is under increasing scrutiny, as is the harvesting of personal data. It should be noted that some right holders are actively seeking to prevent their content being analysed, accessed, or processed by any AI tools, regardless of whether they are “inhouse”, third party, secure/ protected/ ring fenced or otherwise. Different jurisdictions have differing legislation and court cases will further define AI and copyright legal questions over time.\n\n\nAI and original work protected by copyright\nAI is bringing a new urgency to the requirement of originality, which is a fundamental aspect of copyright. According to the Berne Convention for the Protection of Literary and Artistic Works, a copyright protected work has to reflect/contain the intellectual creation of the author. This requirement of originality is a fundamental part of what copyright protects and is a feature of national and EU legislation endorsed in many court cases.\n\nExample 3\nIn the US the person or company wishing to register copyright can apply for copyright registration from the US Copyright Office (USCO). USCO received and examined an application containing images that were created by generative AI program and service Midjourney. Users of Midjourney operate through “prompts” (text commands) which include the description of what Midjourney should generate. Midjourney does not interpret prompts as specific instructions to create a particular expressive result, but simply converts words and phrases into smaller tokens that are used for the training of data and to generate an image. The process is not controlled by the user because it is not possible to predict what Midjourney will create ahead of time.\nUSCO concluded that the images generated by Midjourney are not original works of authorship protected by copyright, since Midjourney generates images in an unpredictable way. The fact that Midjourney´s specific output cannot be predicted by users makes Midjourney different for copyright purposes from other tools used by artists for example editing tools and assistive tools that allow the choice of specific changes and include specific steps to control the final image by the user.\nIn the US District Court Of Columbia copyright case Thaler v. Perlmutter, the Court decided that human authorship is a fundamental requirement for copyright claim.\nIn the EU, courts have the same requirement of human authorship and require the ability of the author to control the creative process by making free choices. The Court of Justice of the European Union has considered the originality requirement in the cases C‑5/08 Infopaq and C-145/10 Painer defining that copyright can only apply to a work which is original in the sense that it is its author’s own intellectual creation. An intellectual creation is an author’s own if it reflects the author’s personality. That is the case if the author was able to express his creative abilities in the production of the work by making free and creative choices. Simple facsimile copies of original works, then would not be copyright works in their own right.\n\n\nExample 4\nDigital copies of historic artworks that are in the Public Domain are created to replicate the original as far as possible. The items are photographed and then the images are uploaded to a website.\nThese digital surrogates would not contain any originality as defined and would not attract a new, distinct copyright. The creator of the photograph is trying to copy the original, they are not making free creative choices. They are not creating their own creative outputs.\nThere is a parallel here with generative AI perhaps.\n\n\nExample 5\nIn the Czech decision by the Prague Municipal Court 11.10 2023, a person (claimant) filed a lawsuit against a law firm (defendant) after it published an image on its website without the claimant’s permission. The claimant had created an image with the generative AI service DALL-e. The image shows two hands signing a business contract, and it had been created by the person using the following text prompt : “create a visual representation of two parties signing a business agreement in a formal environment; for example, in a conference room or a law office in Prague. Show only the hands.”\nIn the Court’s opinion, the image cannot as a matter of principle be protected by copyright, because such an image is not the result of creative activity of a natural person – an author. The Court concluded that the image is not a copyright-protected work. The claimant argued that the image was created on the basis of his specific prompt, which justified the claimant’s copyright to the image. The Court considered that the prompt itself could only be regarded as a theme or idea for a work, neither of which can be protected by copyright. The Czech Copyright Act specifically lists themes, ideas and similar more abstract concepts as excluded from copyright protection, which is a principle generally recognised internationally.",
    "crumbs": [
      "Topic Guides",
      "Copyright & Licensing: Current context and considerations for researchers and libraries using AI in research today"
    ]
  },
  {
    "objectID": "copyright.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "copyright.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Copyright & Licensing: Current context and considerations for researchers and libraries using AI in research today",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nThis guide contains a ton of resources for learning about copyright but for something a little bit different, check out the Copyright Card Game or online quizzes such as European Copyright 10 Questions.",
    "crumbs": [
      "Topic Guides",
      "Copyright & Licensing: Current context and considerations for researchers and libraries using AI in research today"
    ]
  },
  {
    "objectID": "copyright.html#recommended-readingviewing",
    "href": "copyright.html#recommended-readingviewing",
    "title": "Copyright & Licensing: Current context and considerations for researchers and libraries using AI in research today",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nOn February 26 2025, the LIBER Copyright & Legal Matters Working Group hosted a webinar to explore the implications of the AI Act for scientific libraries and how they can prepare for its implementation. The session aimed to provide the attendees with a deeper understanding of the AI Act and you can watch it here:\n\n\n\nWatch the video\n\n\nCopyright and AI is a constantly moving area and it can be hard to keep up with the latest developments but we can recommend following the The IPKat (ipkitten.blogspot.com) and Kluwer Copyright Blog (kluweriplaw.com) and News from the LIBER Copyright & Legal Matters Working Group for excellent updates.\nMuch guidance is based on national law, so you will need to keep an eye on copyright legislation, which will provide an overview of the local picture. These international networks and organisations are also useful places for getting more in depth information:\n\nWIPO\nEU Intellectual Property Office\nCreative Commons\nKR21\nCommunia Association",
    "crumbs": [
      "Topic Guides",
      "Copyright & Licensing: Current context and considerations for researchers and libraries using AI in research today"
    ]
  },
  {
    "objectID": "copyright.html#finding-communities-of-practice",
    "href": "copyright.html#finding-communities-of-practice",
    "title": "Copyright & Licensing: Current context and considerations for researchers and libraries using AI in research today",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nIf you are employed in an institution that is a member of LIBER do join or reach out to the LIBER Copyright & Legal Matters Working Group, a group of librarians, lawyers, professors and communications professionals who monitor current European law and react to proposed changes, on behalf of libraries, archives, researchers and students.",
    "crumbs": [
      "Topic Guides",
      "Copyright & Licensing: Current context and considerations for researchers and libraries using AI in research today"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Over the past few decades, the development of excellent self-paced tutorials and training materials for undertaking digital scholarship and data science in libraries have proliferated online.\nFor library professionals who are relatively new to this area however, it can be hard to know where to begin! Without knowing a little bit about the context of how new technologies and methodologies are being deployed in research libraries, this dearth of resources can seem quite daunting. Even if you might have an idea of what learning you’d like to undertake, it can be difficult and time consuming to try and navigate the wealth of individual training resources out there on your own.\nDigital Scholarship & Data Science Topic Guides for Library Professionals (DS Topic Guides) are written for and by research library professionals across LIBER to help share some of our experience and knowledge and lower those barriers to entry for others!\nEach Topic Guide aims to provide a gentle, quick and curated introduction to the key topics and technologies we grapple with in Digital Scholarship and Data Science today:\n\nIntroduction to the topic\nRelevance to the library sector (Case studies/Use cases)\nHands-on activities and other self-guided tutorial(s)\nRecommended Reading & Viewing\nFinding Communities of Practice\n\nTake me to the Topic Guides"
  },
  {
    "objectID": "digitalmapping.html",
    "href": "digitalmapping.html",
    "title": "Digital Mapping",
    "section": "",
    "text": "Cultural Heritage collections are full of object stories. Objects move through time and space and encounter multiple people and other objects. Additionally, as explained in the guide on data visualisation, humans are better able to see and interpret data based on visualising them with colours and shapes.\nWhen we combine these two concepts, it is therefore not surprising that one of the most common visualisations for cultural heritage collections, and library collections specifically, are maps.\nHowever, creating a map or adding objects on a map are not neutral processes: maps reproduce ways of seeing, giving specific interpretations of the world. For example, a map created according to a Mercator projection (created by Gerardus Mercator in 1569) will expand the lands above the Equator, therefore making Madagascar as big as Great Britain while it is actually twice the size. This projection reflects a view of the world that is western-centric. Despite criticism, this projection is actually the base of Web Mercator.\nWeb Mercator is a de facto standard for Web Mapping and it is used by Google Maps, Bing Maps and OpenStreetMap, therefore continuing the reinforcement of the same way of seeing the world while hiding the fact that it is a way of seeing the world.\nRecognising that every map is not neutral and it is instead a cultural artefact is particularly important when approaching the digital mapping of collections as the maps may enhance their partiality.\nAt the same time, using maps as a visualisation tool has the potential both to make libraries aware of new and unheard stories within their collections, to connect collections from multiple institutions (see for example Heritage for All) and to identify gaps that may be worth exploring.\nBecause of their ubiquity, they have the potential to be easy to explore for different audiences, keeping in mind that accessibility and usability, as much as with other visualisations and interfaces, need to be kept at the centre of their development.\nKnowing who will use the maps and for what purpose is therefore essential as much as being aware of the interpretation that the map is proposing and how this interpretation is influenced by the adoption of a specific technology and how the process of mapping happened.\nFor example, visualising indigenous knowledge in a Web Mercator-based application may be problematic as the approach to place and its visualisation may be different than the one presented on some platforms and may require a discussion with communities on how to best represent their interpretation of place and their culture.\nTo digitally map collections, there are a few options available in terms of tools and softwares:\n\nFirst of all, there are specialistic software available such as QGIS and ArcGIS where geolocated data (that is data that have coordinates) can be visualised on maps chosen by the librarian/researcher and historical maps can be associated with coordinates.\nIt is possible to visualise geolocated data on online platforms designed for digital mapping that are not specifically for the Humanities, such as Google Maps.\nThere are online tools designed to visualise humanities data in multiple ways and have an option of visualising data on a map such as Recogito, where annotated places on a text can be visualised, or Palladio that is used to visualise data as networks.\nThere are generic visualisation tools such as Tableau, that has been explored in the data visualisation guide.\nFinally, it is worth mentioning the work the IIIF community in standardising the georeferencing of maps in IIIF and creating geospatial annotations.",
    "crumbs": [
      "Topic Guides",
      "Digital Mapping"
    ]
  },
  {
    "objectID": "digitalmapping.html#introduction",
    "href": "digitalmapping.html#introduction",
    "title": "Digital Mapping",
    "section": "",
    "text": "Cultural Heritage collections are full of object stories. Objects move through time and space and encounter multiple people and other objects. Additionally, as explained in the guide on data visualisation, humans are better able to see and interpret data based on visualising them with colours and shapes.\nWhen we combine these two concepts, it is therefore not surprising that one of the most common visualisations for cultural heritage collections, and library collections specifically, are maps.\nHowever, creating a map or adding objects on a map are not neutral processes: maps reproduce ways of seeing, giving specific interpretations of the world. For example, a map created according to a Mercator projection (created by Gerardus Mercator in 1569) will expand the lands above the Equator, therefore making Madagascar as big as Great Britain while it is actually twice the size. This projection reflects a view of the world that is western-centric. Despite criticism, this projection is actually the base of Web Mercator.\nWeb Mercator is a de facto standard for Web Mapping and it is used by Google Maps, Bing Maps and OpenStreetMap, therefore continuing the reinforcement of the same way of seeing the world while hiding the fact that it is a way of seeing the world.\nRecognising that every map is not neutral and it is instead a cultural artefact is particularly important when approaching the digital mapping of collections as the maps may enhance their partiality.\nAt the same time, using maps as a visualisation tool has the potential both to make libraries aware of new and unheard stories within their collections, to connect collections from multiple institutions (see for example Heritage for All) and to identify gaps that may be worth exploring.\nBecause of their ubiquity, they have the potential to be easy to explore for different audiences, keeping in mind that accessibility and usability, as much as with other visualisations and interfaces, need to be kept at the centre of their development.\nKnowing who will use the maps and for what purpose is therefore essential as much as being aware of the interpretation that the map is proposing and how this interpretation is influenced by the adoption of a specific technology and how the process of mapping happened.\nFor example, visualising indigenous knowledge in a Web Mercator-based application may be problematic as the approach to place and its visualisation may be different than the one presented on some platforms and may require a discussion with communities on how to best represent their interpretation of place and their culture.\nTo digitally map collections, there are a few options available in terms of tools and softwares:\n\nFirst of all, there are specialistic software available such as QGIS and ArcGIS where geolocated data (that is data that have coordinates) can be visualised on maps chosen by the librarian/researcher and historical maps can be associated with coordinates.\nIt is possible to visualise geolocated data on online platforms designed for digital mapping that are not specifically for the Humanities, such as Google Maps.\nThere are online tools designed to visualise humanities data in multiple ways and have an option of visualising data on a map such as Recogito, where annotated places on a text can be visualised, or Palladio that is used to visualise data as networks.\nThere are generic visualisation tools such as Tableau, that has been explored in the data visualisation guide.\nFinally, it is worth mentioning the work the IIIF community in standardising the georeferencing of maps in IIIF and creating geospatial annotations.",
    "crumbs": [
      "Topic Guides",
      "Digital Mapping"
    ]
  },
  {
    "objectID": "digitalmapping.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "digitalmapping.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Digital Mapping",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nHaving introduced the importance of maps to visualise people, objects and relationships in cultural heritage, it is not surprising that digital mapping provides multiple potential applications and challenges for libraries. Maps can be used, as I said before, to communicate an interpretation with audiences, to let them explore collections or to analyse their collections as data, to see gaps and uncover stories.\nFor example, the project Mapping the Republic of Letters by Stanford University has used maps to visualise and let users explore the places where editions of Voltaire were published between 1712 and 1800 or to visualise the places tourists explored during the Grand Tour.\nIf the previous example illustrates how geolocated data can help study editions and letters and see the bigger picture, maps can also be used to let users explore collections, as mentioned before with Heritage for All. An example is WarSampo that lets users explore the relevant places, events, people and photographs linked to Finland during World War II using Linked Open Data, or the Holocaust refugee map developed by The Weiner Holocaust Library that includes not only documents and photographs but also recorded interviews.\nFinally, digital mapping can be used for digital storytelling, narrating stories related to places such as with Curiocity by the National Library of Singapore. The library has curated, along with the National Archives, stories related to places in Singapore, mixing maps and photographs to explore their past and present cultural significance.",
    "crumbs": [
      "Topic Guides",
      "Digital Mapping"
    ]
  },
  {
    "objectID": "digitalmapping.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "digitalmapping.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Digital Mapping",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nHere are some helpful tools that can be used to begin digital mapping. They are divided by typology: web-based, software, and coding-based. Some are designed specifically for humanities and cultural heritage collections data while others are designed for more generic geospatial data. Most have great documentation and tutorials available.\n\nWeb-based tools\n\nPalladio is useful for testing multiple visualisations with Humanities data. It offers the option to upload .csv files, structure them and visualise them in maps, timelines and networks. There are a few tutorials for beginners available here.\nRecogito is a platform for annotating text. It is also possible to apply Name Entity Recognition, disambiguate places and visualise them on the map feature provided (see this 10 minute tutorial).\nGoogle Maps allows saving and sharing places and trails. It is also possible to create, customise and share maps through My Maps. To understand how it works and how it differs from Google Earth Pro, there is a tutorial on Programming Historian.\nAllMaps is designed to visualise and georeference maps in IIIF.\n\n\n\nSoftware\n\nArcGIS is probably the most well-known Geographic Information System (GIS) software, providing enhanced capabilities compared to the previous web-based options, including spatial data analysis. It is a commercial tool which offers a range of products, from desktop software to cloud-based platforms. For a brief introduction to GIS and its basic components,this course by the MIT is particularly handy. There are a few beginner tutorials in the documentation provided by Esri.\nQGIS is an open-source and free software that offers some of the capabilities of ArcGIS. There are beginner tutorials on Programming Historian on installing it and creating layers and a comprehensive documentation on the official website.\nGoogle Earth Pro is a free software based on Google Earth but offers enhanced capabilities for handling geospatial data and performing some basic data analysis (for the difference with Google Maps see here)\n\n\n\nCoding\nThere are multiple libraries for digital mapping, depending on the programming language used.\n\nFor Python, geopandas, basemap and matplotlib are good for static maps, while Folium, Plotly, Dash (for dashboards) and ipyleaflet are used to create interactive maps in Python and in Jupyter Notebooks.\nFor R, for static maps there is ggplot while, for interactive maps, leaflet is a good option.\nFinally, it is worth mentioning the D3 Javascript Library that is specifically designed for data visualisation and includes a lot of coded examples of maps.\n\n\n\nAdditional tutorials\n\nQGIS and ArcGIS\n\nClifford, J., MacFadyen, J., & Macfarlane, D. (2013). Georeferencing in QGIS 2.0. Programming Historian, 2. https://doi.org/10.46430/phen0027\nColson, J. (2017). Geocoding historical data using QGIS. Programming Historian, 6. https://doi.org/10.46430/phen0066\nGeospatial Historian. (n.d.) Lessons. Retrieved January 22 2025.Lessons – Geospatial Historian\n\n\n\nProgramming in Python and R\n\nPham, K. (2017). Web mapping with Python and Leaflet. Programming Historian, 6. https://doi.org/10.46430/phen0070\nRyan, Y. (2022). Making an interactive web application with R and Shiny. Programming Historian, 11. https://doi.org/10.46430/phen0105",
    "crumbs": [
      "Topic Guides",
      "Digital Mapping"
    ]
  },
  {
    "objectID": "digitalmapping.html#recommended-readingviewing",
    "href": "digitalmapping.html#recommended-readingviewing",
    "title": "Digital Mapping",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nThere are a few additional readings that may be interesting to get more information or to explore additional options for digital mapping.\n\nOther library guides on digital mapping and GIS\n\nNYU Libraries. (n.d.). Mapping and timelines. Retrieved January 21, 2025, from https://guides.nyu.edu/digital-humanities/tools-and-software/mapping-and-timelines\nUC Berkeley Library. (n.d.). Digital mapping. Retrieved January 21, 2025, from https://guides.lib.berkeley.edu/dh/mapping#s-lg-box-33113349\nUBC Library. (n.d.). Getting started with GIS. Retrieved January 21, 2025, from https://guides.library.ubc.ca/gis/gettingstarted\nUniversity of Reading Library. (n.d.). Digital maps. Retrieved January 21, 2025, from https://libguides.reading.ac.uk/maps/digitalmaps",
    "crumbs": [
      "Topic Guides",
      "Digital Mapping"
    ]
  },
  {
    "objectID": "digitalmapping.html#finding-communities-of-practice",
    "href": "digitalmapping.html#finding-communities-of-practice",
    "title": "Digital Mapping",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nIf you are interested in knowing more about digital mapping, there are a few communities of practice, conferences and summer schools that you may be interested to attend.\n\nThe Spatial Humanities Conference is held annually and it is specialised in geospatial technologies, geodata and the Humanities.\nThe GeoHumanities community is a special interest group part of the ADHO (Alliance of Digital Humanities Organisations).\nIIIF has an active community working on creating standards for mapping.\nThe Digital Humanities Oxford Summer School offers multiple DH courses, both theoretical and practical. Humanities Data is a good introduction to Humanities data processing and it usually includes some coursework on GIS. If there is an interest in data visualisation and programming at a more advanced level, there is a course specialised on Applied Data Analysis that explores, among others, geomapping.\nThe Cultural Heritage Data School by the University of Cambridge is dedicated to exploring cultural heritage data using digital methods and approaches. The programme changes every year and may include lectures on digital mapping.",
    "crumbs": [
      "Topic Guides",
      "Digital Mapping"
    ]
  },
  {
    "objectID": "guidelines.html",
    "href": "guidelines.html",
    "title": "Author Guidance & Style Guide",
    "section": "",
    "text": "Each DS Topic Guide follows a distinct structure, beginning with a Header Section (Front Matter) composing of the guide Title, Authors, Affiliations, Published date, Last modified info, and Abstract followed by five key content blocks detailed below. Please have a look at the Getting Started in DS as an example of how these pieces all fit together into one complete DS Topic Guide.\n\n\nThis markdown template shows an example of what information is required for the header, and how each section should be formatted and can be used as the basis for starting a new guide. Note: The project team will ensure each_DS Topic Guide_ is given an individual DOI through Zenodo once published.\n\n\n\nThis section should provide a concise overview of the topic, written in a relaxed and natural way. It does not need to contain the world’s knowledge, just enough high-level knowledge to get the key concepts across. It should be pitched at a beginner/foundational level.\nLinking can be used liberally for terminology and concepts throughout if explaining a particular term is more complex than space allows. Please see the Style Guidelines below for more writing tips!\n\n\n\nThis section provides a clear explanation of the topics’ specific relevance to the work of libraries.\nIt should contain:\n\nA short paragraph or two setting the scene as to why the topic is relevant to the work of libraries. Here you might also like to present opportunities and, if relevant, potential challenges for libraries around the topic as well.\nUp to 3 examples of real world (or potential) applications/case studies/projects, briefly explained, with links to further information if available. Note that it is not necessary to write up or create a new case study or use case yourself here, though you’re more than welcome to! If there are quite a few other examples to include, links to these can be added at the end of the section (Example: “For further case studies, visit….”).\n\n\n\n\nThe objective of this section is to enable learners to familiarise themselves with the basics of the topic through active practice via self-paced tutorials and hand-on activities. Authors are not expected to create new activities or tutorials in this section, but rather to provide selected links to existing hands on tutorials which are known by them to have proven value and can be personally recommended for library professionals in particular.\nThe tutorials recommended should be free, online and suitable for independent study, and ideally, focussed on the library professional perspective where possible. Authors may wish to link to practical exercises or quizzes, specific online lessons that may exist in other online platforms, Juptyer Notebooks and GLAM workbench materials that provide detailed explanations of the steps learners can follow.\nSuggested tutorials should be open access, not behind a paywall.\nFor the sake of consistency across the various topic guides, it is helpful if tutorial references are structured as follows:\n\nThe title/name of the activity/tutorial, the URL (added as a hyperlink) and/or provide a DOI if there is one)\nA brief, personal explanation below it (no more than 200 words) as to why the author recommends this particular tutorial, and, optionally, an indication of topics covered and the level of complexity.\n\n\n\n\nThe section on recommended reading and viewing contains references to more passive learning resources such as:\n\nOpen access articles discussing the topic at a general level, or containing contextual information.\nVideo recordings of lectures about the topic, which do not demand practical activities from the viewer (these can be embedded here)\nPodcasts about the topic\n\nWhen including citations please make sure to provide the DOI with your text if possible so that the project team can compile a dedicated Zotero Library.\nWe discourage adding links and recommendations to materials that are behind paywalls, or otherwise inaccessible easily online/remotely. We recommend use of a VPN for verifying this as sometimes library institution set-ups can make it harder to realise that articles are not free!. If, however, there is an obvious seminal reference work, provide a full reference for it, explain that it is paywalled content or not digitised, and point readers to Worldcat if possible so they can find it in their library.\n\n\n\nThis section provides guidance to library professionals on where they can find networks or additional support from like-minded colleagues for taking their learning journey further. It should include:\n\nWhere to find (local/national/international) Communities of Practice or other relevant networks and organisations who can help with furthering their understanding of the topic.\nIf relevant you might also point to specific summer schools, conferences and other learning events that may further enhance networking and thus learning around a particular topic."
  },
  {
    "objectID": "guidelines.html#structuring-the-ds-topic-guide",
    "href": "guidelines.html#structuring-the-ds-topic-guide",
    "title": "Author Guidance & Style Guide",
    "section": "",
    "text": "Each DS Topic Guide follows a distinct structure, beginning with a Header Section (Front Matter) composing of the guide Title, Authors, Affiliations, Published date, Last modified info, and Abstract followed by five key content blocks detailed below. Please have a look at the Getting Started in DS as an example of how these pieces all fit together into one complete DS Topic Guide.\n\n\nThis markdown template shows an example of what information is required for the header, and how each section should be formatted and can be used as the basis for starting a new guide. Note: The project team will ensure each_DS Topic Guide_ is given an individual DOI through Zenodo once published.\n\n\n\nThis section should provide a concise overview of the topic, written in a relaxed and natural way. It does not need to contain the world’s knowledge, just enough high-level knowledge to get the key concepts across. It should be pitched at a beginner/foundational level.\nLinking can be used liberally for terminology and concepts throughout if explaining a particular term is more complex than space allows. Please see the Style Guidelines below for more writing tips!\n\n\n\nThis section provides a clear explanation of the topics’ specific relevance to the work of libraries.\nIt should contain:\n\nA short paragraph or two setting the scene as to why the topic is relevant to the work of libraries. Here you might also like to present opportunities and, if relevant, potential challenges for libraries around the topic as well.\nUp to 3 examples of real world (or potential) applications/case studies/projects, briefly explained, with links to further information if available. Note that it is not necessary to write up or create a new case study or use case yourself here, though you’re more than welcome to! If there are quite a few other examples to include, links to these can be added at the end of the section (Example: “For further case studies, visit….”).\n\n\n\n\nThe objective of this section is to enable learners to familiarise themselves with the basics of the topic through active practice via self-paced tutorials and hand-on activities. Authors are not expected to create new activities or tutorials in this section, but rather to provide selected links to existing hands on tutorials which are known by them to have proven value and can be personally recommended for library professionals in particular.\nThe tutorials recommended should be free, online and suitable for independent study, and ideally, focussed on the library professional perspective where possible. Authors may wish to link to practical exercises or quizzes, specific online lessons that may exist in other online platforms, Juptyer Notebooks and GLAM workbench materials that provide detailed explanations of the steps learners can follow.\nSuggested tutorials should be open access, not behind a paywall.\nFor the sake of consistency across the various topic guides, it is helpful if tutorial references are structured as follows:\n\nThe title/name of the activity/tutorial, the URL (added as a hyperlink) and/or provide a DOI if there is one)\nA brief, personal explanation below it (no more than 200 words) as to why the author recommends this particular tutorial, and, optionally, an indication of topics covered and the level of complexity.\n\n\n\n\nThe section on recommended reading and viewing contains references to more passive learning resources such as:\n\nOpen access articles discussing the topic at a general level, or containing contextual information.\nVideo recordings of lectures about the topic, which do not demand practical activities from the viewer (these can be embedded here)\nPodcasts about the topic\n\nWhen including citations please make sure to provide the DOI with your text if possible so that the project team can compile a dedicated Zotero Library.\nWe discourage adding links and recommendations to materials that are behind paywalls, or otherwise inaccessible easily online/remotely. We recommend use of a VPN for verifying this as sometimes library institution set-ups can make it harder to realise that articles are not free!. If, however, there is an obvious seminal reference work, provide a full reference for it, explain that it is paywalled content or not digitised, and point readers to Worldcat if possible so they can find it in their library.\n\n\n\nThis section provides guidance to library professionals on where they can find networks or additional support from like-minded colleagues for taking their learning journey further. It should include:\n\nWhere to find (local/national/international) Communities of Practice or other relevant networks and organisations who can help with furthering their understanding of the topic.\nIf relevant you might also point to specific summer schools, conferences and other learning events that may further enhance networking and thus learning around a particular topic."
  },
  {
    "objectID": "guidelines.html#style-guidelines",
    "href": "guidelines.html#style-guidelines",
    "title": "Author Guidance & Style Guide",
    "section": "Style Guidelines",
    "text": "Style Guidelines\n\nThink: Natural, Casual, Accessible, and Internationally Inclusive Content\n\nImagine that a colleague has come to you casually asking about the topic over tea. How might you go about explaining it to them in your own words? During the course of that casual conversation what key things would you leave in and what might you leave out in the interest of getting them to a basic understanding of a complex topic quickly?\n\nThough reviewed and edited by peers, DS Topics Guides are not meant to be written in the style of scholarly papers or journal articles, and instead their style is more in keeping with LibGuides, blogs and personal articles. As such, they should be written in a natural and relaxed manner, but to ensure consistency and inclusivity across our content, please consider these general guidelines:\n\nClarity and Simplicity: Write in a clear and straightforward manner, using simple language that is easy to understand for learners of all backgrounds and proficiency levels.\nLinking to Technical Terms: When introducing technical terms or concepts that may be unfamiliar to some learners, provide hyperlinks to additional resources or definitions where they can learn more. This helps to enhance understanding and allows learners to explore topics in more depth at their own pace. Ensure that the linked resources are reliable and authoritative to provide accurate information to the learners.\nAvoid Colloquialisms and Regionalisms: While it’s essential to maintain a casual and natural tone, please refrain from using colloquial expressions or regionalisms that may not be universally understood by our diverse audience.\nCultural Sensitivity: Be mindful of cultural differences and avoid language or examples that may be offensive or insensitive to any group of people. When providing examples or references, strive for universality and inclusivity.\nGender Neutrality: Use gender-neutral language whenever possible to ensure inclusivity and avoid assumptions about gender roles or identities.\nGlobal Perspective: Consider the international nature of our audience when crafting examples, scenarios, and references. Aim for content that resonates with learners from various cultural backgrounds and geographical locations."
  },
  {
    "objectID": "guidelines.html#checklist-for-authors-reviewerseditors",
    "href": "guidelines.html#checklist-for-authors-reviewerseditors",
    "title": "Author Guidance & Style Guide",
    "section": "Checklist for Authors & Reviewers/Editors",
    "text": "Checklist for Authors & Reviewers/Editors\nThe following checklist reflects the key areas Reviewers and Editors will be checking over with each submitted DS Topic Guide. Authors should find this helpful too and consult it before submitting their work for final review.\n\nFront Matter (for Topic Guides only)\n\nAbstract is clear, concise and reflective of topic\nAuthor names are linked to a LIBER profile where possible or a personal/business bio page\nAuthor orcid id’s are included and resolving correctly\nTitle follows correct capitalisation (Capitalise the first, last and ‘important’ words)\nCheck for references to old “DS Essentials” and replace with “DS Topic Guides”\n\n\n\nContent\n\nContent is clear and complete and follows the Style Guidelines above\nText reflects the aim of the section (for example, Finding Communities of Practice recommends networks/networking/conferences etc. rather than advanced tutorials/lessons)\nImages on the page are directly illustrative of the point/necessary\nCheck for typos\nCheck any activities provided work\nRecommended hands-on activities and self-guided tutorials should be Open Access and freely available online\nRecommended Reading & Viewing content (scholarly texts/articles/books/manuals) should be Open Access by default and not behind a paywall. (We recommend use of a VPN for verifying this as sometimes library institution set-ups can make it harder to realise that articles are not free!) If this is not possible and it is a seminal reference work, provide a full reference for it, explain that it is paywalled content, and point readers to Worldcat if possible so they can find it in their library.\n\n\n\nText Formatting Edits\n\nWritten in British English which is the style of the EU\nCheck for markdown errors\nCheck for references to old “DS Essentials” and replace with “DS Topic Guides”\n“Topic Guides” is always referred to and written in italics as “DS Topic Guides”\n“Digital Scholarship & Data Science Topic Guides for Library Professionals” is always italicised when referenced in text. Note the use of “&” and not “and”\nHeadings are correctly formatted. The title of each guide is already in h1# so main section headers such as “Introduction” should be second-level header with h2 tag ##.\nCapitalise the first, last and ‘important’ words of every heading; for example, ‘Snow White and the Seven Dwarves’.\nReplace Latin abbreviation (eg., ex., etc.,) with full text (Accessibility)\n\n\n\nLinks\n\nCheck all links to ensure there are no broken ones\nInternal links to content within the site (such as reference to other DS Topic Guides) should be relative rather than absolute and use the .qmd extenstion. For example the markdown would look like “See the [IIIF Topic Guide](iiif.qmd).” rather than “See the [IIIF Topic Guide](https://libereurope.github.io/ds-topic-guides/iiif.html).”\nExternal links should include the “https://”\nWe highly discourage linking to articles that are behind paywalled content. However, if it is a seminal reference work, provide a full reference for it, explain that it is paywalled content, and point readers to Worldcat if possible so they can find it in their library.\n\n\n\nImages\n\nImages should be public domain or CC0 as our guides are shared under CC-BY\nImages have alt-tags and captions (Accessibility)\nImages are all saved in the project GitHub folder: https://github.com/libereurope/ds-topic-guides/tree/main/book/images (this will be automated, reviewer can ignore)\nImages are in .jpg or .png or .svg format and ideally less than 1MB (this will be automated, reviewer can ignore)\nImages are no larger than the width of our body text column (width should be less than 949 pixels) (this will be automated, reviewer can ignore)"
  },
  {
    "objectID": "gettingstarted.html",
    "href": "gettingstarted.html",
    "title": "Getting Started in DS",
    "section": "",
    "text": "In this DS Topic Guide, we aim to help you connect your work in libraries to the world of digital scholarship and data science, and inspire new possibilities for evolving your practices. We’ll provide tips on how to use Digital Scholarship & Data Science Topic Guides for Library Professionals as a personal resource for building new skills. We’ll explore broad definitions of digital scholarship and data science in the context of research libraries, and share key resources to help you understand how emerging technologies and methods are transforming the traditional work of collecting, curating, creating, and sharing in cultural heritage institutions—and why developing skills in this area can be so valuable!\n\n\nThough there are many definitions of what constitutes “digital scholarship” out there, we tend to favour the broadest of interpretations, that is, roughly, any type of innovative research or library activity that combines the methodologies from traditional humanities & social science disciplines with computational tools and digital methods provided by computing disciplines, such as data science.\nThough closely aligned to, and generously informed by, the academic discipline Digital Humanities, “digital scholarship” allows us to consider more broadly the full range of innovative scholarly activities our users seek to undertake with our digital collections and data, across a diverse range of disciplines.\n\n“Digital scholarship” allows us to define a space for us heritage professionals, where research is undertaken, in our own right, during the course of our daily work, utilising computational methods in the curation, creation, collecting and sharing of our digital collections and data, but is not confined to formal academic pursuits or a particular discipline.\n\n\n\n\nNot quite. Data science, also an interdisciplinary academic field, is more specifically focussed on the use of algorithms, machine learning, and statistical modeling to make predictions and uncover deeper insights from noisy, structured, and unstructured data. In libraries, data science is used to improve services, enhance user experiences, and optimize library operations through data-driven insights. Here are a few ways it’s applied:\n\nUser Behavior Analysis: By analyzing data on book checkouts, website visits, and user preferences, libraries can understand which genres or materials are most popular, and tailor their collections accordingly. This helps in making data-informed decisions about acquisitions and promotions.\nPredicting Trends: Data science can help libraries forecast trends — for example, which types of books might become popular in the future or when certain resources are likely to be in demand. This allows libraries to better plan their inventory and schedules.\nImproving Resource Allocation: Libraries can use data science to optimise staffing and the allocation of resources. By examining patterns in library visits, they can predict busy times and adjust staff schedules, ensuring efficient service delivery.\nPersonalized Recommendations: Just like Netflix recommends movies, libraries can use data science to suggest books or resources to users based on their reading history and preferences, making library services more personalized.\nEnhancing User Engagement: By analyzing usage patterns and feedback data, libraries can identify gaps in services or areas for improvement. They can use this information to engage users better and develop targeted programs, like workshops or events, based on what users are interested in.\n\n\nDigital scholarship and data science intersect in ways that are especially valuable for libraries, offering rich possibilities for advancing library services and operations. Digital Scholarship & Data Science Topic Guides for Library Professionals explores these synergies to support informed, innovative library work.",
    "crumbs": [
      "Topic Guides",
      "Getting Started in DS"
    ]
  },
  {
    "objectID": "gettingstarted.html#introduction",
    "href": "gettingstarted.html#introduction",
    "title": "Getting Started in DS",
    "section": "",
    "text": "In this DS Topic Guide, we aim to help you connect your work in libraries to the world of digital scholarship and data science, and inspire new possibilities for evolving your practices. We’ll provide tips on how to use Digital Scholarship & Data Science Topic Guides for Library Professionals as a personal resource for building new skills. We’ll explore broad definitions of digital scholarship and data science in the context of research libraries, and share key resources to help you understand how emerging technologies and methods are transforming the traditional work of collecting, curating, creating, and sharing in cultural heritage institutions—and why developing skills in this area can be so valuable!\n\n\nThough there are many definitions of what constitutes “digital scholarship” out there, we tend to favour the broadest of interpretations, that is, roughly, any type of innovative research or library activity that combines the methodologies from traditional humanities & social science disciplines with computational tools and digital methods provided by computing disciplines, such as data science.\nThough closely aligned to, and generously informed by, the academic discipline Digital Humanities, “digital scholarship” allows us to consider more broadly the full range of innovative scholarly activities our users seek to undertake with our digital collections and data, across a diverse range of disciplines.\n\n“Digital scholarship” allows us to define a space for us heritage professionals, where research is undertaken, in our own right, during the course of our daily work, utilising computational methods in the curation, creation, collecting and sharing of our digital collections and data, but is not confined to formal academic pursuits or a particular discipline.\n\n\n\n\nNot quite. Data science, also an interdisciplinary academic field, is more specifically focussed on the use of algorithms, machine learning, and statistical modeling to make predictions and uncover deeper insights from noisy, structured, and unstructured data. In libraries, data science is used to improve services, enhance user experiences, and optimize library operations through data-driven insights. Here are a few ways it’s applied:\n\nUser Behavior Analysis: By analyzing data on book checkouts, website visits, and user preferences, libraries can understand which genres or materials are most popular, and tailor their collections accordingly. This helps in making data-informed decisions about acquisitions and promotions.\nPredicting Trends: Data science can help libraries forecast trends — for example, which types of books might become popular in the future or when certain resources are likely to be in demand. This allows libraries to better plan their inventory and schedules.\nImproving Resource Allocation: Libraries can use data science to optimise staffing and the allocation of resources. By examining patterns in library visits, they can predict busy times and adjust staff schedules, ensuring efficient service delivery.\nPersonalized Recommendations: Just like Netflix recommends movies, libraries can use data science to suggest books or resources to users based on their reading history and preferences, making library services more personalized.\nEnhancing User Engagement: By analyzing usage patterns and feedback data, libraries can identify gaps in services or areas for improvement. They can use this information to engage users better and develop targeted programs, like workshops or events, based on what users are interested in.\n\n\nDigital scholarship and data science intersect in ways that are especially valuable for libraries, offering rich possibilities for advancing library services and operations. Digital Scholarship & Data Science Topic Guides for Library Professionals explores these synergies to support informed, innovative library work.",
    "crumbs": [
      "Topic Guides",
      "Getting Started in DS"
    ]
  },
  {
    "objectID": "gettingstarted.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "gettingstarted.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Getting Started in DS",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\n\nSo what might “digital scholarship and data science in libraries” actually look like?\n\na subject librarian using a digital tool (like OpenRefine) to clean up a set of catalogue records in order to understand gaps in the metadata and collection scope\na collaborative project to automatically transcribe handwritten texts from old manuscripts\na library director reading up on the latest in AI and seeking expert perspectives in order to write a strategy document\na metadata specialist packaging up digital collections into datasets that can be used by researchers\na digitisation project looking to improve the searchability of printed books in under-resourced languages\na marketing analysts using data analytics and data science techniques to understand visitor trends\nan assistant curator attending meetings of an international knowledge exchange networks (such as International GLAM Labs Community)\na reference librarian pointing a researcher to datasets that might help them with their research enquiry\nan imaging technician creating a 3D model of a collection item\na licensing manager keeping up to date on the latest uses of Text and Data Mining (TDM) and AI in research so that digital collections meets the needs of library users and staff who want to work with them at scale\na project manager leading a major interdisciplinary research project using the latest technologies to ask research questions of digital heritage collections\na curator creating an interactive online exhibit with annotations\na research software engineer using a machine learning model to identify genre of digitised texts\nan assistant librarian attending a summer school on working with cultural heritage data\n\n\n\nWhy is it important for library staff to learn Digital Scholarship and Data Science skills?\nIn the recommended reading section of this DS Topic Guide we have linked to a number of key competency and skills reports and frameworks that define the need for such skills in library work. But in the most high-level terms we know that:\n\nLibraries need to continually keep apace this digital turn in order to understand the change in service requirements and support colleagues and each other keen to make the most of it.\nDigital scholarship work is collaborative, requires input across disciplines and domain expertise, our curatorial experts have an essential role to play in that.\nWe’ve so much to gain from understanding digital methods and having closer collaborations with digital scholars—there’s a synergy in solving shared issues such as correcting OCR, enriching collections metadata, conquering back-cataloguing and so on.\nDigital scholars are, today, using technology in innovative ways, expectations have already changed, they’re seeking access at scale to our collections for computational analysis, they’re using Generative AI to ask their research questions, we need to understand these technologies to understand how the nature of archival enquiry is changing\nCultural heritage digital collections are only going to grow and we need the digital skills to work confidently with them at scale",
    "crumbs": [
      "Topic Guides",
      "Getting Started in DS"
    ]
  },
  {
    "objectID": "gettingstarted.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "gettingstarted.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Getting Started in DS",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nEach DS Topic Guide on this site includes a range of specific hands-on activities and other self-guided tutorials colleagues across Europe personally recommend. When you’re ready to go further and have a better idea of the specific skills you need for a particular task, we can recommend having a good search through these excellent platforms which host or link to a great many in-depth training materials:\n\nAI4Culture https://ai4culture.eu/\nAI4Culture is a capacity building platform for the application of AI in the Cultural Heritage Sector. AI4Culture has been co-funded by the European Union under the Digital Europe Programme. Their aim is to enable professionals, researchers, and enthusiasts within the sector with the resources they need to integrate AI into their daily workflow, find creative ways to use them and solve their current problems. The platform hosts a pool of readily deployed AI software tools, along with training and testing datasets that have been curated for use within the sector.\n\n\nCLARIN Learning Resources https://www.clarin.eu/content/learning-and-training-resources\nCLARIN stands for “Common Language Resources and Technology Infrastructure”, it is a European Research Infrastructure Consortium (ERIC). The CLARIN Learning Hub gives access to open educational resources on various topics of relevance to digital scholarship and data science, including full online training modules to learn these new skills.\n\n\nDARIAH-Campus https://campus.dariah.eu/\nDARIAH stands for “Digital Research Infrastructure for the Arts and Humanities”, like CLARIN it was established as a European Research Infrastructure Consortium (ERIC). As a pan-European infrastructure it supports digital research as well as the teaching of digital research methods for arts and humanities scholars working with computational methods. Though not specific to the library professional context, tutorials here are useful for applying techniques to digital collections.\n\n\nThe Glam Workbench https://glam-workbench.net/\nThe GLAM Workbench is the brainchild of Tim Sherratt, a historian, and is a collection of Jupyter notebooks to help you explore and use data from GLAM institutions (galleries, libraries, archives, and museums). It includes tools, tutorials, examples, hacks, and even some pre-harvested datasets. It is aimed at researchers in the humanities but has useful tutorials for anyone interested in working with GLAM data.\n\n\nIneo https://www.ineo.tools/\nIneo is a project developed and maintained by CLARIAH (“Common Lab Research Infrastructure for the Arts and Humanities”, a collaboration of CLARIN and DARIAH) that lets you search, browse, find and select digital resources for research in humanities and social sciences. It offers access to thousands of tools, datasets, workflows, standards and learning material. It is a work in progress so do keep that in mind when browsing.\n\n\nLibrary Carpentry https://librarycarpentry.org/\nLibrary Carpentry is an international volunteer community, under the Carpentries, focussed building software and data skills within library and information-related communities. The lessons here are meant to be taught as workshops led by a Carpentries certified instructor (for a fee) but you may find it useful to have a read through the content which is open and available to all.\n\n\nThe Programming Historian https://programminghistorian.org/en/\nThe Programming Historian has been publishing peer-reviewed tutorials on digital tools and techniques for humanists since 2008 and though they’re generally aimed at academic researchers, staff at British Library have found them highly useful over the years in their own work!\n\n\nSocial Sciences & Humanities Open Marketplace https://marketplace.sshopencloud.eu/search?order=score&categories=training-material\nBuilt as part of the Social Sciences and Humanities Open Cloud project (SSHOC), the Social Sciences and Humanities Open Marketplace is a discovery portal which pools and contextualises resources for Social Sciences and Humanities research communities: tools, services, training materials, datasets, publications and workflows. The Marketplace highlights and showcases solutions and research practices for every step of the SSH research data life cycle.",
    "crumbs": [
      "Topic Guides",
      "Getting Started in DS"
    ]
  },
  {
    "objectID": "gettingstarted.html#recommended-readingviewing",
    "href": "gettingstarted.html#recommended-readingviewing",
    "title": "Getting Started in DS",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nThere is no shortage of recommended reading lists out there, and again, each DS Topic Guide on this site will have their own recommended reading on a particular topic. Here we present a number of key reports and publications which articulate the broad sectoral view of why such skills are necessary for library professionals.\n\nDigital Scholarship & Data Science Skills Competency Frameworks\n\n\nLIBER Publications\n\nLIBER Digital Skills for Library Staff & Researchers Working Group - LIBER Europe have lots of resources here, including a very useful diagram Identifying Open Science Skills for Library Staff & Researchers\nLIBER Job Description Repository Contains job description examples for Digital Curator and other digital roles which reference the types of skills required for such work.\nEurope’s Digital Humanities Landscape: A Study From LIBER’s Digital Humanities & Digital Cultural Heritage Working Group is a report based on a Europe-wide survey run by LIBER’s Digital Humanities & Digital Cultural Heritage Working Group. The survey focused on digital collections and the activities libraries undertake around them. It covered the following topics and themes including staffing/skills\n\n\n\nKey Publications specific to digital scholarship and data science skills for research library staff\n\nThe British Library and the Arts and Humanities Research Council published a report on skills: Scoping Skills and Developing Training Programme for Managing Repository Services in Cultural Heritage Organisations. There is a very useful section (Section 3.) that references several other digital skills frameworks for research library staff across Europe.\nLippincott, Joan K. Directions in Digital Scholarship: Support for Digital, Data-Intensive, and Computational Research in Academic Libraries. Coalition for Networked Information, June 2023. https://doi.org/10.56561/ULHJ1168\nPadilla, Thomas. ‘Responsible Operations: Data Science, Machine Learning, and AI in Libraries’. OCLC, 26 August 2020.\nCordell, R. C. (2020). Machine Learning + Libraries: A Report on the State of the Field. LC Labs, Library of Congress.\nFederer L. Defining data librarianship: a survey of competencies, skills, and training. J Med Libr Assoc. 2018 Jul;106(3):294-303. doi: 10.5195/jmla.2018.306. Epub 2018 Jul 1. PMID: 29962907; PMCID: PMC6013124.\n\n\n\nGeneral Competencies for Librarians which include reference to digital\n\nAmerican Library Association (ALA) Library Competencies (Various roles): Library Competencies | Tools, Publications & Resources (ala.org) (USA)\nCanadian Association of Research Libraries Competencies for Librarians in Canadian Research Libraries Publications and Documents (including specifically Competencies-Final-EN-1-2.pdf (Canada)\nCILIP: the library and information association Professional Knowledge & Skills Base - (UK)",
    "crumbs": [
      "Topic Guides",
      "Getting Started in DS"
    ]
  },
  {
    "objectID": "gettingstarted.html#finding-communities-of-practice",
    "href": "gettingstarted.html#finding-communities-of-practice",
    "title": "Getting Started in DS",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nAs you embark on learning more about digital scholarship and data science in a library context, you might want to explore and join existing communities of practice.\n\nLIBER Working Groups\nWorking groups are open to staff at participating LIBER Member institutions:\n\nLIBER Data Science in Libraries\nLIBER Digital Scholarship & Digital Cultural Heritage\nOr have a look at the other LIBER Working Groups - LIBER Europe\n\n\n\nInternational Communities/Networks\n\nAI4LAM An international, participatory community focused on advancing the use of artificial intelligence in, for and by libraries, archives and museums (Free)\nCode4Lib International, diverse and inclusive community of developers and technologists for libraries, museums, and archives who are dedicated to seeking to share ideas and build collaboration (Free)\nIIIF Community IIIF Groups meet regularly to discuss various contexts of IIIF usage for a particular idea or initiative from technical and non-technical perspectives. (Free)\nIMPACT Centre of Competence IMPACT is a not for profit organisation with the mission to make the digitisation of text “better, faster, cheaper” and to further advance the state-of-the-art in the field of document imaging, language technology and the processing of historical text (Paid membership model)\nInternational GLAM Labs Community An international group dedicated to sharing knowledge around setting up, maintaining and sustaining Galleries, Libraries, Archives and Museums’ cultural heritage innovation Labs (Free)\nMuseums Computer Group The Museums Computer Group (MCG) is a non-profit association of individuals, volunteers, who share a common interest in encouraging, improving and influencing best practice in the use of technology and digital platforms within the museum and heritage sector. (Free to join their discussion list)\nREAD Co-op/Transkribus Community A co-operative organisation, born out of two major EU projects, with the goal of revolutionising access to archival documents by providing a comprehensive range of tools and services that empower researchers, institutions, and individuals with cutting-edge technology such as Handwritten Text Recognition (Paid membership model, but also some free membership options for students and scholars)\n\n\n\nNational Networks (European)\nIreland/UK - RLUK Digital Scholarship Network\n\n\n\n\n\n\nAre we missing something?\n\n\n\n\n\nWe’d love to hear it! Suggest edits by opening a new Issue or adding to the discussion on existing Issues on the project Github. If you’re new to GitHub don’t worry, we have a DS Topic Guide for that: GitHub: How to navigate and contribute to Git-based projects! Or just drop us a line at digitalresearch@bl.uk!",
    "crumbs": [
      "Topic Guides",
      "Getting Started in DS"
    ]
  },
  {
    "objectID": "licensing.html",
    "href": "licensing.html",
    "title": "Licensing & Re-use",
    "section": "",
    "text": "The project team behind this resource is committed to upholding the values of open source and open science and are keen for the Topic Guides and other content provided here to be used widely and adapted where necessary for local contexts. To enable this, Digital Scholarship & Data Science Topic Guides for Library Professionals, including all Topic Guides published here, are contributed under a CC BY 4.0 Deed | Attribution 4.0 International | Creative Commons license.\nThis means you are free to:\nShare — copy and redistribute the material in any medium or format for any purpose, even commercially.\nAdapt — remix, transform, and build upon the material for any purpose, even commercially.\nThe licensor cannot revoke these freedoms as long as you follow the license terms.\nUnder the following terms:\nAttribution — You must give appropriate credit , provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\nNo additional restrictions — You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits."
  },
  {
    "objectID": "licensing.html#citing-this-resource",
    "href": "licensing.html#citing-this-resource",
    "title": "Licensing & Re-use",
    "section": "Citing this Resource",
    "text": "Citing this Resource\nSuggested citations are written in APA (American Psychological Association) citation style for Journal Articles and are given on each individual guide:\nMcGregor, Nora, “IIIF,” Digital Scholarship & Data Science Topic Guides for Library Professionals (2024), [DOI link]\nTo cite the full resource:\nMcGregor, N., Verhaar, P., Double, J., & Kiraly, P. (Eds.). (2025, March 12). Digital Scholarship & Data Science Topic Guides for Library Professionals. https://libereurope.github.io/ds-topic-guides"
  },
  {
    "objectID": "ai-ml.html",
    "href": "ai-ml.html",
    "title": "AI & Machine Learning in Libraries",
    "section": "",
    "text": "Talk of AI is now firmly part of our everyday discourse at home and at work. In the library profession it takes centre stage in our conferences, in our strategies, funding calls, research proposals, and even our library systems. There is hope and excitement around the current capabilities of AI, but also hype, fear and anger, all of which can obfuscate our ability to truly get to grips with its implications.\nThis guide aims to help gently introduce and demystify AI and machine learning and its applications in a library context. It’s a fast moving area of course, and a complex one, so we can’t hope to cover it all here, but we won’t let that stop us from trying to get to the bottom of it!\nBy providing a bit of jargon busting alongside practical examples and links to further reading, and communities of practice this guide aims to help you on your way to better understanding today’s AI in a library context.\nSo let’s get started!\n\n\nArtificial intelligence (AI) has many definitions, and a simple google search will make your head spin, but I find it’s best understood as a really broad field of Computer Science, an umbrella term you could say, that refers to the research and development of systems capable of doing tasks that simulate human capabilities such as:\n\nhuman learning\ncomprehension\nproblem solving\ndecision making\ncreativity\nautonomy\n\nThings sometimes get confusing in AI discourse when people use the term machine learning (ML) interchangeably with AI. If we want to be a little more precise in the way we talk about these things, machine learning is a field of study focussed on the development of algorithms and models that allow computers (machines) to learn patterns and relationships from data and make predictions on new data. We might think of ML as a “pathway to enabling AI”. It is a core technology underpinning the subfields of AI, like Robotics, but also a core technology used in a ton of other fields outside of Computer Science like the Digital Humanities. So while AI as a field is concerned with building machines that mimic human capabilities, machine learning is typically only concerned with performing a specific task and providing accurate results by identifying patterns.\n\nThe primary task of machine learning is prediction.\n\nIf we want a computer to do something, we need to give it instructions. A typical algorithm provides a set of step-by-step instructions, in order of operation, for solving a problem or performing a task such as batch renaming files on a server so that they’re all lower-case for instance (“Look in this location, find any capitalised letter, replace it with its equivalent lower case, task complete”).\nInstead of being explicitly programmed to complete a specific task against a defined set of finite rules, however, machine learning provides a set of technologies and methods for finding patterns, relationships, and trends in data, in order to make decisions or predictions on new data on its own. So instead of giving the computer explicit instructions for every task, machine learning algorithms and models are focussed on learning from data, determining how accurate their prediction is and improving their performance on a task through training.\n\nA machine learning model is essentially a set of parameters that process input data into output data, where output might be for instance a classification, a summary of an input text, an answer to an input question, or a continuation of a conversation. The parameters of the model are quantitative representations of the patterns in data learned by the model during training, when a machine learning algorithm has pushed data through it. The model, once trained, can be used to extrapolate predictions about new data not used during training.\n\nIn general we treat model parameters as a “black box”: we don’t need to worry about their actual values, only whether or not they collectively process data in the way we’d like.\nThough AI and machine learning are highly interlinked today, early developments in the field did not always make use of machine learning. GOFAI (“good old fashioned AI”) is a phrase sometimes used to refer to systems that exhibit apparently “intelligent” behaviour but that are based on rules and algorithmic constraints that humans have coded themselves, rather than having a machine learn these rules from data. Much early AI fell within this paradigm, for instance the famous ELIZA chatbot created in 1966 by MIT professor Joseph Weizenbaum used clever rules to turn statements input by users into questions thrown back at them to create a somewhat convincing, though limited, interaction.\nTraditional AI is a term sometimes employed nowadays to refer to AI systems which use machine learning for doing rule based prediction tasks (as distinct from Generative AI). These systems analyse data we give them, recognise patterns and provide insights on that data. This is the type of AI we have made a whole lot of use of in the library world to date for things like classifying collections for metadata enhancement for instance (e.g., such as automatically transcribing handwritten texts, or identifying genre of digitised texts).\n Snapshot of an interface from Transkribus software used to create data to train a model to recognise handwritten arabic manuscript pages from a British Library collection.\nGenerative AI on the other hand refers broadly to systems whose primary function is to generate new content (e.g., continuing a conversation, writing a book, creating a piece of art), often in response to text or image prompts. This is the territory inhabited by conversation generating AI systems like ChatGPT for example, and we’re only just now exploring the potential applications for these new powerful systems in library work.\nThough there’s no doubt that today’s AI systems are shockingly convincing in how well they imitate humans, and can fool even the brightest among us, what we’re actually seeing are just very advanced machine learning algorithms and models performing specific and discrete functions (like holding a convincing conversation) extremely well. We’re a long way off (if ever) from machines having sentience (or, Artificial General Intelligence (AGI)/Strong AI). So, while there are plenty of things to worry about when it comes implementing AI solutions in libraries, sentient take-over is not one of them. For more on the topic of things to look out for in machine learning projects, have a read of this section of the Library Carpentry AI in GLAM lesson on Managing Bias.\n\n\n\nIf you have a bit of python experience and are ready to get started with implementing your own machine learning project, recommendations for where to begin can be found in the hands-on activity and other self-guided tutorials section. But if you’re not intending to directly set-up machine learning models yourself, it’s still helpful to have even the most high-level view of some of the process that working with ML involves.\n\n\nBefore you begin to embark on any kind of machine learning project it’s really useful to take a moment to make sure that you actually need the prediction capabilities of machine learning models. Often times a simple rules based programmed can resolve an issue and it’s not always necessary to use ML.\nBut let’s imagine for the sake of this guide, we have digitised thousands of 19th century books from our library collection. One problem with the associated metadata for each book is that the language field is empty for them, a holdover from historical cataloguing processes that library professionals everywhere will be all too familiar with. Not only do you need to be able to clearly state what language these texts are written in, you’d like to go a step further and list all the languages that may be represented in each. For instance a book primarily written in English may feature within it a multitude of languages, possibly even endangered languages and dialects that could be highly useful for researchers to be aware of. The scale of this task is such that we simply cannot do it manually, we cannot read all the pages let alone identify every language easily. We need a language-identification system, on the level of sentences, a system that can go through the digitised texts and tell us “sentence X is in language Y” for this task. We will need machine learning.\n\n\n\nDepending on the task, we could try to create our own specialised machine learning model and train it ourselves from scratch, or use one which is already trained and use some techniques to optimise the results. And indeed, up until recently, building and training specialised models from scratch was the norm for many traditional classification tasks relevant in the library world, but the pre-trained models of today, particularly foundation models, have become so powerful that they are quickly taking over as the first port of call for machine learning explorations in every domain, including libraries and cultural heritage.\n\nThere has been a major pardigm shift from building specialised models for specific tasks to using a single, large pre-trained model that can be prompted or fine-tuned.\n\nBy far the most common approach is to make use of an existing pre-trained model, and more specifically a general purpose “foundation” or “base” model, which has already been trained on lots and lots of diverse data already. At the very least you will want to always first test out how a pre-trained model fairs before embarking on the building of specialised models from scratch which can be costly, time consumming, and require special expertise and loads of data to train properly.\n\nA foundation model is a large-scale AI model built using significant data and computational resources to cover a general application (e.g., image processing, text processing) in a non-domain-specific way.\n\nFoundation models are typically provided by big companies with access to the resources required for building these models. Not all pre-trained models are foundation models, but these are generally the ones which your machine project will leverage as they can cover a variety of different objectives in terms of input and output due to the huge amounts of data and computation required to build one of these models from the ground up. Some examples of foundation models available in one way or another to the public at the time of writing include:\n\nThe GPT family of models, provided by OpenAI as the basis for the ChatGPT conversational AI as well as for development through OpenAI’s development platform;\nThe Llama family of models, which are, like GPT, models that facilitate text-based linguistic interaction and are provided as open source resources developed by Meta;\nStable Diffusion, a model for general image generation typically based on textual prompts provided by Stability AI;\nBERT, a family of models outside the generative paradigm used for natural language processing;\nWhisper, a family of models for converting spoken audio input into textual transcriptions, provided by OpenAI;\nDALL-E, a family of models for converting text into generated images, provided by OpenAI.\n\nGPT, Llama and BERT are more strictly speaking Large Language Models (LLMs). Language models in general are a type of machine learning model designed to predict the likelihood of a sequence of text, which means that they can be set up to predict the most likely way to continue a conversation. LLM’s are highly complex neural networks that have been exposed to an enormous amount of text from books, articles, websites, and more. Not all foundation models are strictly LLM’s though, for instance Whisper is a foundation model for audio (an automatic speech recognition model) and though DALL·E uses language models to understand your prompt, at its core is an image-generation model trained on images, not text, and its output is an image, not text.\nOh, and there are small language models (SLM) too which are becoming increasingly of interest, particularly as they are lighter, quicker and can be more efficient, generating much less of a carbon footprint when in use. Some SLMs are derived directly from existing LLMs, while others have been independently trained on a less enormous quantity of data, but all have been optimised using various clever model compression techniques to use far fewer parameters than the big ones, and in turn not as much memory and computing resource. Current SLMs you might hear of today are:\n\nDistilBERT\nGemma 2B/7B\nGPT-4o mini\nPhi-2\nMistral 7B\nTinyLlama\n\nSLMs may be right for institutions interested in supporting digital sustainability initiatives as a means of saving computing resource and energy where possible and are worth exploring.\n\n\n\nYou can visit the websites above for all of the major foundation models but it’s also worth checking sites like Hugging Face and Kaggle which are great places to find existing pre-trained machine learning models of all kinds, usually categorised by task domains they are designed to handle:\n\nAudio\nComputer vision\nText\nMultimodal (covering all of the above)\n\nMost sites which provide models will include vast amounts of detailed instructions for accessing the model, including setting up your environment, structuring your data, running the model, evaluating the results, fine-tuning the model and so on.\nBut for our more basic overview purposes here, the general idea is this:\nHaving defined our task and need for machine learning, we would begin with selecting an existing model based on the task we want to accomplish. In our use case of our 19th Century Books collection lacking language-field data, we’re interested in language identification so might like to start with a classifier trained for this task such as the XLM-RoBERTa Base Language Identification model. We would set up our environment, then show the pre-trained model our unlabelled data (in our example, this might be the text from the OCR pages of our digitised texts, delivered to the model in a structure it can read, like plain text) and see how well it does what we want it to do (does it identify the languages featured on each page with a level of accuracy we expect). We would evaluate how the output that the current state of the model offers compares to our target output (our gold-standard, as labelled by a knowledgeable human). For instance if we know that a sentence is “tengo mi dos huevos”, we want the model output to be “spanish”. A standard set-up would involve a model which outputs a probability associated with each possible target - in this case the candidate languages - such that all possible targets add up to 1 (this is called multi-class classification). The candidate language assigned the highest probability is taken as the predicted target.\nDepending on how well the model performs there are several ways in which we can then work with the pre-trained model to improve its results.\nFine-tuning involves taking a pre-trained model and adjusting it to better fit new data we give it. Fine-tuning modifies the model parameters by training it on our specialised data which actually changes the weights of the model to improve its performance for our data. Fine-tuning requires labelled data and compute resources and has the potential to be time-consumming, but is also a really useful process for domain-specific tasks. In our 19th Century Books example the labelled data might be labelled sentences where we indicate the target language for each sentence. Training a model with labelled data is called supervised machine learning, in that we are applying specific annotations to the data our machine algorithm will be pushing through the model. Other types are:\n\nUnsupervised: The model is given data that has not been manually categorised and labeled and asked to put it into groups (find patterns) without guidance. This is typically how foundation models have been trained.\nSemi-supervised: A combination of supervised and unsupervised.\nReinforcement Learning: The model learns about the world by interacting with its environment as it goes (for instance when ChatGPT asks a user whether or not the response it has given is what the user expected, this human feedback feeds into stages of quality control to improve model outputs).\n\nLet’s say the current state of the pre-trained model associates a probability of 0.2 with a Spanish-language input being assigned the “spanish” label. We want the probability to be 1.0, so we take the difference between those two of 0.8, and the algorithm uses this “loss” to update the model. The algorithm handles all of these moves forwards and backwards through the model, running an input, checking how the output compares with the target, and then updating the parameters of the model based on the loss.\nWhen we’re happy with the way the model handles the training data we have provided it, we’re done with fine-tuning and can now use the model to do inference, which is the term often used for making a prediction “in the wild” so to speak on completely new data.\nWe use basically the same algorithm to crank new, unseen data through our model, but rather than update the model we’re using it to give us the results we’re looking for, mainly languages identified in each of our 19th Century Books, along with a probability associated with each result. We can decide how we use these outputs in our cataloguing and what our appetitite for risk is (for instance we might set a confidence threshold at 80% and only include language results in the catalogue above that threshold).\nPrompt engineering on the other hand does not change the model itself but involves guiding the model to respond more accurately and contextually by crafting well-structured inputs (prompts). Prompt engineering doesn’t change the models weights in any way.\nRetrieval Augmented Generation (RAG) is another method you can use to help guide a pre-trained model to respond more contextually without changing the weights of the model itself. RAG is a method where the LLM is connected to an external knowledge base or document store, allowing it to fetch relevant real-time information and then generate responses based on that retrieved content. Some recent experiments in using RAG in the library world are being explored for tasks like optimising search of web archive files, or as a means for interactive sensitively with Indigenous data.\n\n\n\n\n\nLibrarians and archivists are often early adopters and experimenters with new technologies. Our field is also interested in critically engaging with technology, and we are well-positioned to be leaders in the slow and careful consideration of new technologies. Therefore, as librarians and archivists have begun using artificial intelligence (AI) to enhance library services, we also aim to interrogate the ethical issues that arise while using AI to enhance collection description and discovery and streamline reference services and teaching. -Introduction to the Special Issue: Responsible AI in Libraries and Archives\n\nThe seminal publication on the topic of acting responsibly with regards to experimenting with and implementing AI in libraries is Responsible Operations: Data Science, Machine Learning, and AI in Libraries by Thomas Padilla. Though it came out in 2019 it is still a vital reference resource for libraries to understand challenges and opportunities around their institutions and people engaging with data science, machine learning, and artificial intelligence (AI).\nMore recently, the authors of the Introduction to the Special Issue: Responsible AI in Libraries and Archives of the Journal of eScience Librarianship discuss seven overarching ethical issues that come to light in seven presented case studies of AI. These overarching issues are privacy, consent, accuracy, labor considerations, the digital divide, bias, and transparency and describes strategies suggested by case study authors to reduce harms and mitigate these issues.\nThe case studies are compiled by the Responsible AI in Libraries and Archives project which aims to support ethical decision-making for AI projects in libraries and archives by producing tools and strategies that support responsible use of AI in our profession.",
    "crumbs": [
      "Topic Guides",
      "AI & Machine Learning in Libraries"
    ]
  },
  {
    "objectID": "ai-ml.html#introduction",
    "href": "ai-ml.html#introduction",
    "title": "AI & Machine Learning in Libraries",
    "section": "",
    "text": "Talk of AI is now firmly part of our everyday discourse at home and at work. In the library profession it takes centre stage in our conferences, in our strategies, funding calls, research proposals, and even our library systems. There is hope and excitement around the current capabilities of AI, but also hype, fear and anger, all of which can obfuscate our ability to truly get to grips with its implications.\nThis guide aims to help gently introduce and demystify AI and machine learning and its applications in a library context. It’s a fast moving area of course, and a complex one, so we can’t hope to cover it all here, but we won’t let that stop us from trying to get to the bottom of it!\nBy providing a bit of jargon busting alongside practical examples and links to further reading, and communities of practice this guide aims to help you on your way to better understanding today’s AI in a library context.\nSo let’s get started!\n\n\nArtificial intelligence (AI) has many definitions, and a simple google search will make your head spin, but I find it’s best understood as a really broad field of Computer Science, an umbrella term you could say, that refers to the research and development of systems capable of doing tasks that simulate human capabilities such as:\n\nhuman learning\ncomprehension\nproblem solving\ndecision making\ncreativity\nautonomy\n\nThings sometimes get confusing in AI discourse when people use the term machine learning (ML) interchangeably with AI. If we want to be a little more precise in the way we talk about these things, machine learning is a field of study focussed on the development of algorithms and models that allow computers (machines) to learn patterns and relationships from data and make predictions on new data. We might think of ML as a “pathway to enabling AI”. It is a core technology underpinning the subfields of AI, like Robotics, but also a core technology used in a ton of other fields outside of Computer Science like the Digital Humanities. So while AI as a field is concerned with building machines that mimic human capabilities, machine learning is typically only concerned with performing a specific task and providing accurate results by identifying patterns.\n\nThe primary task of machine learning is prediction.\n\nIf we want a computer to do something, we need to give it instructions. A typical algorithm provides a set of step-by-step instructions, in order of operation, for solving a problem or performing a task such as batch renaming files on a server so that they’re all lower-case for instance (“Look in this location, find any capitalised letter, replace it with its equivalent lower case, task complete”).\nInstead of being explicitly programmed to complete a specific task against a defined set of finite rules, however, machine learning provides a set of technologies and methods for finding patterns, relationships, and trends in data, in order to make decisions or predictions on new data on its own. So instead of giving the computer explicit instructions for every task, machine learning algorithms and models are focussed on learning from data, determining how accurate their prediction is and improving their performance on a task through training.\n\nA machine learning model is essentially a set of parameters that process input data into output data, where output might be for instance a classification, a summary of an input text, an answer to an input question, or a continuation of a conversation. The parameters of the model are quantitative representations of the patterns in data learned by the model during training, when a machine learning algorithm has pushed data through it. The model, once trained, can be used to extrapolate predictions about new data not used during training.\n\nIn general we treat model parameters as a “black box”: we don’t need to worry about their actual values, only whether or not they collectively process data in the way we’d like.\nThough AI and machine learning are highly interlinked today, early developments in the field did not always make use of machine learning. GOFAI (“good old fashioned AI”) is a phrase sometimes used to refer to systems that exhibit apparently “intelligent” behaviour but that are based on rules and algorithmic constraints that humans have coded themselves, rather than having a machine learn these rules from data. Much early AI fell within this paradigm, for instance the famous ELIZA chatbot created in 1966 by MIT professor Joseph Weizenbaum used clever rules to turn statements input by users into questions thrown back at them to create a somewhat convincing, though limited, interaction.\nTraditional AI is a term sometimes employed nowadays to refer to AI systems which use machine learning for doing rule based prediction tasks (as distinct from Generative AI). These systems analyse data we give them, recognise patterns and provide insights on that data. This is the type of AI we have made a whole lot of use of in the library world to date for things like classifying collections for metadata enhancement for instance (e.g., such as automatically transcribing handwritten texts, or identifying genre of digitised texts).\n Snapshot of an interface from Transkribus software used to create data to train a model to recognise handwritten arabic manuscript pages from a British Library collection.\nGenerative AI on the other hand refers broadly to systems whose primary function is to generate new content (e.g., continuing a conversation, writing a book, creating a piece of art), often in response to text or image prompts. This is the territory inhabited by conversation generating AI systems like ChatGPT for example, and we’re only just now exploring the potential applications for these new powerful systems in library work.\nThough there’s no doubt that today’s AI systems are shockingly convincing in how well they imitate humans, and can fool even the brightest among us, what we’re actually seeing are just very advanced machine learning algorithms and models performing specific and discrete functions (like holding a convincing conversation) extremely well. We’re a long way off (if ever) from machines having sentience (or, Artificial General Intelligence (AGI)/Strong AI). So, while there are plenty of things to worry about when it comes implementing AI solutions in libraries, sentient take-over is not one of them. For more on the topic of things to look out for in machine learning projects, have a read of this section of the Library Carpentry AI in GLAM lesson on Managing Bias.\n\n\n\nIf you have a bit of python experience and are ready to get started with implementing your own machine learning project, recommendations for where to begin can be found in the hands-on activity and other self-guided tutorials section. But if you’re not intending to directly set-up machine learning models yourself, it’s still helpful to have even the most high-level view of some of the process that working with ML involves.\n\n\nBefore you begin to embark on any kind of machine learning project it’s really useful to take a moment to make sure that you actually need the prediction capabilities of machine learning models. Often times a simple rules based programmed can resolve an issue and it’s not always necessary to use ML.\nBut let’s imagine for the sake of this guide, we have digitised thousands of 19th century books from our library collection. One problem with the associated metadata for each book is that the language field is empty for them, a holdover from historical cataloguing processes that library professionals everywhere will be all too familiar with. Not only do you need to be able to clearly state what language these texts are written in, you’d like to go a step further and list all the languages that may be represented in each. For instance a book primarily written in English may feature within it a multitude of languages, possibly even endangered languages and dialects that could be highly useful for researchers to be aware of. The scale of this task is such that we simply cannot do it manually, we cannot read all the pages let alone identify every language easily. We need a language-identification system, on the level of sentences, a system that can go through the digitised texts and tell us “sentence X is in language Y” for this task. We will need machine learning.\n\n\n\nDepending on the task, we could try to create our own specialised machine learning model and train it ourselves from scratch, or use one which is already trained and use some techniques to optimise the results. And indeed, up until recently, building and training specialised models from scratch was the norm for many traditional classification tasks relevant in the library world, but the pre-trained models of today, particularly foundation models, have become so powerful that they are quickly taking over as the first port of call for machine learning explorations in every domain, including libraries and cultural heritage.\n\nThere has been a major pardigm shift from building specialised models for specific tasks to using a single, large pre-trained model that can be prompted or fine-tuned.\n\nBy far the most common approach is to make use of an existing pre-trained model, and more specifically a general purpose “foundation” or “base” model, which has already been trained on lots and lots of diverse data already. At the very least you will want to always first test out how a pre-trained model fairs before embarking on the building of specialised models from scratch which can be costly, time consumming, and require special expertise and loads of data to train properly.\n\nA foundation model is a large-scale AI model built using significant data and computational resources to cover a general application (e.g., image processing, text processing) in a non-domain-specific way.\n\nFoundation models are typically provided by big companies with access to the resources required for building these models. Not all pre-trained models are foundation models, but these are generally the ones which your machine project will leverage as they can cover a variety of different objectives in terms of input and output due to the huge amounts of data and computation required to build one of these models from the ground up. Some examples of foundation models available in one way or another to the public at the time of writing include:\n\nThe GPT family of models, provided by OpenAI as the basis for the ChatGPT conversational AI as well as for development through OpenAI’s development platform;\nThe Llama family of models, which are, like GPT, models that facilitate text-based linguistic interaction and are provided as open source resources developed by Meta;\nStable Diffusion, a model for general image generation typically based on textual prompts provided by Stability AI;\nBERT, a family of models outside the generative paradigm used for natural language processing;\nWhisper, a family of models for converting spoken audio input into textual transcriptions, provided by OpenAI;\nDALL-E, a family of models for converting text into generated images, provided by OpenAI.\n\nGPT, Llama and BERT are more strictly speaking Large Language Models (LLMs). Language models in general are a type of machine learning model designed to predict the likelihood of a sequence of text, which means that they can be set up to predict the most likely way to continue a conversation. LLM’s are highly complex neural networks that have been exposed to an enormous amount of text from books, articles, websites, and more. Not all foundation models are strictly LLM’s though, for instance Whisper is a foundation model for audio (an automatic speech recognition model) and though DALL·E uses language models to understand your prompt, at its core is an image-generation model trained on images, not text, and its output is an image, not text.\nOh, and there are small language models (SLM) too which are becoming increasingly of interest, particularly as they are lighter, quicker and can be more efficient, generating much less of a carbon footprint when in use. Some SLMs are derived directly from existing LLMs, while others have been independently trained on a less enormous quantity of data, but all have been optimised using various clever model compression techniques to use far fewer parameters than the big ones, and in turn not as much memory and computing resource. Current SLMs you might hear of today are:\n\nDistilBERT\nGemma 2B/7B\nGPT-4o mini\nPhi-2\nMistral 7B\nTinyLlama\n\nSLMs may be right for institutions interested in supporting digital sustainability initiatives as a means of saving computing resource and energy where possible and are worth exploring.\n\n\n\nYou can visit the websites above for all of the major foundation models but it’s also worth checking sites like Hugging Face and Kaggle which are great places to find existing pre-trained machine learning models of all kinds, usually categorised by task domains they are designed to handle:\n\nAudio\nComputer vision\nText\nMultimodal (covering all of the above)\n\nMost sites which provide models will include vast amounts of detailed instructions for accessing the model, including setting up your environment, structuring your data, running the model, evaluating the results, fine-tuning the model and so on.\nBut for our more basic overview purposes here, the general idea is this:\nHaving defined our task and need for machine learning, we would begin with selecting an existing model based on the task we want to accomplish. In our use case of our 19th Century Books collection lacking language-field data, we’re interested in language identification so might like to start with a classifier trained for this task such as the XLM-RoBERTa Base Language Identification model. We would set up our environment, then show the pre-trained model our unlabelled data (in our example, this might be the text from the OCR pages of our digitised texts, delivered to the model in a structure it can read, like plain text) and see how well it does what we want it to do (does it identify the languages featured on each page with a level of accuracy we expect). We would evaluate how the output that the current state of the model offers compares to our target output (our gold-standard, as labelled by a knowledgeable human). For instance if we know that a sentence is “tengo mi dos huevos”, we want the model output to be “spanish”. A standard set-up would involve a model which outputs a probability associated with each possible target - in this case the candidate languages - such that all possible targets add up to 1 (this is called multi-class classification). The candidate language assigned the highest probability is taken as the predicted target.\nDepending on how well the model performs there are several ways in which we can then work with the pre-trained model to improve its results.\nFine-tuning involves taking a pre-trained model and adjusting it to better fit new data we give it. Fine-tuning modifies the model parameters by training it on our specialised data which actually changes the weights of the model to improve its performance for our data. Fine-tuning requires labelled data and compute resources and has the potential to be time-consumming, but is also a really useful process for domain-specific tasks. In our 19th Century Books example the labelled data might be labelled sentences where we indicate the target language for each sentence. Training a model with labelled data is called supervised machine learning, in that we are applying specific annotations to the data our machine algorithm will be pushing through the model. Other types are:\n\nUnsupervised: The model is given data that has not been manually categorised and labeled and asked to put it into groups (find patterns) without guidance. This is typically how foundation models have been trained.\nSemi-supervised: A combination of supervised and unsupervised.\nReinforcement Learning: The model learns about the world by interacting with its environment as it goes (for instance when ChatGPT asks a user whether or not the response it has given is what the user expected, this human feedback feeds into stages of quality control to improve model outputs).\n\nLet’s say the current state of the pre-trained model associates a probability of 0.2 with a Spanish-language input being assigned the “spanish” label. We want the probability to be 1.0, so we take the difference between those two of 0.8, and the algorithm uses this “loss” to update the model. The algorithm handles all of these moves forwards and backwards through the model, running an input, checking how the output compares with the target, and then updating the parameters of the model based on the loss.\nWhen we’re happy with the way the model handles the training data we have provided it, we’re done with fine-tuning and can now use the model to do inference, which is the term often used for making a prediction “in the wild” so to speak on completely new data.\nWe use basically the same algorithm to crank new, unseen data through our model, but rather than update the model we’re using it to give us the results we’re looking for, mainly languages identified in each of our 19th Century Books, along with a probability associated with each result. We can decide how we use these outputs in our cataloguing and what our appetitite for risk is (for instance we might set a confidence threshold at 80% and only include language results in the catalogue above that threshold).\nPrompt engineering on the other hand does not change the model itself but involves guiding the model to respond more accurately and contextually by crafting well-structured inputs (prompts). Prompt engineering doesn’t change the models weights in any way.\nRetrieval Augmented Generation (RAG) is another method you can use to help guide a pre-trained model to respond more contextually without changing the weights of the model itself. RAG is a method where the LLM is connected to an external knowledge base or document store, allowing it to fetch relevant real-time information and then generate responses based on that retrieved content. Some recent experiments in using RAG in the library world are being explored for tasks like optimising search of web archive files, or as a means for interactive sensitively with Indigenous data.\n\n\n\n\n\nLibrarians and archivists are often early adopters and experimenters with new technologies. Our field is also interested in critically engaging with technology, and we are well-positioned to be leaders in the slow and careful consideration of new technologies. Therefore, as librarians and archivists have begun using artificial intelligence (AI) to enhance library services, we also aim to interrogate the ethical issues that arise while using AI to enhance collection description and discovery and streamline reference services and teaching. -Introduction to the Special Issue: Responsible AI in Libraries and Archives\n\nThe seminal publication on the topic of acting responsibly with regards to experimenting with and implementing AI in libraries is Responsible Operations: Data Science, Machine Learning, and AI in Libraries by Thomas Padilla. Though it came out in 2019 it is still a vital reference resource for libraries to understand challenges and opportunities around their institutions and people engaging with data science, machine learning, and artificial intelligence (AI).\nMore recently, the authors of the Introduction to the Special Issue: Responsible AI in Libraries and Archives of the Journal of eScience Librarianship discuss seven overarching ethical issues that come to light in seven presented case studies of AI. These overarching issues are privacy, consent, accuracy, labor considerations, the digital divide, bias, and transparency and describes strategies suggested by case study authors to reduce harms and mitigate these issues.\nThe case studies are compiled by the Responsible AI in Libraries and Archives project which aims to support ethical decision-making for AI projects in libraries and archives by producing tools and strategies that support responsible use of AI in our profession.",
    "crumbs": [
      "Topic Guides",
      "AI & Machine Learning in Libraries"
    ]
  },
  {
    "objectID": "ai-ml.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "ai-ml.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "AI & Machine Learning in Libraries",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nFor a really great overview of a wide range of AI use cases in Libraries I recommend having a read of Section 3: Library Applications in Cox A & Mazumdar’s Defining artificial intelligence for librarians (2022) from 2022 which covers back-end operations and library services for users.\nIn this guide we’ll focus our attention on just these two particular subfields of AI research areas, Natural Language Processing (NLP) and Computer Vision (CV) to give you a general sense of how machine learning is practically applied in the library context today.\n\nNatural Language Processing (NLP)\n\nNatural Language Processing (NLP) is concerned with making AI systems more capable of natural and effective interaction with humans (text and speech). It involves the development of a wide range of algorithms, models, and systems for analysing, understanding and extracting meaningful information from textual and speech data representing human language.\n\n\n\nText & Conversation Generation\nLanguage models have been around for decades without generating headlines or much of a fuss. They perform natural language processing tasks such as generating and classifying text, answering questions, and translating text and are the backbone of NLP. It wasn’t until the release of ChatGPT where the text & conversation generation capabilities of these new powerful large language models truly caught the popular imagination! ChatGPT’s publicly available interface and remarkable performance has allowed anyone with access to the internet to experiment with LLMs, so it’s worth spending a little time here unpacking just what ChatGPT is and its potential impact on library work.\nThe large language models behind ChatGPT and other similar systems, have learned something about patterns in grammar and word meaning, including the way that meaning arises contextually across multiple sentences and multiple turns in a conversation. If we compare this latest generation of LLM’s to early AI chat systems like ELIZA, mentioned earlier, the latter, without ML, was limited to a specific set of scripted responses. When you ask ChatGPT a question, you are presenting the machine learning model with new information it tries to make a prediction on, in this case, it tries to newly generate a response most likely to match the pattern of conversation.\nYou ask questions or give prompts to the model, and it provides responses in natural language, or rather, estimates what should come next in a conversation.\n\nWhen a system like ChatGPT gives a response, it isn’t actually looking up information and then composing that information into a response; it’s just making an estimation of a response based on patterns it has seen. So, when you ask it factual questions, especially ones with common answers or phrases, it might give you an answer that sounds right but remember this is because it’s mimicking what it has seen in its training data.\n\nLibrarians are still investigating use cases for new Generative AI applications such as this, and new multimodal versions and search integrations are allowing for even more capabilities. For now though at least, ChatGPT is certainly useful as a personal writing assistant or tool to help librarians in tasks like:\n\ncreating a title for a new exhibition\ncreating exhibition labels\noutlining a basic structure for an information literacy workshop\nwriting small bits of code to make your life easier and providing the steps for using it\ncreating a blog post on a topic for which you are very familiar\nhelping you reword or rephrase something for different audiences\nsummarising a text\ngathering keywords to use in searches\nwriting a funding proposal\n\nIt’s good to get in the habit of trying out (see our hands-on activity below for this) and being aware of how these particular models work as more and more library users will be using this technology too, and may not know quite have a clear understanding of what’s behind the responses generated by them. We’ve seen librarians having to answer queries about citations that have been made up by ChatGPT, article references which sound very much like they exist, but have just been hallucinated by the model!\n\nSubject indexing to enhance library catalogue search\nNamed Entity Recognition (NER) is a text analysis process within NLP that helps turn unstructured text into structured text. A sentence or a chunk of text is parsed through to find entities that can be put under categories like names, organisations, locations, quantities, monetary values, percentages, etc.\n\nIn the library world it can be used as part of a process to understand what subjects (people, places, concepts) are contained within a digitised text and help us enhance our catalogue records for items or search functionality. There is a very nicely outlined use case here of how the United States Holocaust Memorial Museum used NER to automatically extract person names and location from Oral History Transcript to improve indexing and search in their catalogue.\n\n\nAutomatic Language Detection & Genre Classification\nThe British Library has used different machine learning techniques and experiments derived from the field of NLP to assign language codes and genre classification to catalogue records, in order to enhance resources described. In the first phase of the Automated Language Identification of Bibliographic Resources project, language codes were assigned to 1.15 million records with 99.7% confidence. The automated language identification tools developed will be used to contribute to future enhancement of over 4 million legacy records. The genre classification case study includes a nice description of machine learning as well as references to other use cases for metadata clean up.\n\n\n\nComputer Vision (CV)\n\nComputer Vision (CV) is concerned with enabling machines to interpret and make decisions based on visual data from the world. We can use computer vision to train models to automatically analyse and understand useful information from images and videos.\n\nIn the library world we can use this to label millions of images with descriptive metadata (“this is a picture of a cat”), or, as we see below, a model can be trained to classify this image as a newspaper based on objects identified in the layout (for example, a nameplate for the newspaper, a headline, photographs, and illustrations and so on). The model learns how to identify that this is the NYT based on learning from other newspaper images it’s seen (for example, if given NY Tribune, NY Times, and NY Post images, it can distinguish between the various titles).\nYou ask questions or give prompts to the model, and it provides responses in natural language, or rather, estimates what should come next in a conversation. When ChatGPT gives a response, it isn’t actually looking up information and then composing that information into a response; it’s just making an estimation of a response based on patterns it has seen. So, when you ask it factual questions, especially ones with common answers or phrases, it might give you an answer that sounds right but remember this is because it’s mimicking what it has seen in its training data.\nLibrarians are still investigating use cases for this new Generative AI applications, but for now at least, ChatGPT is certainly useful as a personal writing assistant or tool to help give you ideas for\n\ncreating a title for a new exhibition\ncreating exhibition labels\noutlining a basic structure for an information literacy workshop\ncreating a blog post on a topic for which you are very familiar\nhelping you reword something for different audiences\nwriting a funding proposal!\n\nIt’s also good to get in the habit of trying out and being aware of how these particular models work as more and more library users will be using this technology too, and may not know quite have a clear understanding of what’s behind the responses generated by them. We’ve seen librarians having to answer queries about citations that have been made up by ChatGPT, article references which sound very much like they exist, but have just been hallucinated by the model!\n\n\nComputer Vision (CV) use cases\nWe can use computer vision to train models to automatically analyse and understand useful information from images and videos.\nIn the library world we can use this to label millions of images with descriptive metadata (“this is a picture of a cat”), or, as we see below, a model can be trained to classify this image as a newspaper based on objects identified in the layout (for example, a nameplate for the newspaper, a headline, photographs, and illustrations and so on). The model learns how to identify that this is the NYT based on learning from other newspaper images it’s seen (for example, if given NY Tribune, NY Times, and NY Post images, it can distinguish between the various titles).\n\n\n\nPutting it altogether: ML + CV + NLP\nOne of the state of the art applications of machine learning seen in cultural heritage at the moment is Handwritten Text Recognition (HTR). The idea with HTR is to convert digitised handwritten documents into searchable and machine readable text. To achieve this HTR actually uses a combination of Computer Vision (CV) and Natural Language Processing (NLP).\nSince handwriting can be tricky and ambiguous you might have a Computer Vision model try to identify possible letters from the shapes, and another to work out what the most likely word is from those shapes. But let’s imagine that there’s a smudge on the page, and the letters and maybe even whole words are completely illegible. In that case you might turn to your NLP language models which look at sentence level predictions, taking into account words in the whole line of text, the model uses that context to work out what words are most likely missing in those smudged spots!\nSometimes a model trained for a particular task (in the case of this HTR example, identifying a particular handwriting style) can be applied to similar content (other handwriting styles) with very good results. Transkribus has Public AI Models (transkribus.org) that have been created by users of the system and are then shared and can be reused by anyone.",
    "crumbs": [
      "Topic Guides",
      "AI & Machine Learning in Libraries"
    ]
  },
  {
    "objectID": "ai-ml.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "ai-ml.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "AI & Machine Learning in Libraries",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\n\nTutorials in machine learning\nThese free online tutorials come highly recommended by library colleagues for learning how to get started using machine learning. Some level of python knowledge/understanding is typically required/advisable for these (aside from Elements of AI which aims to get anyone started with or without programming skills):\n\nAI for Humanists\nHugging Face - Learn\nKaggle - Learn\nElements of AI\nPractical Deep Learning\nList of AI Tutorials recommended by GLAM professionals on AI4LAM\nSocial Science and Humanities Open Marketplace (SSH Open Marketplace) tutorials on AI\n\n\n\nQuick Hands-on Activities by topic\nThe following short hands-on activities/exercises below were developed by the Digital Research Team for British Library staff as part of the Digital Scholarship Training Programme. They are useful and fun for novices to try things out quickly and to get a sense of the technologies without having to install or download or programme anything.\n\nActivity 1: Explore Natural Language Processing\nCopy and paste a paragraph of text from somewhere around the web, or from your own collections, and see how each of these cloud services handle it:\n\nCloud Natural Language\nIBM Watson Natural Language Understanding Text Analysis\ndisplaCy\nVoyant Tools (voyant-tools.org) Voyant Tools is an open-source, web-based application for performing text analysis. It supports scholarly reading and interpretation of texts or corpus, particularly by scholars in the digital humanities, but also by students and the general public. It can be used to analyse online texts or ones uploaded by users.\nAnnif - tool for automated subject indexing There are many video tutorials here and the ability to demo the tool\n\n\n\nActivity 2: Explore ChatGPT in the Library Profession\nLogin to use the freely available ChatGPT (openai.com) interface.\nTo get a useful response from ChatGPT, prompting is key. If you only ask a simple question, you may not be happy with the results and decide to dismiss the technology too quickly, but today’s purpose is to have a deeper play in order to develop our critical thinking and information evaluation skills, allowing us to make informed decisions about utilising tools like ChatGPT in our endeavours. Basics of Prompting | Prompt Engineering Guide (promptingguide.ai) gives a nice quick walk-through of how to start writing good prompts or you can take a free course.\nHave a play trying to get ChatGPT to generate responses to some of the questions here (or come up with your own questions!) Critically evaluate the responses you receive from ChatGPT, what are its strengths and weaknesses, ethical considerations and challenges of using AI tools such as this.\n\nIs the information/response credible?\nAre there any biases in the responses?\nDoes the information align with what you know from other sources?\n\n\n\nActivity 3: Explore Computer Vision & Handwritten Text Recognition\nFind an image from somewhere on the web, or from your own collection, and see how each of these cloud services handles it! Try with some images of basic objects to see results (cars, fruit, bicycles…) and images with text within them.\n\nGoogle Cloud Vision API\nVisual Geometry Group - University of Oxford\nTranskribus (Try for free, but does require a free account to login)\n\n\n\nActivity 4: Exploring Hands-on AI (workshop materials)\nThe following workshop Exploring Hands-on AI was delivered to LIBER colleagues at the LIBER 2024 Annual Conference, but you can walk through many of the exercises at your own pace independently! Through a series of hands-on exercises, presented via Google Collab sheets, the workshop aimed to clarify how data science, and more particularly, generative AI systems based on Large Language Models (LLMs) can be applied within a library context and covers:\n\nGoogle Colab Basics\nIntroduction to Machine Learning\nObject detection with YOLO\nLarge Language Models\nRetrieval Augmented Generation",
    "crumbs": [
      "Topic Guides",
      "AI & Machine Learning in Libraries"
    ]
  },
  {
    "objectID": "ai-ml.html#recommended-readingviewing",
    "href": "ai-ml.html#recommended-readingviewing",
    "title": "AI & Machine Learning in Libraries",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\n\nLibrary of Congress Artificial Intelligence Planning Framework is an excellent and comprehensive resource for any library professional or institution interested in embarking on AI and Machine Learning implementations.\nAI for Humanists offers an exceptional overview in their guides to the ways in which humanists are using machine learning models in their research of cultural heritage collections. Though not particular to the library profession, their growing collection of use cases (with links to papers) and code tutorials are a great place to learn more, and get ideas for how these applications could be beneficial to our own goals of enhanced curation.\n\nColleagues from across GLAM institutions internationally are developing a Library Carpentry Intro to AI for GLAM (BETA) lesson and though it’s written to be delivered as an interactive workshop it is easily readable and contains loads of useful and practical context already around starting machine learning projects in GLAM.\nNearly every university in the world now has some sort of guide on AI but I have found Librarians & Faculty: How can I use Artificial Intelligence/Machine Learning Tools? - Research Guides at Northwestern University particularly useful in its coverage and topics selected and its librarian focus.\n\nThe AI4LAM community maintains an excellent curated list, Awesome AI for LAM, of resources, projects, and tools for using Artificial Intelligence in Libraries, Archives, and Museums.\n\n\n\n\nWatch the video\n\n\nThere are of course untold numbers of lists out there with resources for learning more about AI & Machine Learning but I think this particular guide is exceptionally useful in its coverage and topics selected, particularly as they are quite specifically for Librarians: Add’tl Reading for Librarians & Faculty - Using AI Tools in Your Research - Research Guides at Northwestern University",
    "crumbs": [
      "Topic Guides",
      "AI & Machine Learning in Libraries"
    ]
  },
  {
    "objectID": "ai-ml.html#finding-communities-of-practice",
    "href": "ai-ml.html#finding-communities-of-practice",
    "title": "AI & Machine Learning in Libraries",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nThe AI4Lam group is an excellent, engaged and welcoming international organisation dedicated to all things AI in Libraries, Archives and Museums. It’s free for anyone to join and is a great first step for anyone interested in learning more about this topic!",
    "crumbs": [
      "Topic Guides",
      "AI & Machine Learning in Libraries"
    ]
  },
  {
    "objectID": "project-overview.html",
    "href": "project-overview.html",
    "title": "ABOUT",
    "section": "",
    "text": "Digital Scholarship & Data Science Topic Guides for Library Professionals is a project first undertaken in 2023 as a joint collaboration between the Digital Scholarship & Digital Cultural Heritage (DSDCH) and the Data Science in Libraries (DSLib). It is an open and collaboratively curated training reference resource that aims to:"
  },
  {
    "objectID": "project-overview.html#are-ds-topic-guides-for-me",
    "href": "project-overview.html#are-ds-topic-guides-for-me",
    "title": "ABOUT",
    "section": "Are DS Topic Guides for me?",
    "text": "Are DS Topic Guides for me?\nAre you someone working in or around research libraries with an interest in learning more about how to do cool and interesting things with digital collections and data at your institution? Wondering how data science techniques can help you in your work? Are you interested in gaining valuable digital literacy skills and knowledge to support emerging areas of modern scholarship such as Digital Humanities? Do you need some of the technology jargon you hear about these days demystified?\nThen this resource is for you!\nIt is very important to us that the guides here are inclusive and intellectually accessible, challenging but not terrifying and as such we focus primarily on an introductory audience where no programming or particular digital skills are required.\nThough written from the research library professional perspective we think these guides will be useful for anyone currently (or aspiring to be) working in and around digital collections and data in heritage institutions:\n\nLibrary & Information Science students\nProject managers\nDevelopers\nInformation specialists\nMetadata Managers\nSubject librarians\nSystem librarians\nInstitutional leadership\n\nAnd so many more!"
  },
  {
    "objectID": "project-overview.html#project-history",
    "href": "project-overview.html#project-history",
    "title": "ABOUT",
    "section": "Project History",
    "text": "Project History\nThe impetus for this project has its roots in LIBER member British Library’s Digital Scholarship Training Programme, where for over a decade now, Digital Curators there have maintained bespoke learning resources on digital scholarship tools and methods for internal use as part of their staff training courses, talks reading group and Hack & Yacks. The team there has been looking for places to share these existing resources and expertise more widely with the sector in a dedicated collaborative learning space online.\nMeanwhile, both LIBER working groups noted that while the exponential expansion of digital collections, and the computational methods and tools to interact with them presented an opportunity for LIBER library professionals working within national, research, university to cultural heritage institutions, to not only support digital scholars on increasingly more complex computationally driven research, but also to apply such methods in the care and curation of heritage collections, capacity building in digital scholarship methods, including data science, was unevenly distributed across this community however.\nThough training for the sector around the use of data science and computational methods in the library context has grown considerably within the last decade, fifty-eight percent of LIBER Member respondents still noted as recently as 2019 ‘technical knowledge - such as coding or tool expertise’ as the primary deficit in their environments in Europe’s Digital Humanities Landscape: A Study From LIBER’s Digital Humanities & Digital Cultural Heritage Working Group.\nThe provision of these valuable resources remains fragmented online and across institutions at a local, national and international level. This leads to considerable inefficiency and inconsistent skills acquisition across LIBER member institutions and sustains an imbalance between the wide-spread ambitions of scholars to undertake computational research with library and heritage collections, and a paucity of institutions with capacity to enable these. Equally, it impedes research libraries and cultural institutions from benefiting from the digital transformations computational methods can offer."
  },
  {
    "objectID": "project-overview.html#core-project-team",
    "href": "project-overview.html#core-project-team",
    "title": "ABOUT",
    "section": "Core Project Team",
    "text": "Core Project Team\nThe site and content are maintained by the Co-Chairs and select Members of the collaborating WGs listed here who act as the core project delivery team and editors of this resource.\nJodie Double, Editor DSDCH\nPéter Király, Editor DSLib\nNora McGregor, Editor & Site Maintainer DSDCH\nPeter Verhaar, Editor DSLib"
  },
  {
    "objectID": "project-overview.html#community-contributors",
    "href": "project-overview.html#community-contributors",
    "title": "ABOUT",
    "section": "Community Contributors",
    "text": "Community Contributors\nEach Topic Guide is written by specific named contributors but we also welcome changes and contributions to this resource via logging issues or if you’re a more seasoned GitHub user, via pull requests to the github repository."
  },
  {
    "objectID": "project-overview.html#acknowledgements",
    "href": "project-overview.html#acknowledgements",
    "title": "ABOUT",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe project team would like to thank all of our LIBER working group members who have contributed to this resource and supported our endeavours here. We have taken great inspiration from other incredible training initiatives such as the British Library Digital Scholarship Training Programme, DH Literacy Guidebook and The Programming Historian in the development of this resource and thank those projects for paving the way!"
  },
  {
    "objectID": "project-overview.html#contact-us",
    "href": "project-overview.html#contact-us",
    "title": "ABOUT",
    "section": "Contact us",
    "text": "Contact us\nIf you’d like to ask a question of the project team you can please send an email to Nora McGregor, nora.mcgregor@bl.uk or any of Co-Chairs and select Members of the collaborating WGs listed here who act as the core project delivery team and editors of this resource.\nJodie Double, Editor DSDCH\nPéter Király, Editor DSLib\nNora McGregor, Editor DSDCH\nPeter Verhaar, Editor DSLib"
  },
  {
    "objectID": "reports.html",
    "href": "reports.html",
    "title": "SKILLS COMPETENCY FRAMEWORKS & REPORTS",
    "section": "",
    "text": "SKILLS COMPETENCY FRAMEWORKS & REPORTS\nThe following are Skills Competency Frameworks & Key Reports relating to supporting Digital Scholarship and Data Science for library staff\n\nLIBER Publications\n\nLIBER Digital Skills for Library Staff & Researchers Working Group - LIBER Europe have lots of resources here, including a very useful diagram Identifying Open Science Skills for Library Staff & Researchers\nLIBER Job Description Repository Contains job description examples for Digital Curator and other digital roles which reference the types of skills required for such work.\nEurope’s Digital Humanities Landscape: A Study From LIBER’s Digital Humanities & Digital Cultural Heritage Working Group is a report based on a Europe-wide survey run by LIBER’s Digital Humanities & Digital Cultural Heritage Working Group. The survey focused on digital collections and the activities libraries undertake around them. It covered the following topics and themes including staffing/skills\n\n\n\nKey Publications specific to digital scholarship and data science skills for research library staff\n\nThe British Library and the Arts and Humanities Research Council published a report on skills: Scoping Skills and Developing Training Programme for Managing Repository Services in Cultural Heritage Organisations. There is a very useful section (Section 3.) that references several other digital skills frameworks for research library staff across Europe.\nLippincott, Joan K. Directions in Digital Scholarship: Support for Digital, Data-Intensive, and Computational Research in Academic Libraries. Coalition for Networked Information, June 2023. https://doi.org/10.56561/ULHJ1168\nPadilla, Thomas. ‘Responsible Operations: Data Science, Machine Learning, and AI in Libraries’. OCLC, 26 August 2020.\nCordell, R. C. (2020). Machine Learning + Libraries: A Report on the State of the Field. LC Labs, Library of Congress.\nFederer L. Defining data librarianship: a survey of competencies, skills, and training. J Med Libr Assoc. 2018 Jul;106(3):294-303. doi: 10.5195/jmla.2018.306. Epub 2018 Jul 1. PMID: 29962907; PMCID: PMC6013124.\n\n\n\nGeneral Competencies for Librarians which include reference to digital\n\nAmerican Library Association (ALA) Library Competencies (Various roles): Library Competencies | Tools, Publications & Resources (ala.org) (USA)\nCanadian Association of Research Libraries Competencies for Librarians in Canadian Research Libraries Publications and Documents (including specifically Competencies-Final-EN-1-2.pdf (Canada)\nCILIP: the library and information association Professional Knowledge & Skills Base - (UK)"
  },
  {
    "objectID": "programming.html",
    "href": "programming.html",
    "title": "Programming for Librarians: Where to begin",
    "section": "",
    "text": "Even just a little bit of programming knowledge can bring huge efficiencies to the work of library professionals. Librarians can use code to solve problems, automate tasks, analyse data, and explore new tools for curating, collecting, and conducting research into our digital collections and data. And though many of these tasks could be done manually, it is often far more efficient to develop or make use of existing computer code to perform such tasks, particularly those which are by their nature highly repetitive. It can be very helpful for librarians, because of this, to become proficient in a programming language. Examples of programming languages include Python, R, Java, Perl or PHP.\n\n\nProgramming can be described as the process of using computers to address specific problems. Computer programs consist of a sequence of statements which explain how an activity can be automated. Programming languages generally implement a certain algorithm. An algorithm is an explicit and unambiguous description of the steps that need to be followed to arrive at a well-defined end result. The algorithm constitutes the logic of the program. The statements or commands in a programming language also need to be expressed in a format that computers can understand. They need to be expressed using a syntax, which prescribes how words, numbers, and punctuation marks ought to be used.\nThere are a number of different languages in which you can write computer code. While it is difficult to find reliable data about this, it feels safe to claim that Python and R are currently the most widely used languages for experimentation and automation in the library sector. When you start learning how to program, it is probably best to focus on one language initially and to develop skills in one specific programming language. It must be stressed, however, that when you develop your expertise in one concrete language, this will inevitably help you to improve your computational thinking and to broaden your understanding of how programming languages function in general. Once you have learned about the nuts and bolts of one language, it becomes much easier to gain mastery over another language.\n\n\n\nPython, firstly, is a free and open source, general purpose language. It is widely used to carry out data science tasks, such as cleaning, enriching, analysing and visualising data, and as a ‘glue’ language to create interfaces to ‘stick’ other software together. Python makes use of a sparse, readable and intuitive syntax, and this makes the language relatively easy to learn for beginners. Within the Python community, numerous code libraries have been shared which extend the basic functionalities of the language.\nR, is another free and open source programming language that was created originally with a specific focus: to support statistical analyses. Later it was developed towards a more general direction with the help of thousands of software packages created by the R community and today it is used to solve as many types of problems as Python or Java. R’s syntax and its most important data structures are quite different from that of Python, and it requires a different way of thinking.\nBecause of this strong data analysis background we suggest you choose R if you would like to run core data analysis tasks that require a more advanced statistical toolbox, and does not require many additional steps before and after the calculations. Python on the other hand provides somewhat less statistical functionalities, but supports much more general purpose tasks out of the box like file management, string, date and time manipulation, and computer networking. None of these tasks are impossible with the other programming languages, but what they are designed for and what they make easy are different. In data science related tasks they are quite close, e.g. Python’s Pandas library and R’s data frames (particularly in the Tidyverse package) have similar functionalities. Another - subjective - factor of the decision could be the aesthetics of the graphics Python and R produces: both of them have more than one plotting library, each are easy to recognise by their distinctive graphical style.",
    "crumbs": [
      "Topic Guides",
      "Programming for Librarians: Where to begin"
    ]
  },
  {
    "objectID": "programming.html#introduction",
    "href": "programming.html#introduction",
    "title": "Programming for Librarians: Where to begin",
    "section": "",
    "text": "Even just a little bit of programming knowledge can bring huge efficiencies to the work of library professionals. Librarians can use code to solve problems, automate tasks, analyse data, and explore new tools for curating, collecting, and conducting research into our digital collections and data. And though many of these tasks could be done manually, it is often far more efficient to develop or make use of existing computer code to perform such tasks, particularly those which are by their nature highly repetitive. It can be very helpful for librarians, because of this, to become proficient in a programming language. Examples of programming languages include Python, R, Java, Perl or PHP.\n\n\nProgramming can be described as the process of using computers to address specific problems. Computer programs consist of a sequence of statements which explain how an activity can be automated. Programming languages generally implement a certain algorithm. An algorithm is an explicit and unambiguous description of the steps that need to be followed to arrive at a well-defined end result. The algorithm constitutes the logic of the program. The statements or commands in a programming language also need to be expressed in a format that computers can understand. They need to be expressed using a syntax, which prescribes how words, numbers, and punctuation marks ought to be used.\nThere are a number of different languages in which you can write computer code. While it is difficult to find reliable data about this, it feels safe to claim that Python and R are currently the most widely used languages for experimentation and automation in the library sector. When you start learning how to program, it is probably best to focus on one language initially and to develop skills in one specific programming language. It must be stressed, however, that when you develop your expertise in one concrete language, this will inevitably help you to improve your computational thinking and to broaden your understanding of how programming languages function in general. Once you have learned about the nuts and bolts of one language, it becomes much easier to gain mastery over another language.\n\n\n\nPython, firstly, is a free and open source, general purpose language. It is widely used to carry out data science tasks, such as cleaning, enriching, analysing and visualising data, and as a ‘glue’ language to create interfaces to ‘stick’ other software together. Python makes use of a sparse, readable and intuitive syntax, and this makes the language relatively easy to learn for beginners. Within the Python community, numerous code libraries have been shared which extend the basic functionalities of the language.\nR, is another free and open source programming language that was created originally with a specific focus: to support statistical analyses. Later it was developed towards a more general direction with the help of thousands of software packages created by the R community and today it is used to solve as many types of problems as Python or Java. R’s syntax and its most important data structures are quite different from that of Python, and it requires a different way of thinking.\nBecause of this strong data analysis background we suggest you choose R if you would like to run core data analysis tasks that require a more advanced statistical toolbox, and does not require many additional steps before and after the calculations. Python on the other hand provides somewhat less statistical functionalities, but supports much more general purpose tasks out of the box like file management, string, date and time manipulation, and computer networking. None of these tasks are impossible with the other programming languages, but what they are designed for and what they make easy are different. In data science related tasks they are quite close, e.g. Python’s Pandas library and R’s data frames (particularly in the Tidyverse package) have similar functionalities. Another - subjective - factor of the decision could be the aesthetics of the graphics Python and R produces: both of them have more than one plotting library, each are easy to recognise by their distinctive graphical style.",
    "crumbs": [
      "Topic Guides",
      "Programming for Librarians: Where to begin"
    ]
  },
  {
    "objectID": "programming.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "programming.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Programming for Librarians: Where to begin",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nProgramming languages can be used for a variety of library tasks.\n\nConverting one data format into another data format. A computer system may export descriptions in the MARCXML format, while another application may demand JSON input. Such transformations can be carried out using a computer program, for example using the PyMarc package.\nDownloading large numbers of files using computer code. This could include interacting with APIs for web services like OCLC, or library information systems like Alma, Aleph, or Folio. See the DS Topic Guide on APIs\nAnalysing and visualising data to interpret the data or to see patterns in large datasets. See the DS Topic Guides on Working with Data, Collections as Data, and Data Visualisation.\nCreating AI and Machine Learning applications. See the DS Topic Guide on AI and ML\n\nThe British Library, The National Archives and Birkbeck University in the UK partnered on a trial of a one-year part-time postgraduate Certificate (PGCert), Computing for Cultural Heritage, as part of a £4.8 million University skills drive in 2019-2021. The 20 staff projects devised and undertaken by the students demonstrate the wide range of what programming skills can enable for those working in a library setting.\nThe code4lib journal publishes many articles with case studies on applications in library contexts.",
    "crumbs": [
      "Topic Guides",
      "Programming for Librarians: Where to begin"
    ]
  },
  {
    "objectID": "programming.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "programming.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Programming for Librarians: Where to begin",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nStarting any of these tutorials requires somewhere to write code. This will likely either be in an Integrated Development Environment (IDE), a code editing program that has lots of helpful features for writing and running code, or a tool that runs in-browser. Which you choose is influenced by personal taste and whether institutional IT restrictions allow installing software on your computer. If you can install software on your laptop then the tutorials below will contain all the information needed. If not then we have tried to include options that work in-browser without installing anything.\n\nPython Tutorials\nThe tutorials below recommend local installs of Python. If this isn’t possible you can try Google Colab, a ‘notebook’ style interface, or this online IDE, for an IDE interface. Notebooks use different blocks of code called cells that can be run individually, and are good for experimentation. IDEs run an entire code file each time, and are better for reproducibility and larger code projects. * The Bits and Bots study group is an excellent place to begin to get to grips with Python in a playful way. The study group was founded by professionals in digital preservation Francesca Mackenzie (National Archives UK), Lotte Wijsman, and Susanne van den Eijkel (National Archives of the Netherlands) who realised that a basic level of technical skills can significantly enhance their work in archives. You can join their study group when it’s running or just follow along with the course materials on their project github at your own pace: - Guide to building games with Python Modules 1-8. pdf - Guide to Website/Twine Based Games Modules 1-8.pdf\n\nTo develop a basic understanding of the main features of Python, you can also follow the Introductory course on Python that was developed at the University of Leiden. It is a self-paced course which you can follow to familiarise yourself with central concepts such as variables, operators, flow control, data structures and functions. The tutorial also explains some of the code libraries which can be used more specifically for data science tasks such as data acquisition, working with APIs, data analysis and data visualisation. The tutorial includes a large number of exercises with which you can hone your programming skills, together with answers which can help you if you get stuck.\nOnce you’ve gained a basic proficiency of the central Python concepts, there are a number of useful resources to improve your programming skills specifically with library related materials.\n\nYou can follow the tutorial Python for Librarians, which was developed by the Library Carpentry.\nTim Sherratt: GLAM Workbench (2021- is “a collection of Jupyter notebooks to help you explore and use data from GLAM institutions (that’s galleries, libraries, archives, and museums). It includes tools, tutorials, examples, hacks, and even some pre-harvested datasets.” Sherratt focuses on Australian and New Zealand datasets, but you can reuse the code for other sources. The notebooks are written in Python using Jupyter Notebook, a web-based interactive computing platform that combines live code, equations, narrative text, visualisations, interactive dashboards and other media. You can find many tutorials and GLAM related scripts in this form.\nProgramming Historian is a set of programming tutorials (both in Python and R) focusing on particular computational problems (such as network analysis, data managements, APIs, machine learning) made mainly for historians, but several tutorials use GLAM data.\n\n\n\n\nR Tutorials\nTo get started with R we recommend the Library Carpentry’s tutorial Introduction to R which teaches you how to do basic data science tasks on tabular data, such as spreadsheets or relational database tables. If you can’t install R locally then you can use Posit Cloud, which runs the standard interface to R in your browser.",
    "crumbs": [
      "Topic Guides",
      "Programming for Librarians: Where to begin"
    ]
  },
  {
    "objectID": "programming.html#recommended-readingviewing",
    "href": "programming.html#recommended-readingviewing",
    "title": "Programming for Librarians: Where to begin",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\n\nPython\nPython.org/Documentation is a great place to start as it holds loads of useful introductions for getting started, including a Beginner’s Guide.\nThe open access Python for Data Analysis, 3rd edition by Wes McKinney.\nPaul Vierthaler’s Hacking the Humanities Tutorials provides a quite useful video lecture series introducing python to beginners.\n\n\nR\nThe R: Documentation webpage contains a huge amount of useful documentation to help you on your way.\nWe can also highly recommend this recently published book on R, Hands-On Data Science for Librarians written by Sarah Lin and Dorris Scot (Boca Raton:CRC Press, 2023, ISBN 978-1-032-10999-2) which hopefully your institution may have a copy of.\nThough not written specifically for librarians, but probably the best introduction to the topic of data science with R is R for Data Science. 2nd edition by Hadley Wickham, Mine Çetinkaya-Rundel and Garrett Grolemund (O’Reilly, 2023, ISBN 978–1-492-09740-2), and available free online.",
    "crumbs": [
      "Topic Guides",
      "Programming for Librarians: Where to begin"
    ]
  },
  {
    "objectID": "programming.html#finding-communities-of-practice",
    "href": "programming.html#finding-communities-of-practice",
    "title": "Programming for Librarians: Where to begin",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nWhen you’re first getting started in programming, being able to ask others questions when you get stuck is so important! We recommend seeking out local study groups dedicated to learning programming (for example one like the Bits and Bots study group), or maybe even think about starting your own with like-minded colleagues!\nCode4lib is a volunteer-driven collective of folks that has been around since 2003 and amongst their many resources are a large range of chat rooms for folks interested in the convergence of technology and cultural heritage. Visit Chat | Code4Lib to find the discussion you need or join their mailing list.\nYou might also want to have a look on Github, which is one of the largest source code repositories on the web, for the “code4lib” tag to denote repositories that are somewhat relevant for the library community. They are not tutorials, but during your learning process you can check if there is a software solution already for your particular problem, and you can even try to engage with one or more tools as a contributor. Contributions are always welcome on #code4lib projects, and even if you have just begun programming you can provide important feedback about the usage of a tool, or you can improve code documentation as a first step. See our DS Topic Guide on for guidance on how to contribute to projects on GitHub.\n\n\n\n\n\n\nAre we missing something?\n\n\n\n\n\nWe’d love to hear it! Suggest edits by opening a new Issue or adding to the discussion on existing Issues on the project Github. If you’re new to GitHub don’t worry, we have a DS Topic Guide for that: GitHub: How to navigate and contribute to Git-based projects! Or just drop us a line at digitalresearch@bl.uk!",
    "crumbs": [
      "Topic Guides",
      "Programming for Librarians: Where to begin"
    ]
  },
  {
    "objectID": "workingwdata.html",
    "href": "workingwdata.html",
    "title": "Working with Data",
    "section": "",
    "text": "Data is all around us. Libraries are full of data, but staff often think they do not work with data. Have you ever been asked how many loans a book has had? What about how many visitors came in at the weekend? Or if a certain book is available? These are all data questions, answerable with quantifiable facts.\nData is often considered to be a number, like the number of times a book has been loaned, but what about who borrowed it, or what their review was. These are all different forms of data, and can provide all new insights to the popularity of a book - maybe a book was borrowed 10 times, but in fact got 10 1-star reviews, versus a book borrowed 5 times with 5 5-star reviews. Using this data, which one would you recommend?\nThis guide will explore the use of data within libraries through proposing questions to think through when undertaking your own data tasks, alongside practical examples from libraries around the world.\n\n\nData is often overwhelming (the term ‘Big Data’ floats around a lot!), but that shouldn’t stop you from exploring it, because it can greatly improve your work.\nFirst, let’s establish some definitions (for this guide at least) of common terms surrounding working with data, particularly library related data:\n\n\n\n\n\n\n\n\nTerm\nDefinition\nLibrary Example\n\n\n\n\nData\nA collection of values that convey information\nData about visitors’ experience of a library\n\n\nData Point\nA single point of data\nOne user review of a library\n\n\nDataset\nA collection of data from a single source or about a single subject\nA spreadsheet of all user reviews of the library\n\n\nMetadata\nData that provides information about other data. Metadata can also be data, for example if we analysed all the titles in a catalogue, the titles are metadata about works, but become the data for our analysis\nColumn names in a spreadsheet containing user reviews, or metadata in a library catalogue like price, ISBN, author, and title\n\n\nDatabase\nA place to store data in an organised way\nThe main catalogue\n\n\nDataframe\nA table of data, particularly in analysis or visualisation software\nA dataframe of user reviews in an analysis package\n\n\nQualitative Data\nDescriptive, non-numerical data that captures characteristics, experiences, or meanings through words, observations, or categories\nThe text part of reviews where how library users explain how they felt about their visit\n\n\nQuantitative Data\nNumerical data that can be analysed using statistical techniques\n1-5 star ratings of user visits\n\n\nData Analytics\nCleaning, organizing, visualising, and interpreting existing data to support decision-making\nLooking at trends in user visits to see if demand is going up or down\n\n\nData Science\nCombining statistics, programming and machine learning to further examine, interpret, and make predictions from data\nCreating a book recommender algorithm\n\n\n\n\n\n\nHere are some examples to get you thinking about what data points stem from a library. Consider the different aspects of a library below, which do you interact with? What data might they generate? Do you consider yourself to work with data?\n\n\n\n\n\n\n\nArea\nExample Data Points\n\n\n\n\nBuilding\n- Footfall / entry counts - Types of facilities and their usage (reading room reservations / lockers) - Opening hours - Environmental Information (light, humidity, temp) - Maintenance records - Accessibility services - Security incidents - Wif-fi usage patterns\n\n\nInstitution\n- Staffing demographics and roles - Departments - Budget allocation and spend - Qualifications & education - Skills & training - Suppliers - Stakeholders/Partnership/Donor Data\n\n\nCollection\n- Catalogue Metadata - Collection item counts by format - Digitisation status - Usage statistics (downloads, API hits) - Subscriptions - Purchasing/Acquisitions - Systems and software in use\n\n\nEngagement\n- Event attendance - Helpdesk queries and topics (in-person, email, online chat etc.) - User feedback - Website analytics - Tours & Exhibition attendance",
    "crumbs": [
      "Topic Guides",
      "Working with Data"
    ]
  },
  {
    "objectID": "workingwdata.html#introduction",
    "href": "workingwdata.html#introduction",
    "title": "Working with Data",
    "section": "",
    "text": "Data is all around us. Libraries are full of data, but staff often think they do not work with data. Have you ever been asked how many loans a book has had? What about how many visitors came in at the weekend? Or if a certain book is available? These are all data questions, answerable with quantifiable facts.\nData is often considered to be a number, like the number of times a book has been loaned, but what about who borrowed it, or what their review was. These are all different forms of data, and can provide all new insights to the popularity of a book - maybe a book was borrowed 10 times, but in fact got 10 1-star reviews, versus a book borrowed 5 times with 5 5-star reviews. Using this data, which one would you recommend?\nThis guide will explore the use of data within libraries through proposing questions to think through when undertaking your own data tasks, alongside practical examples from libraries around the world.\n\n\nData is often overwhelming (the term ‘Big Data’ floats around a lot!), but that shouldn’t stop you from exploring it, because it can greatly improve your work.\nFirst, let’s establish some definitions (for this guide at least) of common terms surrounding working with data, particularly library related data:\n\n\n\n\n\n\n\n\nTerm\nDefinition\nLibrary Example\n\n\n\n\nData\nA collection of values that convey information\nData about visitors’ experience of a library\n\n\nData Point\nA single point of data\nOne user review of a library\n\n\nDataset\nA collection of data from a single source or about a single subject\nA spreadsheet of all user reviews of the library\n\n\nMetadata\nData that provides information about other data. Metadata can also be data, for example if we analysed all the titles in a catalogue, the titles are metadata about works, but become the data for our analysis\nColumn names in a spreadsheet containing user reviews, or metadata in a library catalogue like price, ISBN, author, and title\n\n\nDatabase\nA place to store data in an organised way\nThe main catalogue\n\n\nDataframe\nA table of data, particularly in analysis or visualisation software\nA dataframe of user reviews in an analysis package\n\n\nQualitative Data\nDescriptive, non-numerical data that captures characteristics, experiences, or meanings through words, observations, or categories\nThe text part of reviews where how library users explain how they felt about their visit\n\n\nQuantitative Data\nNumerical data that can be analysed using statistical techniques\n1-5 star ratings of user visits\n\n\nData Analytics\nCleaning, organizing, visualising, and interpreting existing data to support decision-making\nLooking at trends in user visits to see if demand is going up or down\n\n\nData Science\nCombining statistics, programming and machine learning to further examine, interpret, and make predictions from data\nCreating a book recommender algorithm\n\n\n\n\n\n\nHere are some examples to get you thinking about what data points stem from a library. Consider the different aspects of a library below, which do you interact with? What data might they generate? Do you consider yourself to work with data?\n\n\n\n\n\n\n\nArea\nExample Data Points\n\n\n\n\nBuilding\n- Footfall / entry counts - Types of facilities and their usage (reading room reservations / lockers) - Opening hours - Environmental Information (light, humidity, temp) - Maintenance records - Accessibility services - Security incidents - Wif-fi usage patterns\n\n\nInstitution\n- Staffing demographics and roles - Departments - Budget allocation and spend - Qualifications & education - Skills & training - Suppliers - Stakeholders/Partnership/Donor Data\n\n\nCollection\n- Catalogue Metadata - Collection item counts by format - Digitisation status - Usage statistics (downloads, API hits) - Subscriptions - Purchasing/Acquisitions - Systems and software in use\n\n\nEngagement\n- Event attendance - Helpdesk queries and topics (in-person, email, online chat etc.) - User feedback - Website analytics - Tours & Exhibition attendance",
    "crumbs": [
      "Topic Guides",
      "Working with Data"
    ]
  },
  {
    "objectID": "workingwdata.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "workingwdata.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Working with Data",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\n\nData with purpose\nLibrary data should be collected, stored and actively managed with purpose. In fact, particularly if it is personally identifiable data, ensuring you have a well defined reason for collecting it (and data management plans in place) may actually be required by law. The purpose of data collection could be any of the following:\n\nProvide insights to pertinent questions, and thus make informed decisions.\nProvide evidence of a decision or conclusion.\nExplore relationships / trends, to tell a story and provide insights.\nExplain (and display) complex information more effectively.\n\nData can provide additional power when making a decision (big or small) - knowing exactly how many users visit at the weekend can help the library plan a realistic budget for staffing to stay open on the weekend. Conversely if your data collection reveals visitor numbers are low on weekends, those numbers can help direct your institution to investigate this further and make changes to attract more users or change opening hours.\nData can inform Library decisions about:\n\nCollection Management: What is being used, when, by who, and why (if you have a review system).\n\nExample: Within our collection, how often do users borrow material published before 2000? (But remember, borrowing is not the only form of usage!).\n\nOutreach: Who forms our community, what matters to the community.\n\nExample: Does the demographic of people attending our events reflect the demographic of people in our wider community? Which events could make the library more appealing to the wider community?\n\nFunding / Expenditure: Use data about patron usage, reader requests or patron feedback as part of an application for additional funding or staffing to prove your standing within the community.\n\nExample: How much would it cost to buy an additional copy of a book, if it had more than three reservations at a given time?\n\nReviewing Success: When starting a new project or approach, consider how it will be measured as a success. Knowing how to measure the impact of a project will enable you to actively reflect and produce solid facts about the success of your project.\n\nExample: How can we measure patron satisfaction currently, so that at the end of the project a 10% increase can be quantified and thus the project can be considered a success?\n\n\nHere are a few real world examples of libraries utilising their data to make powerful changes.\n\nTelling Stories with Library Data July 2021 In this article by Michael Cummings, Assistant Museum Librarian for Library Systems, he explores how The Metropolitan Museum of Art (New York) Watson Library produces data visualisations using Microsoft Power BI in regards to their library activities. Using the library management system (data which they already had), Google Analytics, digital collections and some manual tallies, they explored six years of library data. This exploration demonstrated the benefit of moving their blog from an external website, created an index of African American Artists, and allowed sharing library metrics for the previous six years publicly.\nWhen is a Year Complete? October 2023 It is well known that publications databases take time to update, but how long? Collection Analysis Librarian at Iowa State University, Eric Schares, wanted to know how long to wait before being able to analyse a calendar year of publications, and set about doing so by comparing data from three different publication services (Dimensions, Web of Science and Open Alex). The data (which continues to be updated), documented the rise of Open Alex, and demonstrated part of the effectiveness of each service for Iowa State University - however he also shares his code and thus you can replicate this data question at your own institution.\nUse of Institution Data Analysis for Publisher Negotiations July 2023 | Utilising Data to Understand the Institutions Relationship with a Publisher March 2024 Data Analyst at the University of Cambridge Library Niamh Malin, was responsible for supporting the many publisher negotiations. The first paper addresses the use of Microsoft Excel and Dimensions publications data to document the relationship of the university with Springer Nature. The second presentation documents the transition to Microsoft Power BI to create data dashboards which ensure every publisher negotiation is informed by usage and publications data.\nUsing Analytics to Extract Value from the Library’s Data - Event Part One: Analytics Behind the Scenes and Part Two: Actionable Data Analysis September 2018 The National Information Standards Organization (NISO) hosted a two part webinar in 2018 about extracting value from library data. The slides for the six presentations are available, and cover topics such as; setting up a data analysis strategy, analysing metadata, utilising data visualisations, actioning data insights for utilising physical spaces and building confidence in your data analysis skills.\nLibraries support data-driven decision making February 2024 An OCLC-Liber blog post by Rebecca Bryant, Senior Programme Officer at the OCLC Research Library Partnership, following their “Building for the future” program. During the program librarians gathered to discuss their understanding and practical interpretations of data driven libraries.\nUtilising new modes of data to enhance research strategy and collaboration November 2023 Digital Science, the creators of Altmetric, look at how institutions can get high-quality insights into research by standardising their data systems.",
    "crumbs": [
      "Topic Guides",
      "Working with Data"
    ]
  },
  {
    "objectID": "workingwdata.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "workingwdata.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Working with Data",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nThe process of utilising data is a cycle, a standard process model that describes a common approach to data is CRISP-DM (Cross-industry standard process for data mining). It has six stages, listed below.\n\nThis section will enable you to walk through an assessment of your data, through a variety of leading questions, at each stage of the cycle. You can use it to have a think and guided discussion around data held in your Library. How long each phase takes will depend entirely on the project, there is no right or wrong, and it is cyclical, you will return to questions as you develop the project. These are structured as questions because one guide cannot have all the answers, and therefore giving a variety of questions will hopefully ensure you do not miss anything, and learn much more which is relevant to your project. This approach will enable you to assess if the data you have is relevant to the question, and if it has the potential to answer it correctly.\nThis guide hopes to calm your nerves about using data, and to see that you are capable of answering data questions confidently.\n\nBusiness Understanding\nThe Business Understanding phase focuses on understanding the objectives and requirements of the project. Here you are planning your project.\n\nWhat questions are you answering?\nWho are your stakeholders?\nWhat are the criteria and limitations to the project?\nWhat is the goal of using this data?\nWhat resources are (or are not) available?\nWhat is the timeline on the project? And the data input specifically?\nWhat stage of the project requires data analysis?\nWhat is the expected outcome of the data analysis?\n\n\n\nData Understanding\nData Understanding drives the focus to identify, collect, and analyse the relevant datasets.\n\nCan you access the data you require? What is the source?\nWhat data (and metadata) is missing?\nHow reliable is the data?\nWhat relationships are relevant within the data?\nWho is involved in gathering and preparing the data?\n\n\n\nData Preparation\nA common rule of thumb is that 80% of the project is Data Preparation. Note that many of the prompting questions below relate specifically to tabular data as this is the kind of data that many folks in libraries will encounter when first starting to work with data. Tidy data for Librarians for instance is an excellent resource with lots more guidance and advice around preparing data, with a particular focus on spreadsheet/tabular data. But do remember that in the library domain there are many other kinds of data representations and data types.\n\nWhat data is not necessary from the dataset?\nHow are errors or duplications handled?\nWhat new attributes or formulas are required?\nHow do datasets interact with one another?\nIs the data formatted correctly for the analysis you need to do?\nWhat acronyms are in use? Are they formatted correctly?\nHas the data been standardised?\nAre the column names and/or data labels useful and appropriate?\nDoes every database have a unique identifier column?\nHow do you handle missing metadata?\nAre you editing the master / only copy?\nHave you documented the process so that you can provide evidence if needed?\nIs the data in a clear and useable format?\nHave all merged cells been removed / updated?\n\n\n\nModelling\nModelling is often regarded as the most exciting work, but is also often the shortest phase. Now is the time to build and assess various models or visualise your data. See our DS Topic Guide on Data Visualisation for more guidance.\n\nWhat conclusions can be drawn from the data?\nWhat visualisations are appropriate to display the data?\nWhat comparisons and relationships should be highlighted to align with your initial goal?\nDoes the data contradict the hypothesis? Why?\nDo the formulas need to be live? Tip: Live formula within a spreadsheet can cause it to be slow and large in size.\n\n\n\nEvaluation\nThe Evaluation phase looks more broadly at the data project and what to do next.\n\nDid the data fulfil the goal of this project?\nWhat was unable to be achieved?\nWhat is required for the next data project to run more efficiently?\nHas the project been completed successfully?\n\n\n\nDeployment\nDepending on the requirements, the Deployment phase can be as simple as emailing a graph, or as complex as publishing a live dashboard of intricate data and visuals.\n\nWho requires the outcome of this project?\nWill the outcome need to be presented in multiple formats?\nWill this be repeated again? Has it been documented?\nWhere will the data and outcome be stored?\n\n\nStoring a datafile / spreadsheet\nOnce the data exists, it must be stored in an accessible and organised way. There are a variety of software, or open repositories online (see the DS Topic Guide on Supporting Open Research (Science), which can store your data, it is important to take some time to decide on the right destination.\n\nAre the files/your dataset in the right format for its intended reuse/requirements of the repository? (see guidance on different file formats and Choosing the right format for Open Data)\nWill it need to be accessed online and/or offline?\nDoes the data require a software licence to access?\nWhat backups are required?\nHow long are files required to be stored? Will they be deleted or archived?\nHave you established a naming convention for files? Does it account for versions, dating and author? Tip: Have titles be meaningful, in a relevant order, and versions discernible.\nIs there clear documentation of the processes undertaken? Therefore ensuring the task can be repeated, or to justify any conclusions. Tip: This should include the source of the data, any cleaning steps made, and all relevant metadata. Consider using Datasheets for Datasets.",
    "crumbs": [
      "Topic Guides",
      "Working with Data"
    ]
  },
  {
    "objectID": "workingwdata.html#recommended-readingviewing",
    "href": "workingwdata.html#recommended-readingviewing",
    "title": "Working with Data",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\n\nDefining Data Librarians\n\nDefining data librarianship: a survey of competencies, skills, and training (July 2018). Federer aims to define data librarianship by exploring the skills and knowledge that data librarians utilise and the training that they need to succeed.\nIntroduction to Databrarianship: The Academic Data Librarian in Theory and Practice (2016). Thompson and Kellam explore the diverse field of data librarianship, highlighting its key commitment to accessible data.\nData librarianship: a day in the life (2011). Interviews with data librarians across the world highlight the challenge and opportunities of data in libraries, including the creation of data services within an academic library.\nThe future for numeric data services (2011). Exploring the future of data librarians, with trends in visualisation, mapping, standardisation, citation and data management plans.\nLibrarian roles in the digital data-driven world (September 2021). Thai/English paper exploring the role of data librarians as someone who continues to acquire new knowledge and skills with technology and the digital era.\nCILIP (Chartered Institute of Library and Information Professionals) provides definitions on the data science roles available within a library.\n\n\n\nLibrary Endorsed Resources\n\nLIBER members have created the Digital Scholarship & Data Science Topic Guides for Library Professionals, which this guide is a part of. Guidance also includes Data Visualisation and Collections as Data, and Open Research (Open Science) which further detail important elements of working with library data.\nJISC has published resources to support data-driven decisions, including interactive insights on graduate outcomes and conducting online surveys as a form of data gathering.\nThe DSVIL (Data Science and Visualization Institute for Librarians) provides great guidance and resources for finding, cleaning, analysing, visualising and managing data.\nThe Bodleian Library, University of Oxford, hosts a variety of resources to support data analysis (including audio-visual), data mining, and visualisation tools.\nDuke University Libraries have a plethora of resources for data science, data management and data visualisation freely available.\nARL (Associations of Research Libraries) demonstrate the impact data analytics can have within libraries and library communities.\nIFLA (International Federation of Library Associations and Institutions) has a variety of data analytics resources which cover using specific software, trends in the news, and practical applications of data within libraries.\nThe PLA (Public Library Association) (which is part of ALA, the American Library Association), have developed data tools to enable public libraries to be compared across multiple metrics.\n\n\n\nLearning Data Skills Beyond Libraries\nThere are numerous platforms dedicated to data skills more generally, listed below are some of the most popular for beginning your data journey.\n\nThe Carpentries: Community-led coding and data science courses for researchers and librarians, with three specialties; Data Carpentry (data skills for conducting research), Library Carpentry (software and data skills for librarians), and Software Carpentry (lab skills for research computing).\nDatacamp: great for free cheat sheets (webpage or PDF) on data literacy, understanding data knowledge levels and data storytelling. As well as resources and courses for a variety of coding and data software.\nLinkedIn Learning: a wonderful resource of videos for anything from coding to people management to making the perfect Microsoft Excel graph.\nGoogle Skillshop: Training and certification in google analytics which could support queries about library engagement.",
    "crumbs": [
      "Topic Guides",
      "Working with Data"
    ]
  },
  {
    "objectID": "workingwdata.html#finding-communities-of-practice",
    "href": "workingwdata.html#finding-communities-of-practice",
    "title": "Working with Data",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nDepending on what particular area you’re interested in learning more about with regards to working with data, there are any number of communities out there to help you along the way such as: * The Collections as Data Community has an active Google Group open to all where members can ask data related questions and seek advice and support. * Look to local universities, particularly Digital Humanities or Data Science groups within them, for summer schools or other group training open to the public which may provide you with the opportunity to meet like-minded individuals while learning new data skills. For example Cambridge Universities offer online and in-person Cambridge Cultural Heritage Data School annually or the European Summer University in Digital Humanities “Culture and Technology” are great options for learning data skills in a community setting and are open to library professionals, not just academics. * OpenRefine is a free, open source tool for data wrangling and is especially popular with librarians! They have a great OpenRefine community and lots of ways to get in touch for support and to share experiences including the OpenRefine Forum. * If managing research data is a particular area of interest of yours, the LIBER Research Data Management Working Group supports libraries to explore and gain skills to develop services for implementing FAIR research data, data management during research projects, setting up data archives/repositories to store and publish research datasets.",
    "crumbs": [
      "Topic Guides",
      "Working with Data"
    ]
  },
  {
    "objectID": "template.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "template.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "——",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)"
  },
  {
    "objectID": "template.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "template.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "——",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)"
  },
  {
    "objectID": "template.html#recommended-reading-viewing",
    "href": "template.html#recommended-reading-viewing",
    "title": "——",
    "section": "Recommended Reading & Viewing",
    "text": "Recommended Reading & Viewing"
  },
  {
    "objectID": "template.html#finding-communities-of-practice",
    "href": "template.html#finding-communities-of-practice",
    "title": "——",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice"
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "GitHub: How to navigate and contribute to Git-based projects",
    "section": "",
    "text": "GitHub is a collaborative software development platform. Like a kind of Google Docs for coders, it began as a tool for them to collaboratively work on software development projects, ie, allowing different coders to contribute to parts of an overall project, asynchronously and across teams and even international borders, keeping track of changes all the while to make sure the end result is one cohesive software.\nGitHub today however is used for many more different purposes by people in all different fields, not just software developers and software projects, to:\n\nrecord and share information on a collaborative project\nwork asynchronously and internationally\ntest and store scripts and technical documentation\nbuild a public website for a project\n\nGitHub allows people to collaboratively work on and share projects (called ‘repositories’) so that others can copy them, either to adapt or to contribute back to the original project. That’s because any ‘public’ repository of files on GitHub is accessible via the web. (Note that public repositories are created by default when you start a project, but it is also possible to make your repositories private if you like in the settings).\nRepositories may be filled with code, but they may also be filled with pages of text written in markdown and this markdown can be rendered by static site generators such as Jekyll or Hugo into HTML that is understood by the browser. This very website you’re reading from right now is being hosted on GitHub!\nThis guide will help you to understand how to practically navigate and contribute to projects like this one hosted via GitHub, and will demystify some of the basic actions and jargon around using GitHub along the way.\n\n\nBefore we begin, it can be helpful to first grasp what Git, GitHub and GitHub Pages each are and how they relate to each other:\n\nGit is a distributed open source version control system. Version control allows you to track changes in a set of files. It does this by taking snapshots of repositories at each stage of development - in Git these snapshots are known as ‘commits’. In software development this means things can be tested and rolled back, and enables sharing of stages of development. Git can be used on its own but when used with GitHub has more potential for collaborative projects. You can download and install Git on your own machine. It runs in the command line and there are desktop GUIs available. Git for Humans (Alice Bartlett talk at UX Brighton 2016) is a nicely accessible introduction to the purpose and uses of Git.\nGitHub is a web-based software development platform. It has many of Git’s features and can host Git repositories but also provides a web interface and additional functionalities. You can view millions of repositories hosted there. Many are openly licensed, meaning you can freely copy (or fork) them, either to adapt for yourself or to contribute back to the original project. By default the files and folders in a repository you create are public, and with a little bit of extra code, a repository can also be turned into a more public-facing project website, blog or wiki using a feature called GitHub Pages.\nGitHub Pages is a feature within GitHub to turn any repository or project into a website. GitHub can also be used to host a site that has been created using other software packages. One example of such a software is RStudio, a visual editor for Quarto Markdown which is what we use for this DS Topic Guides project!\n\n\n\n\nA barrier to GitHub for beginners is that it has its own terminology for common tasks and actions. While off-putting at first, they do serve to outline core concepts of both Git and GitHub. When using Git in particular, these are important to understand as they relate to git commands such as fork, pull, commit, clone, or branch. There are many quick guides and cheat sheets to this terminology on the web which can be useful to use as a reference while familiarising yourself:\n\nGitHub Docs Glossary\nGit for Librarians Glossary\n\nIn the next section, we will demonstrate the actions behind these keywords as we walk through three types of ways librarians regularly interact with GitHub based projects:\n\nWriting an Issue\nContributing code/content to an existing project\nRe-using existing code/content for a new project",
    "crumbs": [
      "Topic Guides",
      "GitHub: How to navigate and contribute to Git-based projects"
    ]
  },
  {
    "objectID": "github.html#introduction",
    "href": "github.html#introduction",
    "title": "GitHub: How to navigate and contribute to Git-based projects",
    "section": "",
    "text": "GitHub is a collaborative software development platform. Like a kind of Google Docs for coders, it began as a tool for them to collaboratively work on software development projects, ie, allowing different coders to contribute to parts of an overall project, asynchronously and across teams and even international borders, keeping track of changes all the while to make sure the end result is one cohesive software.\nGitHub today however is used for many more different purposes by people in all different fields, not just software developers and software projects, to:\n\nrecord and share information on a collaborative project\nwork asynchronously and internationally\ntest and store scripts and technical documentation\nbuild a public website for a project\n\nGitHub allows people to collaboratively work on and share projects (called ‘repositories’) so that others can copy them, either to adapt or to contribute back to the original project. That’s because any ‘public’ repository of files on GitHub is accessible via the web. (Note that public repositories are created by default when you start a project, but it is also possible to make your repositories private if you like in the settings).\nRepositories may be filled with code, but they may also be filled with pages of text written in markdown and this markdown can be rendered by static site generators such as Jekyll or Hugo into HTML that is understood by the browser. This very website you’re reading from right now is being hosted on GitHub!\nThis guide will help you to understand how to practically navigate and contribute to projects like this one hosted via GitHub, and will demystify some of the basic actions and jargon around using GitHub along the way.\n\n\nBefore we begin, it can be helpful to first grasp what Git, GitHub and GitHub Pages each are and how they relate to each other:\n\nGit is a distributed open source version control system. Version control allows you to track changes in a set of files. It does this by taking snapshots of repositories at each stage of development - in Git these snapshots are known as ‘commits’. In software development this means things can be tested and rolled back, and enables sharing of stages of development. Git can be used on its own but when used with GitHub has more potential for collaborative projects. You can download and install Git on your own machine. It runs in the command line and there are desktop GUIs available. Git for Humans (Alice Bartlett talk at UX Brighton 2016) is a nicely accessible introduction to the purpose and uses of Git.\nGitHub is a web-based software development platform. It has many of Git’s features and can host Git repositories but also provides a web interface and additional functionalities. You can view millions of repositories hosted there. Many are openly licensed, meaning you can freely copy (or fork) them, either to adapt for yourself or to contribute back to the original project. By default the files and folders in a repository you create are public, and with a little bit of extra code, a repository can also be turned into a more public-facing project website, blog or wiki using a feature called GitHub Pages.\nGitHub Pages is a feature within GitHub to turn any repository or project into a website. GitHub can also be used to host a site that has been created using other software packages. One example of such a software is RStudio, a visual editor for Quarto Markdown which is what we use for this DS Topic Guides project!\n\n\n\n\nA barrier to GitHub for beginners is that it has its own terminology for common tasks and actions. While off-putting at first, they do serve to outline core concepts of both Git and GitHub. When using Git in particular, these are important to understand as they relate to git commands such as fork, pull, commit, clone, or branch. There are many quick guides and cheat sheets to this terminology on the web which can be useful to use as a reference while familiarising yourself:\n\nGitHub Docs Glossary\nGit for Librarians Glossary\n\nIn the next section, we will demonstrate the actions behind these keywords as we walk through three types of ways librarians regularly interact with GitHub based projects:\n\nWriting an Issue\nContributing code/content to an existing project\nRe-using existing code/content for a new project",
    "crumbs": [
      "Topic Guides",
      "GitHub: How to navigate and contribute to Git-based projects"
    ]
  },
  {
    "objectID": "github.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "github.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "GitHub: How to navigate and contribute to Git-based projects",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nBoth Git and GitHub have many potential applications for librarians and libraries. Library systems, digital collections and digital preservation are key areas where many resources and scripts are community-led and open source, made available via GitHub and other code repositories. For those working with open source platforms such as Omeka, a basic understanding of GitHub provides access to a host of plugins developed by user communities that can be cloned and applied to your local instance where you may not have resources to develop them yourself. In less technical fields of library work there also is the opportunity to use it to develop or share documentation of working groups and special projects. In all of these, there is the facility to raise an issue or fork a repository and make a pull request if you are looking to query or contribute to a project.\nHere are some examples of library activity on GitHub from Week one reading for “Git and GitHub for Librarians” course and Library Carpentry Library Carpentry: Introduction to Git: Summary and Setup, with some personal additions of our own:\n\nSharing documentation and code for library-related projects and platforms:\n\nLiving with Machines\nBritish Library Repositories\nOmeka exhibit platform and related plugins\nDigital projects of Library of Congress\nCollection Builder\n350+ repositories tagged with #code4lib\n\nDistributing OCR’d text extracted from digitised materials for collections-as-data analysis [example] [example] [example]\nStoring scripts for executing metadata ingests and transformations [example, example, [example]\nArchiving the source texts of open educational resources [example]\nWebsites for library workshops, to share course material or sample datasets [Library Carpentry example, NCSU Digital Scholarship Workshops example, DCU Library example],\nCollaborating on and contributing to a project Digital Scholarship & Data Science Essentials for Library Professionals\n\nLet’s look more closely at some practical ways you can get started navigating GitHub and contributing and making use of GitHub based projects for your library work!\n\n1. Writing Issues\nIssues are a great tool in GitHub to let the people know that something is possibly broken or something new is needed in a project. They help everyone by providing a place for everyone to discuss the concerns raised by the issue and providing a way to manage a response. If you have ever raised a ticket with the helpdesk of your IT department, you will notice a lot of similarities with Issues.\n\n\nPreparing to raise an issue\nIssues are a human centred tool in GitHub, so it is up to everybody involved to get the best out of them. Before raising an issue, consider these steps:\n\nRead the documentation carefully to make sure what you are experiencing is not down to any misunderstandings about the functionality or content provided.\nHave a look through the previous issues to make sure that what you are experiencing has not been raised before. If it has then consider adding details to that ticket and clicking on “Subscribe” under Notifications to be kept up to date with any fixes or changes.\nYou can raise issues for a variety of reasons, not just faults. You can also ask for new functionality or make a suggestion. Be clear, if you can, on the type of request you are making as the project maintainers may have different processes for different request types.\nRemember that what is obvious to you might not be obvious to a project maintainer, so explain fully and be patient with those trying to address the issue.\nSome repositories apply labels to categorize the issue (e.g. bug, feature request, documentation, backend, frontend etc.). If you are enable to label the issue you created please consider the available categories.\n\n\n\nReporting Bugs\nRaising an issue is a great opportunity to help to get something fixed. Providing good quality and complete information can cut down the time it takes to resolve an issue. A good write up should contain these elements:\n\nA full description of what happened, including:\n\nAny error messages or codes\nWhere you were in the project (for examples, for a web application the URL of the page you were on, or if it is not available, the stream of actions to get there)\nWhat you were trying to do\n\nWhat did you expect to happen? Including:\n\nWhat normally happens\nWhat should have happened\nIf the behaviour is new\n\nAny other relevant information: the project maintainer will be looking into possible causes of the issue you are experiencing, so extra information can help enormously. For example of a web application this might include:\n\nThe time of day the error occurred\nWhich browser you use\nIf you have experienced any similar errors with other sites\nWhich operating system you use\nProvide screenshot (if applicable)\nProvide version number of the software (if applicable)\n\n\nSome projects provide issue templates that ask specific questions relevant for the project (for example see the bug report template for Dataverse).\n\n\nRequesting new features\nIssues are not only for reporting bugs. They can be used for requesting new features or further support. In this case explain why it would make sense to add your new feature to the project. Think about who might be affected by your change, both in a positive and negative way. Lastly, consider having a go at the change yourself and submitting a pull request. If this is not possible explain why here.\n\n\nThe life of an issue\nOnce you submit an issue you will see that it will have the status of “Open”. Usually, the issue will then be assigned to a specific member of the project team to examine further. During this process you may see comments added discussing the issue and possible implementations, fixes or feedback. The issue may even have some labels or categories applied to it to help classify the issue type. You may be asked for more information or input to help resolve the issue.\nWhen work on the issue stops, it will be “closed”. In GitHub there are two types of issue close:\n\nClose as completed: This means the issue has been worked on and that work is complete. E.g. this might mean a bug has been fixed or a feature added.\nClose as not planned: This means that the project maintainer has chosen not to work on the issue. This can be for a variety of reasons such as:\n\nWon’t fix: Adding a fix for this feature is outside the current scope of the project, beyond its resources or deemed unnecessary.\nCan’t reproduce: In software, developers must usually reproduce the error in order to fix it. Sometimes this is not possible to do, this might happen for intermittent errors or errors caused by particular hardware configurations.\nDuplicate: The issue has been reported elsewhere. There will usually be a reference to where this is.\nStale: Sometimes issues stay in the system for a long time. This might be because the initial reporter has moved on and is unable to answer queries about it. In this case the issue will be removed.\n\n\n\nWith an issue, there is no expectation that you will provide a solution to the problem you are facing or the new functionality that you would like. At some point you might contribute a fix or something new to a project, maybe as a result of an Issue. When this happens you will raise a “pull request”. This is a proposal to make a group of changes to files in a git repository. A maintainer can then decide whether to accept the pull request and “merge” it with the project, reject it, or send it back for further work.\n\n\n\n2. Contributing content/code to an existing project\nAs you now know, GitHub hosts many open-licensed projects and by clicking the fork button, any GitHub user can instantaneously create their own fully independent copy of that project to work on without affecting the original. This copy or forked project can then be used to work out new features in a piece of software (or in our case, writing new content for a project website) that can eventually be merged back into the original project. Let’s walk through a simplified example using the example of contributing substantial edits to one of the pages of DS Topic Guides. Let’s say you’d like to write up a new use case on one of the DS Topic Guides. That process may look a little like this:\n\n1. Find the GitHub repository/source code behind the website\nOn the web version of DS Topic Guides, look for the GitHub logo in order to be taken to the GitHub repository behind it.\n\n\n\n2. Make your own copy of the project (repository) to work on\nOnce on the project GitHub page you can fork your own copy of the repository so that you have a full copy of the project in your own GitHub account. You will now be owner of this new version and can make edits and changes as you please to the files without it affecting the main project. Just remember that eventually if you want your contributions to be merged with the original project those changes will need to be reviewed and approved by that maintainer.\n\n\n\n3. Find the file you would like to change and make your edits\nNavigate to the file you would like to edit in your own copy of the DSEssentials project and click the pencil icon to edit. (Note that one way you’ll know you’re working in your own copy is by seeing your personal GitHub username ahead of the repository name). Let’s say the Topic Guide on AI & ML in Libraries Literacies needs an update. To find the right file in the directory you can do a search or look for a markdown file in the list which corresponds to the URL on the website version and has the file extension .md. Note, because this website was built using Quarto, the file extension in this example is .qmd. For instance if ml-ai.html is the web version, you would look for ml-ai.qmd in the file directory on GitHub.\n\nClick on the little pencil icon to edit this file in markdown. Markdown is a simple system for marking sections of text that should be stylised in a certain way, e.g. made bold or appear in a list of bullet points. Markdown files have the file extension “.md” and can be edited in GitHub or with a text editor like Notepad. One common file in GitHub repositories that you will find in the markdown format is the “README.md” file which usually covers the purpose of the repository and how to get up and running.\nMarkdown works by using certain text characters to indicate styles. For example, placing a “#” in front of a line will make it a first level header. Placing “##” before the line makes it a second level header. Putting an asterisk around a piece of text, like this “*my text”, will cause that text to be italicised as my text. You can see more markdown commands and experiment with it on the site Markdown Live Preview.\n\n\n4. Commit your changes\nWhenever you are working in a repository that you have forked, you are the Owner of this copy of the original project and can commit (save) any changes and edits you make to files directly to this copy of the project without needing any approval. Committing builds up a history of changes that you can roll back as needed along the way. Each time you commit you have an option of writing a comment as to what has changed which is a useful tool to use and habit to get into for going back in time if you need to reverse something. You can even refer to the related issue with #&lt;issue number&gt; in the commit message, and Github will cross reference the issue and the commit on the web interface.\n\n\n\n5. Contribute your changes back to the original/main project\nWhen you are finished with your changes and would like to share your new updated copy with the original project (in this example, incorporating your new text on Generative AI into the main DS Topic Guide on AI & ML in Libraries) this is when you will start a pull request. Navigate to the Pull Requests section of your forked project (repository) and click on “New pull request” to start that process.\n\nThis will take you to back to the original project you forked from, where you can then formally “create a pull request”. Once this has been made your pull request notifies the maintainers of the original project that you have made changes to their project that they may like to consider merging into theirs. Your pull request will show up under their project where you can discuss and review the changes you’re suggesting. Once the review has been completed and everyone is happy with the changes suggested, the maintainer of the project will merge your pull request and you will have officially contributed (and your profile’s icon will be listed among the contributors)!\n\n\n\n\n3. Re-using existing code/content for a new project\nLet’s say that instead of just contributing code/content to an existing project you want to make use of the whole project, such as a piece of open source software shared there. This is quite common for instance when looking to reuse software that someone has created and shared on GitHub. And it’s a similar process to the above although in this case you might want to clone the repository rather than fork it as cloning allows you to save the whole repository directly to your local machine, rather than within the GitHub platform itself in order to implement it. Developers will find this much more practical than working in GitHub as they go through the process of installing, editing, adapting the code as needed to implement it. See for instance the example mentioned earlier of a variety of open source Omeka plugins being available for install from the Omeka GitHub repository.",
    "crumbs": [
      "Topic Guides",
      "GitHub: How to navigate and contribute to Git-based projects"
    ]
  },
  {
    "objectID": "github.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "github.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "GitHub: How to navigate and contribute to Git-based projects",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nLike many technologies, the best way to learn git or GitHub is to use it. Luckily there are lots of tutorials out there with ready-made exercises and walkthroughs if you don’t have a clear purpose or aim to start with. Depending on your learning style, there are written lesson plans like the Carpentries’ ones, or plenty of walkthrough videos covering everything from basic terminology to full workflow development. Many of these resources begin with a grounding in git, the locally-installed version control software. While not essential to using GitHub, this can be useful to gain a broader understanding of the workings of both git and GitHub and maybe make sense of some of their additional functionalities. There are also specific lessons on various topics such as developing a static website using just GitHub and GitHub pages.\nOur list below is not intended to be comprehensive but is a selection of what is out there and all have been used by library staff like ourselves to get to grips with Git!:\n\nLearn the basics\n\nThis official GitHub tutorial teaches you GitHub essentials like repositories, branches, commits, and pull requests.\nAnother great tutorial over on GitHub uses the Spoon-Knife project, a test repository that’s hosted on GitHub.com that lets you test the fork and pull request workflow.\nGit-for-librarians exercise on branching and one on forking\n\n\n\nMaking a website using GitHub\n\nMaking a website with GitHub & GitHub Pages (lesson plan with video)\nMaking a website with GitHub & Quarto / RStudio (video)\nProgramming Historian - Building a static website with Jekyll and GitHub Pages\nProgramming Historian - Running a collaborative research website and blog with GitHub Pages\n\n\n\nWriting in Markdown (tools to try)\n\nMarkdown Live Preview is a tiny web tool to preview Markdown formatted text.\nWord to MD is a useful tool for uploading word files and transforming it into markdown\n\nAs with most platforms, specific functions or tasks may change occasionally on GitHub. So you may find a lesson which refers to an old term or function that is no longer called what it used to be. GitHub documentation should provide up-to-date information on the current practice in most cases.",
    "crumbs": [
      "Topic Guides",
      "GitHub: How to navigate and contribute to Git-based projects"
    ]
  },
  {
    "objectID": "github.html#recommended-readingviewing",
    "href": "github.html#recommended-readingviewing",
    "title": "GitHub: How to navigate and contribute to Git-based projects",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\n\nLibrary Carpentry Git & Version Control / Software Carpentry\nPush, Pull, Fork: GitHub for Academics (hybridpedagogy.org)\nA Comparative Analysis of the Use of GitHub by Librarians and Non-Librarians\nA reading list for librarians learning about Git and GitHub\nWeek one reading for “Git and GitHub for Librarians” course.",
    "crumbs": [
      "Topic Guides",
      "GitHub: How to navigate and contribute to Git-based projects"
    ]
  },
  {
    "objectID": "github.html#finding-communities-of-practice",
    "href": "github.html#finding-communities-of-practice",
    "title": "GitHub: How to navigate and contribute to Git-based projects",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nThe best way to understand GitHub is to have a play yourself and if you get into trouble we’ve found that asking questions over on the GitHub Community Hub is a great way to get connected with other users. You can also join networks like AI4LAM and ask questions of members of the very engaged and helpful community on their slack channel.",
    "crumbs": [
      "Topic Guides",
      "GitHub: How to navigate and contribute to Git-based projects"
    ]
  },
  {
    "objectID": "crowdsourcing.html",
    "href": "crowdsourcing.html",
    "title": "Crowdsourcing and Citizen Science in Cultural Heritage",
    "section": "",
    "text": "Crowdsourcing in cultural heritage is a method for enabling meaningful public participation in research or practical tasks based on cultural heritage collections or knowledge (adapted from the Collective Wisdom Handbook. In practical terms, a typical example involves asking people outside the institution to contribute effort via tasks such as transcribing, tagging, researching or sharing artefacts. Crowdsourcing projects tend to focus on creating enjoyable, inherently rewarding experiences that lead to high quality data with minimal risk of errors. This means that crowdsourcing platforms can also be a great way for staff to work with their own collections.\nFor example, you could ask locals to share their memories, stories or artefacts about your region, or you could work with the public to co-create an exhibition or collaborate with international experts on research tasks. Some volunteers want follow their interests in specific topics, while others enjoy serendipitious glimpses into varied collections.\nSo perhaps more importantly than the data collected or enhanced, crowdsourcing in cultural heritage is a way of inviting people to spend time with collections and institutions that they might never encounter otherwise. For example, the Living with Machines project engaged over 5000 online volunteers on the citizen science platform, many of whom had no previous experience with library or humanities research, and no previous relationship with the British Library.\nWhile crowdsourcing in other fields might involve payment for tasks completed (for example, via Amazon’s Mechanical Turk), the rewards for contributing to crowdsourcing in cultural heritage are usually intrinsic or altruistic. Thinking of it as a form of online volunteering can be helpful - participants can help enhance collections metadata, research objects or stories, contribute their own knowledge and experience, etc, with opportunities to learn or socialise during the process. But unlike traditional volunteering, crowdsourcing isn’t limited to a venue’s location or hours of operation - online projects can be open to anyone, anywhere in the world, 24 hours a day.\nVolunteers are often motivated by their interest in a topic or source type (e.g. beautiful maps or interesting photographs), the challenge of completing a task well (e.g. deciphering old handwriting), developing their skills and knowledge (e.g. becoming more accurate as they practise palaeography, or learning more about a collection or research question). They often stay motivated because they get feedback from project teams, and gain a sense of purpose or community.\nIncreasingly, libraries might combine crowdsourcing with machine learning and AI tools. For example, by building workflows that automate work such as pre-selecting relevant records, distributing tasks to volunteers with matching skills and interests, then supporting staff in quality checking and formatting the results. Automating tedious (and easily verifiable) tasks with machine learning can help create enjoyable volunteer or staff experiences.\nIt can also be used to ‘scale up’ the results of volunteer work. For example, the ‘Ad or not’ task we created for Living with Machines relies on people’s ability to understand the purpose of short pieces of text - was it an advertisement or part of an article? The results of this task became the ‘ground truth’ dataset for training an experimental machine learning model that could assess whether other texts were ads or not.\n\n\n\nScreenshot of the Zooniverse task interface, with an historical newspaper image on the left, and options to label it ‘yes’ or ‘no’ in response to a question asking if it is an advertisement\n\n\nScreenshot of an early version of the ‘ad or not’ task on Zooniverse. This simple ‘yes or no’ format also meant that the task was available in the Zooniverse app, further increasing its reach.\n\n\n‘Crowdsourcing’ is an awkward name, with implications of ‘outsourcing’ and anonymous crowds, but to date it is the most widely recognised term. Other terms used for similar work include digital public participation, community-generated digital content (CGDC), online volunteering, and variations such as ‘niche-sourcing’ for small or invitation-only projects.\n‘Citizen science’ is another commonly used term with a lot of overlap with crowdsourcing. Citizen science projects might include natural history observations in the world (for example, recording wildlife, or monitoring water quality) or screen-based tasks such as counting penguins in photographs. With roots in a broader concept of ‘science’ (Wissenschaft) as knowledge or areas of study, ‘citizen science’ also includes citizen history, humanities (Geisteswissenschaften), social sciences and any other field that works with knowledge about the world. As inherently interdisciplinary spaces that welcome specialists and the public, libraries are excellent places to host crowdsourcing and citizen science projects.\nWhatever it’s called, recognising the work that volunteers put into projects is important. Some projects do this by hosting events for volunteers, others ensure that they are named and credited in publications and datasets.\n\n\n\nCrowdsourcing isn’t for everyone nor the answer for every need. For example, running a successful project draws on a range of skills, and may require collaboration across many departments in an institution. The development of machine learning / AI methods and increasingly sophisticated crowdsourcing platforms can reduce the amount of work required to gather source material, review and quality control contributions and process the resulting data, but you will still need the resources and inclination for reviewing and sharing progress reports, social interactions with volunteers, and responding to questions. It may take a few iterations to create tasks that produce useful data via tasks that will attract volunteers. If you don’t enjoy talking to volunteers, negotiating with colleagues and wrangling resources (or don’t have any resources to spare), it might not be right for you right now.\nIdeally, you would also be able to ensure that the source collections, desired data results and types of tasks available on your platform of choice are a good match by prototyping or piloting workflows before committing to a full project. That said, you don’t have to limit your project to the types of tasks you’ve seen before - you can invent new tasks and workflows, and work with new technologies to meet your needs. For example, the Living with Machines project invented a ‘close reading’ task that asked volunteers to discern the sense in which specific words were used, supported by computational linguistic analysis.",
    "crumbs": [
      "Topic Guides",
      "Crowdsourcing and Citizen Science in Cultural Heritage"
    ]
  },
  {
    "objectID": "crowdsourcing.html#introduction",
    "href": "crowdsourcing.html#introduction",
    "title": "Crowdsourcing and Citizen Science in Cultural Heritage",
    "section": "",
    "text": "Crowdsourcing in cultural heritage is a method for enabling meaningful public participation in research or practical tasks based on cultural heritage collections or knowledge (adapted from the Collective Wisdom Handbook. In practical terms, a typical example involves asking people outside the institution to contribute effort via tasks such as transcribing, tagging, researching or sharing artefacts. Crowdsourcing projects tend to focus on creating enjoyable, inherently rewarding experiences that lead to high quality data with minimal risk of errors. This means that crowdsourcing platforms can also be a great way for staff to work with their own collections.\nFor example, you could ask locals to share their memories, stories or artefacts about your region, or you could work with the public to co-create an exhibition or collaborate with international experts on research tasks. Some volunteers want follow their interests in specific topics, while others enjoy serendipitious glimpses into varied collections.\nSo perhaps more importantly than the data collected or enhanced, crowdsourcing in cultural heritage is a way of inviting people to spend time with collections and institutions that they might never encounter otherwise. For example, the Living with Machines project engaged over 5000 online volunteers on the citizen science platform, many of whom had no previous experience with library or humanities research, and no previous relationship with the British Library.\nWhile crowdsourcing in other fields might involve payment for tasks completed (for example, via Amazon’s Mechanical Turk), the rewards for contributing to crowdsourcing in cultural heritage are usually intrinsic or altruistic. Thinking of it as a form of online volunteering can be helpful - participants can help enhance collections metadata, research objects or stories, contribute their own knowledge and experience, etc, with opportunities to learn or socialise during the process. But unlike traditional volunteering, crowdsourcing isn’t limited to a venue’s location or hours of operation - online projects can be open to anyone, anywhere in the world, 24 hours a day.\nVolunteers are often motivated by their interest in a topic or source type (e.g. beautiful maps or interesting photographs), the challenge of completing a task well (e.g. deciphering old handwriting), developing their skills and knowledge (e.g. becoming more accurate as they practise palaeography, or learning more about a collection or research question). They often stay motivated because they get feedback from project teams, and gain a sense of purpose or community.\nIncreasingly, libraries might combine crowdsourcing with machine learning and AI tools. For example, by building workflows that automate work such as pre-selecting relevant records, distributing tasks to volunteers with matching skills and interests, then supporting staff in quality checking and formatting the results. Automating tedious (and easily verifiable) tasks with machine learning can help create enjoyable volunteer or staff experiences.\nIt can also be used to ‘scale up’ the results of volunteer work. For example, the ‘Ad or not’ task we created for Living with Machines relies on people’s ability to understand the purpose of short pieces of text - was it an advertisement or part of an article? The results of this task became the ‘ground truth’ dataset for training an experimental machine learning model that could assess whether other texts were ads or not.\n\n\n\nScreenshot of the Zooniverse task interface, with an historical newspaper image on the left, and options to label it ‘yes’ or ‘no’ in response to a question asking if it is an advertisement\n\n\nScreenshot of an early version of the ‘ad or not’ task on Zooniverse. This simple ‘yes or no’ format also meant that the task was available in the Zooniverse app, further increasing its reach.\n\n\n‘Crowdsourcing’ is an awkward name, with implications of ‘outsourcing’ and anonymous crowds, but to date it is the most widely recognised term. Other terms used for similar work include digital public participation, community-generated digital content (CGDC), online volunteering, and variations such as ‘niche-sourcing’ for small or invitation-only projects.\n‘Citizen science’ is another commonly used term with a lot of overlap with crowdsourcing. Citizen science projects might include natural history observations in the world (for example, recording wildlife, or monitoring water quality) or screen-based tasks such as counting penguins in photographs. With roots in a broader concept of ‘science’ (Wissenschaft) as knowledge or areas of study, ‘citizen science’ also includes citizen history, humanities (Geisteswissenschaften), social sciences and any other field that works with knowledge about the world. As inherently interdisciplinary spaces that welcome specialists and the public, libraries are excellent places to host crowdsourcing and citizen science projects.\nWhatever it’s called, recognising the work that volunteers put into projects is important. Some projects do this by hosting events for volunteers, others ensure that they are named and credited in publications and datasets.\n\n\n\nCrowdsourcing isn’t for everyone nor the answer for every need. For example, running a successful project draws on a range of skills, and may require collaboration across many departments in an institution. The development of machine learning / AI methods and increasingly sophisticated crowdsourcing platforms can reduce the amount of work required to gather source material, review and quality control contributions and process the resulting data, but you will still need the resources and inclination for reviewing and sharing progress reports, social interactions with volunteers, and responding to questions. It may take a few iterations to create tasks that produce useful data via tasks that will attract volunteers. If you don’t enjoy talking to volunteers, negotiating with colleagues and wrangling resources (or don’t have any resources to spare), it might not be right for you right now.\nIdeally, you would also be able to ensure that the source collections, desired data results and types of tasks available on your platform of choice are a good match by prototyping or piloting workflows before committing to a full project. That said, you don’t have to limit your project to the types of tasks you’ve seen before - you can invent new tasks and workflows, and work with new technologies to meet your needs. For example, the Living with Machines project invented a ‘close reading’ task that asked volunteers to discern the sense in which specific words were used, supported by computational linguistic analysis.",
    "crumbs": [
      "Topic Guides",
      "Crowdsourcing and Citizen Science in Cultural Heritage"
    ]
  },
  {
    "objectID": "crowdsourcing.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "crowdsourcing.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Crowdsourcing and Citizen Science in Cultural Heritage",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nCultural heritage institutions can support citizen science projects that ask participants to make observations about the natural world. For example, community science projects at the UK’s Natural History Museum ask people to help investigate noise pollution and local pondlife.\nLibraries, archives and museums can ask volunteers to help create or enhance collection data by transcribing handwritten text, entering data from catalogue or specimen cards into databases, or adding tags or labels to describe images. For example, Smithsonian Digital Volunteers: Transcription Center.\nThey might also ask volunteers to research objects or record information from their personal knowledge. For example, photos on Flickr Commons have been tagged with locations, personal names and histories, and specialist object labels identified by people with local or historical knowledge about photographs.\nLibrary and other GLAM staff can initiate projects, help manage and run them, and check, process and ingest data from crowdsourcing projects. For example, the Library of Congress reviewed comments left on their Flickr Commons images, and updated some of their collections records with information provided by the public.\nLibraries can support projects run on national portals, such as Latvia’s iesaisties.lv. They can point language learners or people with local knowledge to projects in a range of languages and other national portals.\nThere’s a strong relationship between IIIF and crowdsourcing. In 2017 the British Library used IIIF images for the ‘In the Spotlight’ project, in part to save hosting costs, and in part to explore the potential for IIIF annotations in crowdsourcing. This later inspired a collaboration with Zooniverse to support importing media into Zooniverse via IIIF manifests. Crowdsourcing platforms that support IIIF include the Digirati / National Library of Wales/Madoc platform and From the Page.",
    "crumbs": [
      "Topic Guides",
      "Crowdsourcing and Citizen Science in Cultural Heritage"
    ]
  },
  {
    "objectID": "crowdsourcing.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "crowdsourcing.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Crowdsourcing and Citizen Science in Cultural Heritage",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nThe best way to learn more about crowdsourcing is to try a range of different projects. This will help you understand participant motivations, get a sense of the importance of great titles and instrutions in getting you started, and think about how data might move between GLAM systems and crowdsourcing platforms.\nYou can find projects to try at:\n\nhttps://www.zooniverse.org/projects\nhttps://fromthepage.com/findaproject\nhttps://scistarter.org/\nhttps://transcription.si.edu/\nhttp://lesherbonautes.mnhn.fr/\n\nIf your organisation has records in Europeana, you might be able to devise crowdsourcing tasks for them on the CrowdHeritage site.",
    "crumbs": [
      "Topic Guides",
      "Crowdsourcing and Citizen Science in Cultural Heritage"
    ]
  },
  {
    "objectID": "crowdsourcing.html#recommended-readingviewing",
    "href": "crowdsourcing.html#recommended-readingviewing",
    "title": "Crowdsourcing and Citizen Science in Cultural Heritage",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nIf you are interested in learning more about participating in, creating or using data from crowdsourcing projects, a recent comprehensive publication is the open access publication, The Collective Wisdom Handbook: Perspectives on Crowdsourcing in Cultural Heritage.\nThe Collective Wisdom Handbook is designed to answer the most common questions that people have as they think about starting, maintaining and using data from a crowdsourcing or citizen science project. Topics covered include:\n\nWhat is crowdsourcing in cultural heritage?\nWhy work with crowdsourcing in cultural heritage?\nIdentifying, aligning, and enacting values in your project\nDesigning cultural heritage crowdsourcing projects\nUnderstanding and connecting to participant motivations\nAligning tasks, platforms, and goals\nChoosing tasks and workflows\nSupporting participants\nWorking with crowdsourced data\nManaging cultural heritage crowdsourcing projects\nConnecting with communities\nPlanning crowdsourcing events\nEvaluating your crowdsourcing project",
    "crumbs": [
      "Topic Guides",
      "Crowdsourcing and Citizen Science in Cultural Heritage"
    ]
  },
  {
    "objectID": "crowdsourcing.html#finding-communities-of-practice",
    "href": "crowdsourcing.html#finding-communities-of-practice",
    "title": "Crowdsourcing and Citizen Science in Cultural Heritage",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nWhen you’re ready to think about creating crowdsourcing projects or working with crowdsourcing data it’s useful to reach out to existing communities of practice for support and feedback.\nThe LIBER Citizen Science Working Group is a community of practice open to all LIBER members. They are currently producing a guide for Citizen science in Research Libraries. Topics published to date include:\n\nSkills: Citizen Science skills development for staff, researchers, and the public\nInfrastructures: As being active in the development of infrastructure for researchers to carry out Citizen Science\n\nThe low-traffic JISCMail discussion list on crowdsourcing has many interested members and is a great way to connect with others who have an interest and experience in crowdsourcing in cultural heritage.",
    "crumbs": [
      "Topic Guides",
      "Crowdsourcing and Citizen Science in Cultural Heritage"
    ]
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contribute to our Project",
    "section": "",
    "text": "The first edition of Digital Scholarship & Data Science Topic Guides for Library Professionals, launched at the LIBER Annual Conference in Summer 2025, draws heavily on the existing skills framework and topics covered in the British Library’s Digital Scholarship Training Programme, as well as those recommended by attendees of development workshops hosted by LIBER working groups throughout 2023/2024.\nThere are several ways to contribute to the ongoing update and future development of this resource.\nFeel free to contact Nora McGregor if you’re keen to discuss!"
  },
  {
    "objectID": "contributing.html#help-us-with-our-wishlist",
    "href": "contributing.html#help-us-with-our-wishlist",
    "title": "Contribute to our Project",
    "section": "Help us with our wishlist!",
    "text": "Help us with our wishlist!\nContribute a new Topic Guide (individually or in collaboration with others). Choose from our current wishlist or propose a new one. Contact digitalresearch@bl.uk with your idea or submit an Issue and we’ll help you get started right away!\n\nNOTE: Our current wishlist for future editions includes but is not limited to topics such as:\n\nClimate Change and Sustainability\nCultural Competency & Ethics in Digital Methods\nDemystifying Computational Environments for Digital Scholarship\nDigital Scholarship/Data Science Project Management, Evaluation and Impact Assessment\nDigital Storytelling\nFoundation models: When and why to use LLMs or SLMs\nResearch Data Management\nWikimedia"
  },
  {
    "objectID": "contributing.html#suggest-edits-to-an-existing-topic-guide",
    "href": "contributing.html#suggest-edits-to-an-existing-topic-guide",
    "title": "Contribute to our Project",
    "section": "Suggest edits to an existing Topic Guide",
    "text": "Suggest edits to an existing Topic Guide\nKnow of a good case study, report or community of practice that LIBER members should know about? Open a new Issue or add to the discussion on existing Issues on the project Github. If you’re new to GitHub don’t worry, we have a Topic Guide for that: GitHub: How to navigate and contribute to Git-based projects!"
  },
  {
    "objectID": "contributing.html#join-a-working-group-and-help-with-ongoing-maintenance-and-review",
    "href": "contributing.html#join-a-working-group-and-help-with-ongoing-maintenance-and-review",
    "title": "Contribute to our Project",
    "section": "Join a working group and help with ongoing maintenance and review",
    "text": "Join a working group and help with ongoing maintenance and review\nWe view this resource as capturing a snapshot in time and will endeavour to fully review all content a minimum of once annually to check for link rot, and content relevancy. The project team editors are committed to making quick fixes when raised throughout the year, while formal sprints and reviews coinciding with LIBER Annual Summer Conference and Winter events will be used for soliciting new content and undertaking more complex updates to the resource each year and creating new editions as necessary. Join one of the LIBER Working Groups Digital Scholarship and Digital Cultural Heritage WG or Data Science in Libraries (open to all staff of LIBER member institutions) to keep up with the developments and help out with ongoing maintenance of this project!"
  },
  {
    "objectID": "contributing.html#take-part-in-a-writing-sprint",
    "href": "contributing.html#take-part-in-a-writing-sprint",
    "title": "Contribute to our Project",
    "section": "Take part in a Writing Sprint!",
    "text": "Take part in a Writing Sprint!\nWe held four online Topic Guide Sprints in 2024.\n\nTuesday 14 May 2024\nTuesday 03 June 2024\nTuesday 08 October 2024\nTuesday 05 November 2024\n\nTo be notified of future sprints check back here for dates/times or join one of our working groups and we’ll email out the details!\n\nPreparing for a sprint:\n\nPlease have a quick read through the Welcome and Topic Guide pages to familiarise yourself with this resource and its purpose.\nWhen you Register you’ll see a list of our latest Topic Guides on our wishlist. Please check off any and all Topic Guides you would be happy to work on during the day of the sprint. On the day each attendee will focus on writing just one Topic Guide however. The information you provide before the sprint will help us organise this ahead of time so that as many Topic Guides as possible have at least one dedicated author.\n\nPlease have a quick read through the Author Guidance and Style Guide section.\nWe’ll be in touch after we receive your registration to send you more details and useful information ahead of the sprint so look out for our email! If you are sent a copy of the Topic Guide Template (Google Doc) that corresponds to your topic of choice ahead of time, add your name to the header, and feel free to add notes or even make a start on drafting your guide before the sprint day if you’re keen!\n\n\n\nWhat to expect on the day:\nA two hour writing sprint will generally follow this format:\n\nWelcome and participant introductions (10 minutes)\n\nOverview of the project & explanation of how the day will run (15 minutes)\n\nAllocation and confirmation of individual Topic Guide Authorship for the day (15 minutes): As a group we’ll go over the wishlist and all participants will be given access to their particular Topic Guide google doc template they’ll be using during and after the sprint.\nWriting Phase (60 minutes): You’ll have an hour to begin writing your respective guide in your Google Doc template. If more than one participant are working together on a topic we can provide breakout rooms so that you can discuss amongst yourselves how to break up the work, and share ideas as you go along. You can have your camera/mic on or off during this time and co-chairs will be on hand to answer any questions or give advice on topics.\nWrapping Up & Logging progress through Github Issues (15 minutes): We have provided an Issue Template which is pre-formatted as a Topic Guide submission checklist and will walk participants through the process of opening a new issue for your draft submission. Please fill in the issue template and provide the necessary information relating to where your draft submission is currently at. Once the issue is saved, it can continue to serve as a space to continue discussion of the ongoing status of your submission and log updates where necessary, including notifying us of when it is fully ready for final review by maintainers.\nClosing Remarks & Next Steps (5 minutes)\n\n\n\nAfter the sprint:\nDuring the sprint, you’ve drafted your Topic Guide in a Google Doc. This Google Doc can continue to serve as a ‘living document’ following the sprint until you feel the content is ready to be published. Throughout this process, we will use your Github issue to maintain an overview of the docs in terms of their status and action points so please keep this updated. Each Topic Guide will be allocated a specific Maintainer contact (one of the co-chairs) who will be in touch and work with you to see the process through to completion. You may request placing your Google Doc under restricted access if you wish. We will not do this by default, but we respect that some contributors would like to keep drafts private until fully ready for submission.\nWhen your Topic Guide is ready for our review in Google Docs, please let us know by logging an update over on Github as a comment on the Issue stating this. You can also tag the maintainers so they receive a notification as well. Maintainers will generally work with you on the final edits necessary for the submission through track changes and comments within the Google Doc itself. Once these have all been resolved maintainers will log a status update in the GitHub issue."
  },
  {
    "objectID": "training-platforms.html",
    "href": "training-platforms.html",
    "title": "TRAINING PLATFORMS",
    "section": "",
    "text": "TRAINING PLATFORMS\nWhen you’re ready to go further and have a better idea of the specific skills you need for a particular task, we can recommend having a good search through these excellent platforms which host or link to a great many in-depth training materials:\n\nDARIAH-Campus\nDARIAH is a pan-European infrastructure for arts and humanities scholars working with computational methods. It supports digital research as well as the teaching of digital research methods. Though not specific to the library professional context, tutorials here are useful for applying techniques to digital collections. https://campus.dariah.eu/\n\n\nThe Glam Workbench\nThe GLAM Workbench is the brainchild of Tim Sherratt, a historian, and is a collection of Jupyter notebooks to help you explore and use data from GLAM institutions (galleries, libraries, archives, and museums). It includes tools, tutorials, examples, hacks, and even some pre-harvested datasets. It’s aimed at researchers in the humanities but has useful tutorials for anyone interested in working with GLAM data. https://glam-workbench.net/\n\n\nIneo\nIneo is a project developed and maintained by CLARIAH that lets you search, browse, find and select digital resources for research in humanities and social sciences. At the end of 2024 it will offer access to thousands of tools, datasets, workflows, standards and learning material. It is a work in progress so do keep that in mind when browsing. https://www.ineo.tools/\n\n\nLibrary Carpentry\nLibrary Carpentry is an international volunteer community, under the Carpentries, focussed building software and data skills within library and information-related communities. The lessons here are meant to be taught as workshops led by a Carpentries certified instructor (for a fee) but you may find it useful to have a read through the content which is open and available to all. https://librarycarpentry.org/\n\n\nThe Programming Historian\nThe Programming Historian has been publishing peer-reviewed tutorials on digital tools and techniques for humanists since 2008 and though they’re generally aimed at academic researchers, staff at British Library have found them highly useful over the years in their own work! https://programminghistorian.org/en/\n\n\nSocial Sciences & Humanities Open Marketplace\nBuilt as part of the Social Sciences and Humanities Open Cloud project (SSHOC), the Social Sciences and Humanities Open Marketplace is a discovery portal which pools and contextualises resources for Social Sciences and Humanities research communities: tools, services, training materials, datasets, publications and workflows. The Marketplace highlights and showcases solutions and research practices for every step of the SSH research data life cycle. https://marketplace.sshopencloud.eu/search?order=score&categories=training-material"
  },
  {
    "objectID": "iiif.html",
    "href": "iiif.html",
    "title": "IIIF",
    "section": "",
    "text": "IIIF (pronounced “triple-eye-eff”) stands for the International Image Interoperability Framework. Quite a tongue twister that one, but it broadly represents two things:\n\na set of open standards for delivering high-quality, attributed digital objects online at scale.\nthe open, international community of software developers, libraries, researchers, educators, museums, universities, creative agencies, and more working together to develop and implement the IIIF APIs to make the above open standards happen.\n\n\n\nEver try to look at a large high-resolution digitised manuscript online only for it to take ages to load, and when it finally does you have no way to actually move around the image easily nor see any of the metadata or annotations related to it?\nOr maybe spent months on end negotiating the terms and methods around sending copies of a variety of differently sized individual images to another institution for them to host as part of collaborative project?\nIIIF brings a whole new efficiency to the way in which we in the cultural heritage sector go about sharing and making our digitised collections available online, while greatly expanding the functionality around the way users interact with them. It’s an open standard, collaboratively developed and maintained by a host of cultural heritage institutions around the world that defines a consistent method for the delivery of images and audio/visual files from servers to different environments on the Web where they can then be viewed and interacted with in many ways.\nIIIF basically specifies a way for browsers to display a digital objects such as images, pdfs, audio/visual files and even 3D files in a way that enables much richer functionality on the Web:\n\nMakes it easier to display large images on the web in a way that is scalable (enabling deep zoom)\nAllows easy comparison between two objects, connecting and uniting materials across institutional boundaries\nDisplays structure and metadata and annotations with the digital collection item. (For a digitised manuscript for instance this might be page order and searchable text, for audio/visual materials, that means being able to deliver complex structures (such as several reels of film that make up a single movie) along with things like captions, transcriptions/translations, annotations, and more.)\n\n\n“At its simplest, IIIF uses APIs to load images quickly and zoom smoothly without additional loading time. But IIIF also allows you to do much more, including pulling IIIF-enabled images from different sites into viewers for comparison without downloading them (at full resolution), and enabling saving links to details of images or portions of A/V files for future reference. IIIF also allows you to use many open-source tools that help you to compare, annotate, transcribe, collaborate, and more. You can even gather multiple IIIF images together from across multiple archival collections and/or institutions to create projects or exhibits without advanced technical skills”. -From IIIF for Archives\n\nMaking your collections IIIF enabled makes it easier to share your content online in a consistent way, enabling portability across different IIIF enabled viewers. This means that rather than endlessly creating copies and different access versions of your images to send all over the world for different projects, they can stay on your same IIIF Image server, but be accessed and displayed by viewers hosted at institutions elsewhere in any fashion you require.\n\n\n\nThe IIIF community is made up of over 100 major cultural heritage organisations worldwide who have formally adopted it, including many of our very own LIBER members. It was started in 2011 as a collaboration between The British Library, Stanford University, the Bodleian Libraries (Oxford University), the Bibliothèque nationale de France, Nasjonalbiblioteket (National Library of Norway), Los Alamos National Laboratory Research Library, and Cornell University. It’s a really nice example of an open, grassroots but global community effort, backed by a consortium of leading cultural institutions, who have been working together to develop and implement this new capability for decades and solve their shared problems with delivering, managing, sharing, and working with their resources.\n\n\n\nIf you want the deep nitty gritty technical stuff around all the APIs and how they fit together there is quite a bit of implementation documentation on their website that goes into all this. But essentially, the basic set up behind making your own digitised collections “IIIF enabled”, as they say, looks a little something like this:\n\nSet-up a IIIF image server (you can choose one developed by the community, or there are IIIF-compatible image servers available from vendors or other web hosts), move your content there and implement the IIIF Image API to make those images and audio/video materials available from there.\nImplement the IIIF Presentation API which creates the all important IIIF Manifest files (also many open source or vendor products can help handle this bit too) for each of your objects. This Manifest file is really the prime unit in IIIF, it essentially combines and packages in json code, information about your images and structural data from your metadata source. It lists all the information that makes up your new IIIF enabled object, from how to display it to what information IIIF viewers should (and should not) display (such as structure or the order of pages, or even as minute as where an illustration is located within an image if you like). If you want to see an example of what one looks like this is a IIIF Manifest from the Bodleian Libraries at University of Oxford relating to this collection item. Each manifest has its own URL and that’s the bit you’ll use to do cool things with the object in different IIIF viewers, such as allowing a manuscript to be easily dragged and dropped into Mirador for instance for comparison with other IIIF enabled manuscripts.\nChoose one of the many IIIF enabled viewers for displaying your images and add it to your own collection site. Again, looking at that same collection item record above, note in the upper left hand-corner (see image below) there is an option to view in Mirador or Universal Viewer which are two different styles of IIIF viewer that afford different functionalities.\nConsider making your IIIF Manifests available publicly for download so users can work with them in all the interesting ways you’ve now enabled!",
    "crumbs": [
      "Topic Guides",
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#introduction",
    "href": "iiif.html#introduction",
    "title": "IIIF",
    "section": "",
    "text": "IIIF (pronounced “triple-eye-eff”) stands for the International Image Interoperability Framework. Quite a tongue twister that one, but it broadly represents two things:\n\na set of open standards for delivering high-quality, attributed digital objects online at scale.\nthe open, international community of software developers, libraries, researchers, educators, museums, universities, creative agencies, and more working together to develop and implement the IIIF APIs to make the above open standards happen.\n\n\n\nEver try to look at a large high-resolution digitised manuscript online only for it to take ages to load, and when it finally does you have no way to actually move around the image easily nor see any of the metadata or annotations related to it?\nOr maybe spent months on end negotiating the terms and methods around sending copies of a variety of differently sized individual images to another institution for them to host as part of collaborative project?\nIIIF brings a whole new efficiency to the way in which we in the cultural heritage sector go about sharing and making our digitised collections available online, while greatly expanding the functionality around the way users interact with them. It’s an open standard, collaboratively developed and maintained by a host of cultural heritage institutions around the world that defines a consistent method for the delivery of images and audio/visual files from servers to different environments on the Web where they can then be viewed and interacted with in many ways.\nIIIF basically specifies a way for browsers to display a digital objects such as images, pdfs, audio/visual files and even 3D files in a way that enables much richer functionality on the Web:\n\nMakes it easier to display large images on the web in a way that is scalable (enabling deep zoom)\nAllows easy comparison between two objects, connecting and uniting materials across institutional boundaries\nDisplays structure and metadata and annotations with the digital collection item. (For a digitised manuscript for instance this might be page order and searchable text, for audio/visual materials, that means being able to deliver complex structures (such as several reels of film that make up a single movie) along with things like captions, transcriptions/translations, annotations, and more.)\n\n\n“At its simplest, IIIF uses APIs to load images quickly and zoom smoothly without additional loading time. But IIIF also allows you to do much more, including pulling IIIF-enabled images from different sites into viewers for comparison without downloading them (at full resolution), and enabling saving links to details of images or portions of A/V files for future reference. IIIF also allows you to use many open-source tools that help you to compare, annotate, transcribe, collaborate, and more. You can even gather multiple IIIF images together from across multiple archival collections and/or institutions to create projects or exhibits without advanced technical skills”. -From IIIF for Archives\n\nMaking your collections IIIF enabled makes it easier to share your content online in a consistent way, enabling portability across different IIIF enabled viewers. This means that rather than endlessly creating copies and different access versions of your images to send all over the world for different projects, they can stay on your same IIIF Image server, but be accessed and displayed by viewers hosted at institutions elsewhere in any fashion you require.\n\n\n\nThe IIIF community is made up of over 100 major cultural heritage organisations worldwide who have formally adopted it, including many of our very own LIBER members. It was started in 2011 as a collaboration between The British Library, Stanford University, the Bodleian Libraries (Oxford University), the Bibliothèque nationale de France, Nasjonalbiblioteket (National Library of Norway), Los Alamos National Laboratory Research Library, and Cornell University. It’s a really nice example of an open, grassroots but global community effort, backed by a consortium of leading cultural institutions, who have been working together to develop and implement this new capability for decades and solve their shared problems with delivering, managing, sharing, and working with their resources.\n\n\n\nIf you want the deep nitty gritty technical stuff around all the APIs and how they fit together there is quite a bit of implementation documentation on their website that goes into all this. But essentially, the basic set up behind making your own digitised collections “IIIF enabled”, as they say, looks a little something like this:\n\nSet-up a IIIF image server (you can choose one developed by the community, or there are IIIF-compatible image servers available from vendors or other web hosts), move your content there and implement the IIIF Image API to make those images and audio/video materials available from there.\nImplement the IIIF Presentation API which creates the all important IIIF Manifest files (also many open source or vendor products can help handle this bit too) for each of your objects. This Manifest file is really the prime unit in IIIF, it essentially combines and packages in json code, information about your images and structural data from your metadata source. It lists all the information that makes up your new IIIF enabled object, from how to display it to what information IIIF viewers should (and should not) display (such as structure or the order of pages, or even as minute as where an illustration is located within an image if you like). If you want to see an example of what one looks like this is a IIIF Manifest from the Bodleian Libraries at University of Oxford relating to this collection item. Each manifest has its own URL and that’s the bit you’ll use to do cool things with the object in different IIIF viewers, such as allowing a manuscript to be easily dragged and dropped into Mirador for instance for comparison with other IIIF enabled manuscripts.\nChoose one of the many IIIF enabled viewers for displaying your images and add it to your own collection site. Again, looking at that same collection item record above, note in the upper left hand-corner (see image below) there is an option to view in Mirador or Universal Viewer which are two different styles of IIIF viewer that afford different functionalities.\nConsider making your IIIF Manifests available publicly for download so users can work with them in all the interesting ways you’ve now enabled!",
    "crumbs": [
      "Topic Guides",
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "iiif.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "IIIF",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nIIIF was initiated by a bunch of libraries and cultural heritage collections holders and it shows! It was proposed in late 2011 as a collaboration between The British Library, Stanford University, the Bodleian Libraries (Oxford University), the Bibliothèque nationale de France, Nasjonalbiblioteket (National Library of Norway), Los Alamos National Laboratory Research Library, and Cornell University. The intention of the consortium has always been to combine resource and effort in building viewers that reflected the way we wanted our digital collections to be displayed online, rather than everyone still spending time and resource making our own custom viewers and building our own content silos. It’s brought a huge amount of efficiency too in the way that we share images with each other and researchers. It’s changed the way collaborative projects are undertaken where endless metadata mapping exercises, contract negotiations around re-use and hosting, and the copying and shipping of digitised materials on external hard drives were the norm.\nThere are quite a number of use cases and case studies available on the IIIF Demos page but let’s have a quick look at three real life (and canonical) examples of IIIF in action.\n\nDeep Zoom and Annotations\nThe example here is of the Ōmi Kuni-ezu 近江國絵圖 Japanese Tax map created in 1837. It’s meant to be read in the round by someone standing in the middle–you can see the scale when this zooms out– the map is eleven by seventeen FEET, and the person standing next to it, Wayne who works in the library at Stanford, is six feet four inches tall. This image is a composite of 158 individual images with a file size of 1.27Gb. IIIF allows just enough of an image to be delivered to a viewer–going from a whole image to just the part that they are zooming in on. Without IIIF, an end user might have to download an extremely large file, but using IIIF provides a smooth and easy viewing experience.\n\nHave a play and view this image in their Mirador viewer here\n\n\nVirtual Reconstruction\nThe virtual reconstruction of this damaged manuscript from Châteauroux in France (Grandes Chroniques de France, ca. 1460) is probably one of the most well-known and best examples of the power of IIIF to support this use case (and my own personal favourite!). At some point in the manuscripts history, fourteen of its illuminations were cut out. These illuminations eventually ended up at the Bibliothèque nationale de France in the 19th century and were digitised individually. In the demo you see the reuniting of the miniatures with the full manuscript as IIIF allows a virtual repositioning of the cut out decorations with the text, virtually reconstructing the manuscript online using the Mirador Viewer so it reflects its original state.\n\nI highly recommend having a play around with the Mirador Viewer: Châteauroux demo.",
    "crumbs": [
      "Topic Guides",
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "iiif.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "IIIF",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nThe following tutorials are two of my favourite recommendations for colleagues interested in having a play with IIIF manifests yourself and in the process gaining a practical understanding of how they work, and the benefits!\n\nIIIF Online Workshops The community itself has created a number of excellent and free self-paced tutorials and though they host live online workshops for a fee, these are recorded and available and useful for newcomers for free afterwards. In fact staff at British Library have walked through these self-guided resources online quite often with great success. There is also the opportunity to hire a IIIF Trainer to come to deliver live bespoke training directly to your institution (for a fee), which we have also partaken in!\nWorking with IIIF images in education, communication and research This is a self-guided workshop available online in Dutch and English and has some excellent exercises to get you familiar with finding IIIF manifests in catalogues and importing them into different viewers. I highly recommend making some time (they recommend 120 minutes) to read through this and try out some of the exercises.",
    "crumbs": [
      "Topic Guides",
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#recommended-readingviewing",
    "href": "iiif.html#recommended-readingviewing",
    "title": "IIIF",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nThere is a useful collection of articles and data related to IIIF being compiled by the community on Zenodo\nThe IIIF organisation has also created a number of useful resources alongside their training materials such as How It Works, a plain-language guide to how the IIIF API’s work and a glossary of “Key concepts you’ll encounter when working with IIIF”.\nIt’s worth having a look at how other institutions have provided IIIF Resources.\nThere is also a massive list of learning resources, Awesome IIIF, compiled and maintained by the IIIF Community if you are looking to take your knowledge a bit further and dig deeper into some of the exciting implementations of IIIF.",
    "crumbs": [
      "Topic Guides",
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#finding-communities-of-practice",
    "href": "iiif.html#finding-communities-of-practice",
    "title": "IIIF",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nLearning about IIIF can be overwhelming at first, especially if you’re not a programmer, but the IIIF Community is a very supportive and engaged one and has created a number of ways to get involved and find support and help.\nI recommend checking out their community page IIIF Community to find details of their next open community calls, or to join their Slack Channel where you can post questions and join the discussion with other users.",
    "crumbs": [
      "Topic Guides",
      "IIIF"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "RECOMMENDED READING LISTS",
    "section": "",
    "text": "RECOMMENDED READING LISTS\nThere is no shortage of recommended reading lists out there, here we try and highlight a few of the most up to date and useful lists out there for the sector today!\nZotero Library for Digital Scholarship and Data Science Essentials for Library Professionals\nThis library contains links to all of the resources referenced within Topic Guides and across our site.\nZotero | Groups &gt; collections as data - projects, initiatives, readings, tools, datasets\nOngoing collection of projects, readings, initiatives, tools, and datasets that are in some way or another related to collections as data. This group is an open resource, welcoming contributions from anyone who has a resource to share.\n[TODO] Data Science in Libraries, is there a Zotero?"
  },
  {
    "objectID": "collectionsasdata.html",
    "href": "collectionsasdata.html",
    "title": "Collections as Data: Getting Started",
    "section": "",
    "text": "GLAM (Galleries, Libraries, Archives and Museums) institutions have been making their content available to the public and researchers in a variety of formats for different purposes for decades. From maps, images, text, historical newspapers or postcards, digitising collections and displaying our items online are at the core of making this information accessible for research and enjoyment. But recent advances in and expansion of the use of AI and Machine Learning in library work and library research have transformed the way in which we and our users expect to access, view, use and search our collections. As the demand for computationally accessible cultural heritage collections continues to grow, and we’re being asked to provide our collections in the form of datasets for instance, how do we go about this practically?\nIn this new context, where GLAM institutions now find themselves playing a leading role as data providers and data curators, the Collections as Data concept emerged as a new approach, guidance and framework to support responsible development and computational use of digital cultural heritage collections. By computational use we mean the application of computational techniques such as natural language processing (NLP), computer vision, and more to the analysis, exploration and reuse at scale of digitised cultural heritage content. For more practical use cases see our AI and Machine Learning in Libraries topic guide.\nThree particularly useful outputs from the Collections as Data community of practice are:\n\nFifty things (2018)\n\n“Want to support collections as data at your institution, but not sure how to begin? Drawing on what we learned from engaging with practitioners and researchers throughout the Always Already Computational project, the project team compiled a list of 50 Things you can do to get started. 50 Things is intended to open eyes, stimulate conversation, encourage stepping back, generate ideas, and surface new possibilities. If any of that gets traction, then perhaps you can make the case for investing in collections as data at your institution in a meaningful, if not systematic, way.”\n\nVancouver Statement on Collections as data (2023)\n\n“The Vancouver Statement suggests a set of principles for thinking through questions that collections-as-data work produces, as part of an expanding global, interprofessional, and interdisciplinary effort to empower memory, knowledge, and data stewards (for example, practitioners and scholars) who aim to support responsible development and computational use of collections as data. This stewardship role only grows in importance as artificial intelligence applications, trained on vast amounts of data, including collections as data, impact our lives ever more pervasively.”\n\nA Checklist to Publish Collections as Data in GLAM Institutions (2023)\n\nDeveloped as part of the collaborative work of the International GLAM Labs community, the checklist covers different aspects such as providing a suggested citation, including documentation about the datasets (for example, README files or tutorials) or sharing examples of use (for example, prototypes or Jupyter Notebooks).",
    "crumbs": [
      "Topic Guides",
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#introduction",
    "href": "collectionsasdata.html#introduction",
    "title": "Collections as Data: Getting Started",
    "section": "",
    "text": "GLAM (Galleries, Libraries, Archives and Museums) institutions have been making their content available to the public and researchers in a variety of formats for different purposes for decades. From maps, images, text, historical newspapers or postcards, digitising collections and displaying our items online are at the core of making this information accessible for research and enjoyment. But recent advances in and expansion of the use of AI and Machine Learning in library work and library research have transformed the way in which we and our users expect to access, view, use and search our collections. As the demand for computationally accessible cultural heritage collections continues to grow, and we’re being asked to provide our collections in the form of datasets for instance, how do we go about this practically?\nIn this new context, where GLAM institutions now find themselves playing a leading role as data providers and data curators, the Collections as Data concept emerged as a new approach, guidance and framework to support responsible development and computational use of digital cultural heritage collections. By computational use we mean the application of computational techniques such as natural language processing (NLP), computer vision, and more to the analysis, exploration and reuse at scale of digitised cultural heritage content. For more practical use cases see our AI and Machine Learning in Libraries topic guide.\nThree particularly useful outputs from the Collections as Data community of practice are:\n\nFifty things (2018)\n\n“Want to support collections as data at your institution, but not sure how to begin? Drawing on what we learned from engaging with practitioners and researchers throughout the Always Already Computational project, the project team compiled a list of 50 Things you can do to get started. 50 Things is intended to open eyes, stimulate conversation, encourage stepping back, generate ideas, and surface new possibilities. If any of that gets traction, then perhaps you can make the case for investing in collections as data at your institution in a meaningful, if not systematic, way.”\n\nVancouver Statement on Collections as data (2023)\n\n“The Vancouver Statement suggests a set of principles for thinking through questions that collections-as-data work produces, as part of an expanding global, interprofessional, and interdisciplinary effort to empower memory, knowledge, and data stewards (for example, practitioners and scholars) who aim to support responsible development and computational use of collections as data. This stewardship role only grows in importance as artificial intelligence applications, trained on vast amounts of data, including collections as data, impact our lives ever more pervasively.”\n\nA Checklist to Publish Collections as Data in GLAM Institutions (2023)\n\nDeveloped as part of the collaborative work of the International GLAM Labs community, the checklist covers different aspects such as providing a suggested citation, including documentation about the datasets (for example, README files or tutorials) or sharing examples of use (for example, prototypes or Jupyter Notebooks).",
    "crumbs": [
      "Topic Guides",
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "collectionsasdata.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Collections as Data: Getting Started",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nMany institutions have started to adopt the collections as data principles to varying degrees, often combining publication of digital collections suitable for computational use, with a lab offering technical support to use them. You might notice that the examples below are all slightly different implementations of the principles though as each institution will necessarily need to decide for themselves, based on their individual goals and constraints (such as lack of resources, staff or IT skills) how to practically adopt the Collections as Data principles.\n\nData Foundry at the National Library of Scotland provides datasets in the form of downloadable files. Each of them includes a website with transparent information about the creation and curation of the content. It also provides examples of use and prototypes in the form of Jupyter Notebooks.\nNational Library of Luxembourg provides a digital collection based on Historical newspapers. It follows an innovative approach by providing different datasets in terms of the size and the content according to the purpose of the reuse (for example “getting started” or “Big Data”).\nBritish Library Labs provides a selection of datasets made available under open licences to experiment.\nDATA-KBR-BE is a project that aims at facilitating data-level access to the KBR (Royal Library of Belgium) digitised and born-digital collections for digital humanities research.\nBiblioteca Virtual Miguel de Cervantes (BVMC Labs) is a digital library that published its bibliographic catalogue in the form of Linked Open Data. It also provides examples of use by means of Jupyter Notebooks.\n\nMore advanced approaches are focused on the concept of data spaces, which are new cloud environments in which data can be shared for research use, while also allowing data suppliers to retain rights and control over the data. This can be useful for contexts in which the access of the data is restricted (for examples, geographic region, licensed content). A workflow to publish Collections as Data: the case of Cultural Heritage data spaces gives more information on this topic in the context of the European common data space for cultural heritage.\nIt is important to note that institutions need to carefully consider how and for what purpose they want to publish their digital content. For instance, the National Library of the Netherlands has recently restricted access to collections for training commercial AI. See our DS Topic Guide on Copyright and Licensing for more on that.",
    "crumbs": [
      "Topic Guides",
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "collectionsasdata.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Collections as Data: Getting Started",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nOne simple activity you could do at your institution is to gather a group of like-minded colleagues together and have a read through Fifty things, Vancouver Statement on Collections as data and/or A Checklist to Publish Collections as Data in GLAM Institutions and have a look at some of the case studies above, and as a group consider the questions:\n\nAre there some activities suggested that are already underway in your institution?\nCan you identify one or two simple things that your institution could do now in order to start using these principles?\nWhat would you like “collections as data” to look like at your institution?\n\nIf you’d like to get a sense of how people are reusing collections published as data, the GLAM Workbench and the new computational access section of the International GLAM Labs Community website both have tutorials that can walk you through using cultural heritage datasets in a variety of ways.",
    "crumbs": [
      "Topic Guides",
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#recommended-readingviewing",
    "href": "collectionsasdata.html#recommended-readingviewing",
    "title": "Collections as Data: Getting Started",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nIf you are interested in reading more about the collections as data concept we highly recommend the Zotero | Groups &gt; collections as data - projects, initiatives, readings, tools, datasets which is an open bibliography collaboratively maintained by the community of practice of projects, readings, initiatives, tools, and datasets that are in some way or another related to collections as data.",
    "crumbs": [
      "Topic Guides",
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#finding-communities-of-practice",
    "href": "collectionsasdata.html#finding-communities-of-practice",
    "title": "Collections as Data: Getting Started",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nJoining the International GLAM Labs Community is a great way to get started on the path of opening your collections up as data as well as the Collections as Data - Google Group as both communities are very active and have a wide range of experience and expertise to share on this topic. LIBER’s Working Groups address important areas of work for the research library community) address important areas of work for the research library community such as Data Science and Research Data Management.Some international organisations also have special interest groups such as the Research Data Alliance (RDA) which hosts the Collections as Data IG.",
    "crumbs": [
      "Topic Guides",
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualisation",
    "section": "",
    "text": "Why do we visualise data? For data to tell us something we need to look for patterns, and we are much better at finding these patterns in colours and shapes than in a table of raw data. Visualisations are key to developing the story we want to tell with our data.\nWhen do we visualise data? There are two main moments when working with data that we need to visualise it. First, in the exploratory phase, when we are trying to understand the data, to get insights that lead to fruitful lines of enquiry. These visualisations are rough and ready, meant for us or at most a select few around us to generate debate about what information the data might contain.\nSecond, in the explanatory phase, when we have understood the data and move on to insightful analyses that generate new understanding. These visualisations communicate what we have learned from the data to others. They need to be clear, because we are trying to explain conclusions we have made from intimate knowledge of the data to people who have not worked with it and are trusting us to explain faithfully what we have learned.\nIn libraries, we use explanatory visualiations to improve content discovery for users of our services, and so they can understand our data, as well as for research carried out by librarians and digital humanists that generates new knowledge. The visual routes we supply into data for our users need to be as clear as anything we’re presenting as research.\nHow do we visualise data? There are a huge variety of tools available. Practitioners may lovingly hand draw images (physically or digitally), like early 19th century visualisations by Florence Nightingale and W. E. B. Du Bois and the work, or art, of Federica Fragapane. More commonly there are deeply customisable packages in modern programming languages like R, Python or Javascript, or commercial plotting software like Tableau. Excel has endured through its simple learning curve, ubiquity and reliable outputs. A whole separate suite of software exists for geospatial and linked data. Ultimately the right choice is decided by our use case, data, resources and skills.",
    "crumbs": [
      "Topic Guides",
      "Data Visualisation"
    ]
  },
  {
    "objectID": "dataviz.html#introduction",
    "href": "dataviz.html#introduction",
    "title": "Data Visualisation",
    "section": "",
    "text": "Why do we visualise data? For data to tell us something we need to look for patterns, and we are much better at finding these patterns in colours and shapes than in a table of raw data. Visualisations are key to developing the story we want to tell with our data.\nWhen do we visualise data? There are two main moments when working with data that we need to visualise it. First, in the exploratory phase, when we are trying to understand the data, to get insights that lead to fruitful lines of enquiry. These visualisations are rough and ready, meant for us or at most a select few around us to generate debate about what information the data might contain.\nSecond, in the explanatory phase, when we have understood the data and move on to insightful analyses that generate new understanding. These visualisations communicate what we have learned from the data to others. They need to be clear, because we are trying to explain conclusions we have made from intimate knowledge of the data to people who have not worked with it and are trusting us to explain faithfully what we have learned.\nIn libraries, we use explanatory visualiations to improve content discovery for users of our services, and so they can understand our data, as well as for research carried out by librarians and digital humanists that generates new knowledge. The visual routes we supply into data for our users need to be as clear as anything we’re presenting as research.\nHow do we visualise data? There are a huge variety of tools available. Practitioners may lovingly hand draw images (physically or digitally), like early 19th century visualisations by Florence Nightingale and W. E. B. Du Bois and the work, or art, of Federica Fragapane. More commonly there are deeply customisable packages in modern programming languages like R, Python or Javascript, or commercial plotting software like Tableau. Excel has endured through its simple learning curve, ubiquity and reliable outputs. A whole separate suite of software exists for geospatial and linked data. Ultimately the right choice is decided by our use case, data, resources and skills.",
    "crumbs": [
      "Topic Guides",
      "Data Visualisation"
    ]
  },
  {
    "objectID": "dataviz.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "dataviz.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Data Visualisation",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nExploratory data visualisations are both quite generic and very dataset specific. Plots like bar charts, histograms, scatter plots, and time series display information in a simple enough way to be reliable and fit the ethos of being quick and informative about the features of your data. Their interpretability also means they are used as explanatory charts, and frequently appear in papers and reports. This Power BI dashboard from Brandi Jagars at University of South Florida Libraries shows how simple visualisations give quick insights into a visitor dataset.\n\n\n\nA dashboard of exploratory graphs.\n\n\nVisualisations designed for publication and for users to consume themselves are much more varied, and (usually) more polished. Ask yourself if you are looking to communicate a succint, unambiguous message or if you are inviting the user to explore the visual.\nMaps are an eye-catching and engaging interface to collections that tap into users’ sense of place. The Mapping Manuscript Migrations project offers a map view of global manuscript migrations allowing users to track the movement of manuscripts filtered by things like collection, author and date of publication.\n\n\n\nMovement of manuscripts from the collection of Sir Thomas Phillipps.\n\n\nPeripleo is a browser-based map viewer that can be used with any cultural heritage dataset with associated location information. It was used in the Heritage for All project to displaying items in hyper-local contexts.\n\n\n\nMap of cultural heritage item locations around Exeter.\n\n\nBoth of these initiatives rely on the concept of Linked Open Data. Each element of linked data is linked to other elements by one of a defined set of relationships, rather than the traditional spreadsheet model where each row is an item with a certain set of properties. This transforms the data into a network, which allows for intuitive, interactive visualisations that let users navigate material contextualised by the items closely linked to it.\nThe Semantic Name Authority Cymru (SNARC) provides views into a linked database of name authority records linked to Wales and the Welsh language. This makes graphs like this family tree of Charlotte Guest, a translator, businesswoman and noble, easy to produce. Displaying parts of the network lets users understand the connections within it, as in this rather large, but very satisfying, graph of Welsh estates, houses and their owners.\n\n\n\nSNARC Welsh estates network graph.\n\n\nThere’s also huge value in making visualisations like this available in physical form within the spaces of a library. The Bohemian Bookshelf was a 2011 project to increase serendipitous book discovery that was installed in the University of Calgary Library. It used 5 different visualisations to ‘entice curiosity’ by taking users to books and authors they might not have otherwise explored. This echoes Dr Mia Ridge’s proposed metric of ‘clicks to curiosity inspired’ in seeking ways to make it easy for users to be inspired by the collection. The ‘book pile’ arranged books by size and page count, acknowledging our natural fascination with the very large and the very small.\n\n\n\nThe Bohemian Bookshelf ‘Book Pile’.\n\n\nYou can explore other use cases in this helpful list from the University of Minnesota Libraries. They’ve catalogued library specific resources for a range of use cases like the teaching, evaluation, and history of data visualisation in libraries.",
    "crumbs": [
      "Topic Guides",
      "Data Visualisation"
    ]
  },
  {
    "objectID": "dataviz.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "dataviz.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Data Visualisation",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nThe best way to understand the value of data visualisations is to produce them for your datasets. Here are a few tools you can plug datasets into, organised by type and the skills needed to use them.\n\nImmediate results\nRAWGraphs is an online platform (no sign up required) you can upload a spreadsheet to (save it as a CSV first) or a JSON file and point and click your way down the page to produce a visualisation. It’s perfect for exploratory analysis and learning about the different kinds of visualisations available.\nVoyant Tools provides a similarly easy entry for corpus scale text data (whole works, or collections of works), though it helps if you know a little about corpus linguistics. There are ‘pre-built’ corpora of Shakespeare, Austen and Frankenstein available if you don’t have your own files to upload.\n\n\nSpreadsheet-based\nExcel remains such an easy way to interact with spreadsheet data. If you haven’t used it before there are lots of resources available online or your institution may have Microsoft Office skills courses. This six hour course from the Open University (UK) doesn’t assume any knowledge. PowerBI is a more advanced Microsoft Office app that allows you to create dashboards from data. It interfaces easily with Office software, but the visualisations aren’t hugely inspiring. Justin Kelly has an introduction for librarians. Google Charts and Sheets and OpenOffice Calc and Impress are equivalent alternatives.\nTableau is one of many commercial softwares for visualisation. There’s a learning curve similar to Excel, and a free (sign-up required) public platform you can try in browser, use the learning resources to get started.\n\n\nIntro to coding\nThe R for Data Science Data Visualisation tutorial covers using ggplot2, the de facto standard for plotting in R. You can code along with an R environment in your browser using Posit Cloud (requires a free account).\nSeaborn is one of the main plotting packages in Python and follows a similar philosophy to ggplot2. You can code along to their tutorial with a Python environment in browser using Google Colab (requires a Google account).\n\n\nMapping and Linked Data\nGeospatial and linked data (where elements of the data can be explicitly linked to other elements) have their own worlds of tools, with functionality also often covered by the types of tools already listed. ArcGIS (now common in its online form) and QGIS are the most common paid and open source Geographic Information Software (GIS) tools available. You can use these to work with data with geographic components and make display worthy maps. Their outputs also plug into Python, R and JavaScript libraries like leaflet or Dash. Programming Historian have a series of mapping lessons, from an intro to QGIS to converting historical place names into locations on a map.\nFor linked data tools like Gephi or Nodegoat have graphical user interfaces, or there are programming packages like igraph, NetworkX, and D3.js.\n\n\nCommon mistakes\nIt’s tempting for beginners to complicate graphs by adding eye-catching effects like animations or 3D elements. This can, however, make graphs harder to read. When in doubt, opt for simplicity! From Data to Viz has a great page on data visualisation caveats which includes examples of common mistakes and is worth reading.",
    "crumbs": [
      "Topic Guides",
      "Data Visualisation"
    ]
  },
  {
    "objectID": "dataviz.html#recommended-readingviewing",
    "href": "dataviz.html#recommended-readingviewing",
    "title": "Data Visualisation",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nMuch of data visualisation is communication, and so is deeply subjective. With that in mind there are graphics catalogues that help guide you towards the best visualisation for your purposes. From Data to Viz starts with types of data and leads you to appropriate graphs, while the Financial Times Visual Vocabulary starts with relationships between elements of your data and guides from there. Both are valuable. The EU also maintains a list of generic resources the include some of the ones mentioned here.\nGood visualisations are guided by design philosophies built upon how we process visual information. Two influential books that develop these philosophies are The Grammar of Graphics by Leland Wilkinson and The Visual Display of Quantitative Information (2nd Ed.) by Edward R. Tufte. The Grammar of Graphics is a hefty tome that proposes a core set of components for graphics then builds them from the ground up, with reference to programming. Visual Display is perhaps a little more accessible and relies on the idea of how ink is used (digitally or physically) to understand what is and is not important in a graphic.\nIf visualisation is communication and how we understand visual communication is subjective, then catering generously to how different people process visual information is important. We might call this accessibility. This talk explains some of the basics of how our brains handle colour, and the importance of colour in visualisation. The Seaborn explanation of colour palettes is a helpful reference, and there is a free colour blindness tester to check that your visualisations encode information in colours that everyone can distinguish. Use alt-text for screen reader access and review for keyboard and content accessibility. Harvard have a helpful guide.\n\n\n\nThe importance of colour in recognising categories.",
    "crumbs": [
      "Topic Guides",
      "Data Visualisation"
    ]
  },
  {
    "objectID": "dataviz.html#finding-communities-of-practice",
    "href": "dataviz.html#finding-communities-of-practice",
    "title": "Data Visualisation",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nData visualisation, like any other skill, takes practice and familiarity with tools to get the best results. It is, however, relatively easy to make simple but effective visualisations that help you understand your data or explain it to someone else. So take some of your data and begin to play around with it. Play is an important word - you are being creative! It is, unfortunately, just as easy to make bad visualisations that confuse you and your audience. But make good mistakes and do the unexpected as you learn, and you will be better for it. Start simply, more complex visualisations will come with time.\nEngage with designers if they’re available to you and you are producing work for users or the wider public. The art of designing things for people applies as much to data visualisation as it does to anything else.\nHere are some communities that you may want to engage with:\n\nThe Data Visualization Society aiming to “celebrate, nurture, and advance the field of data visualization”, host a range of resources and support the Information is Beautiful Awards each year\nThe Canadian Association of Professional Academic Librarians host a Data Visualization Community of Practice\nGeneral data visualisation groups like Data Viz Singapore may not have all the context for library professionals but are likely to be very welcoming spaces\nThere are a range of data visualisation guides that can act to links to communities in US (university) libraries including Harvard, California State Library, NC State University, Michigan State University\nAny data analysis community of practice is also likely to include and element of data visualisation. Meetup.com may have relevant groups near you.\nLIBER (Ligue des Bibliothèques Européennes de Recherche – Association of European Research Libraries) host a Data Science working group which touches on data visualisation\nSCONUL (UK) have a community of interest for library analytics.\n\nAbove all remember your audience. Keep clear in your mind what it is you are trying to communicate, and to who, and ask yourself if your visualisation does that. Continue to refine it until it does, and you’ll have made a good visualisation.",
    "crumbs": [
      "Topic Guides",
      "Data Visualisation"
    ]
  },
  {
    "objectID": "dstp.html",
    "href": "dstp.html",
    "title": "Start your own local DS Training Programme",
    "section": "",
    "text": "The Digital Research Team has been providing training to British Library staff for over a decade through our bespoke Digital Scholarship Training Programme.\nThis DS Topic Guide shares a few things we’ve learned along the way about establishing an internal community of practice and training programme. It includes practical pointers for using the materials presented here to start a localised and sustainable digital scholarship and data science training programme at your own institution!",
    "crumbs": [
      "Topic Guides",
      "Start your own local DS Training Programme"
    ]
  },
  {
    "objectID": "dstp.html#introduction",
    "href": "dstp.html#introduction",
    "title": "Start your own local DS Training Programme",
    "section": "",
    "text": "The Digital Research Team has been providing training to British Library staff for over a decade through our bespoke Digital Scholarship Training Programme.\nThis DS Topic Guide shares a few things we’ve learned along the way about establishing an internal community of practice and training programme. It includes practical pointers for using the materials presented here to start a localised and sustainable digital scholarship and data science training programme at your own institution!",
    "crumbs": [
      "Topic Guides",
      "Start your own local DS Training Programme"
    ]
  },
  {
    "objectID": "dstp.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "dstp.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Start your own local DS Training Programme",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nTo understand the importance of supporting the development of digital skills in our sector, one only needs to look at Digital Scholarship & Data Science Topic Guides for Library Professionals, which contains a plethora of useful real-life applications of new technologies being put into action in libraries. The DS Topic Guide: Getting Started in DS also provides extensive evidence such as a number of Digital Scholarship & Data Science Skills Competency Frameworks for library staff which underscore the urgent need for upskilling.\nBut what are the benefits in particular of establishing a local training programme or building a community of practice for staff in your institution? By far the most compelling argument we’ve found is that by creating a welcoming space, and some structure, where colleagues can regularly come together, explore new ideas and share knowledge, a culture of continuous learning has thrived. This makes keeping up with the latest developments in digital scholarship and data science applications much easier.",
    "crumbs": [
      "Topic Guides",
      "Start your own local DS Training Programme"
    ]
  },
  {
    "objectID": "dstp.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "dstp.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Start your own local DS Training Programme",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nIf you’ve been thinking about getting your own local training programme up and running, whether a small casual interest group or a full-blown digital transformation initiative, the following practical steps can be used to help you on your way!\n\nIdentify shared needs\nWhat are problems you and your colleagues at your institution are facing (cataloguing backlogs? fear or lack of understanding of AI? etc.), and how might a new fledgling interest group help you collectively navigate and solve these? By framing digital scholarship/data science skill training as an investment, as a helpful tool for colleagues to resolve long standing issues, you will find a more willing audience for your efforts. Quick wins in the digital realm (for instance in learning a tool like Open Refine to normalise and analyse catalogue records) can often build confidence very quickly and open the door to trying out new technologies. A decade ago at British Library we realised we had little practical knowledge around the field of Digital Humanities, so our shared need was to gain an overview of what those researchers were up to, and what they needed from the Library. As our community’s knowledge evolved, so did the scope of the training programme and today we are as much interested in how researchers use technologies innovatively as we are with the practical adoption of them ourselves.\n\n\nFind your people\nGet a small group of willing colleagues at your institution together and set up a way for them to communicate regularly, preferably asynchronously/remotely. We use a corporate wide MS Teams channel for this at British Library but you could also set up things like a slack channel or even a whatsapp channel depending on where people at your institution tend to gravitate. Focus on your purpose of bringing people together, think about the messaging and keep it light, “Learn new stuff together that can help make your job easier” usually suffices! Try not to be too targeted - cast the net wide as people of all interests/academic disciplines/job profiles, abilities and backgrounds will undoubtedly have something to contribute: digital scholarship and data science is a collaborative affair and your group will be all the better for its diversity.\nSharing invitations to your events as widely as possible has another side-effect - people who aren’t able to attend our sessions report that they learn a lot just by seeing what kinds of topics we’re covering!\n\n\nGet Buy-in (if you need to!)\nIn some cases you may need buy-in from managers to allow staff time and space to dedicate to learning these new skills. It can be difficult to convince managers sometimes of the value of learning skills now which may take some years to truly embed or come to practical fruition. Spending time building ones “awareness” of the landscape is not always deemed a strong enough basis for approving time away from more immediate tangible deliverables, but awareness is the first step on the learning ladder and is essential for transformation. This resource, Digital Scholarship & Data Science Topic Guides for Library Professionals, contains loads of evidence to share with your management for why investing in digital scholarship and data science staff skills and awareness is key to sustainable digital transformation over time. Have a look at the Getting Started in DS Topic Guide, particularly the section on Relevance to the Library Sector for the latest supporting reports and competency & skills frameworks on this.\nWhen we set up our training programme we laid out some specific objectives to make clear to ourselves and colleagues what we ultimately hoped to achieve with it:\n\nStaff are familiar and conversant with the foundational concepts, methods and tools of digital scholarship.\nStaff are empowered to innovate.\nCollaborative digital initiatives flourish across subject areas within the Library as well as externally.\nOur internal capacity for training and skill-sharing in digital scholarship are a shared responsibility across the Library.\n\nNothing compares to individual staff digital transformation stories though, and at the British Library we’ve started to capture these which help to demonstrate the tangible value such training has for individuals and institutions.\n\n“The Digital Scholarship Training Programme has introduced me to new software, opened my eyes to digital opportunities, provided inspiration for me to improve, and helped me attain new skills” -Graham Jevon, British Library\n\nRead more about Graham Jevon’s digital transformation journey on the British Library Digital Scholarship blog or have a look at this series of videos we created to mark the 10th Anniversary of the British Library Digital Scholarship Training Programme in 2022. It highlights specific transformative experiences for staff who took time out to engage in our Digital Scholarship Training Programme over the years.\n\n\n\nHow are you using your new skills in your own work?\n\n\n\n\nTry running a monthly “Hack & Yack”\nEstablishing what we call a regular Hack & Yack style meeting is a really nice way to learn something new with other colleagues and removing the planning and expectations around a formal training offering. At the British Library our Hack & Yack’s are once a month for two hours on a set day and time. It is a casual, hands-on session where colleagues from across all departments of the institution come together to understand a current topic or digital method by working through an online tutorial or demonstration at everyone’s own pace but with support of each other.\nWe use our Hack & Yack’s as an opportunity to explore new tools/techniques/applications relevant to digital research and keep our own skills up to speed. It is not a formal training session. Attendees are reminded that we’re all learning together and that everyone will be coming to the tutorials with different experience/knowledge/skills.\nEach Hack & Yack usually starts with one person, which may or may not be the organiser, giving a high-level view of the topic of the day, and then sharing one or more tutorials that the group can try out, either stepping through one online activity together as a group or providing time for everyone to explore individually at their own pace and chat about how it’s going as they work through the steps. These sessions aren’t recorded so that attendees can be open and frank about their experience!\nWithin each of the DS Topic Guides is a Hands-on activity and other self-guided tutorial(s) section that would make a perfect start for your first Hack & Yacks or take a look at the many platforms hosting training materials referenced in Getting Started in DS!\n\n\nStart a discussion/reading group\nWhether you call it a ‘reading group’, a discussion group, a lunchtime series - the point is to provide regular opportunities for people to get together, and learn and support each other through discussion. You might set an article, blog post or chapter, a video or podcast.\nThings we’ve found useful:\nBegin each session with quick introductions - name and department, or name and another useful piece of information. Why? Hopefully you’ll have a range of folk and they mightn’t all have met before. It means that everyone has spoken at least once right from the start, which is important for encouraging contributions.\nMake it ok for people not to have finished reading, watching or listening to the thing you’re discussing. You could do a show of hands to see who’s finished it or not finished it. Why? There’s something about ‘confessing’ that lets people ask questions without worrying that it was covered somewhere towards the end. It can also be useful to understand why people didn’t finish (unless they just ran out of time, which is highly relatable) - did the piece get complex, jargony, boring?\nIf you have a range of personality types at your events, you can set some ground rules to encourage more equitable participation. For example, you could ask people to ‘raise their (onscreen) hands’ when they want to contribute, and come to people in the order that their hands were raised. You can also ask people who haven’t spoken much for their thoughts - some people appreciate a prompt.\nEach DS Topic Guide has a Recommended Reading/Viewing section that might give you some good ideas on reading materials.\n\n\nPut together a regular talk series\nDid you go to a conference and were inspired by something you heard? Did someone on social media say something interesting about the intersection of cultural heritage and data? Is someone coming by your Library for a meeting and they’re working in digital scholarship and data science? Don’t be shy! Reach out and ask them if they’d be willing to share their knowledge with your group. At the British Library we call our talk series “21st Century Curatorship talks” and it’s been a wonderful opportunity to learn from the best in the sector, while also growing interest in the wider training programme. When colleagues feel they only have an hour to spare, or are not yet ready for the hands-on of a hack & yack, offering talks are a great way to lower the barrier of entry to complex topics. We often use these talks to gauge interest in a topic and then plan our reading group and hack & yack topics around deepening knowledge of the topic. We’ve found that staff members remember and share the stories they hear at these talks, increasing their impact and reaching people who don’t attend.\n\n\nCreate an identity\nAfter your group of the willing has been running for awhile it’s useful to create an identity for yourselves. Perhaps you consider yourselves a network, or an interest group or maybe you’d like to start formalising your gatherings under a “training programme” moniker. Whatever you decide, it helps to bring in new folks when there is an established identity and even maybe a logo in advertisements.",
    "crumbs": [
      "Topic Guides",
      "Start your own local DS Training Programme"
    ]
  },
  {
    "objectID": "dstp.html#recommended-readingviewing",
    "href": "dstp.html#recommended-readingviewing",
    "title": "Start your own local DS Training Programme",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\n\n\n\nWatch the video\n\n\nIn this short video from a Culture24 Digital Leaders - digital skills, literacy and capacity online seminar, I talk about inspiring and supporting team members from The British Library to pursue digital skills and literacy development through their Digital Scholarship Staff Training Programme.\nThe British Library Digital Research Team regularly share information about the training programme including topics covered over on our Digital Scholarship Blog: * 2024 Year in Review - Digital Scholarship Training Programme * Topics in contemporary Digital Scholarship via five years of our Reading Group * What do deep learning, community archives, Livy and the politics of artefacts have in common? Topics covered in our Reading Group",
    "crumbs": [
      "Topic Guides",
      "Start your own local DS Training Programme"
    ]
  },
  {
    "objectID": "dstp.html#finding-communities-of-practice",
    "href": "dstp.html#finding-communities-of-practice",
    "title": "Start your own local DS Training Programme",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nThe Digital Research Team of the British Library are more than happy to be contacted at digitalresearch@bl.uk for a chat about their experiences in starting up a local DS training programme.",
    "crumbs": [
      "Topic Guides",
      "Start your own local DS Training Programme"
    ]
  },
  {
    "objectID": "topicguides.html",
    "href": "topicguides.html",
    "title": "Topic Guides",
    "section": "",
    "text": "Written by LIBER professionals working in research libraries across Europe, the aim of each guide is to provide a gentle and concise introduction to practical applications of digital scholarship and data science to the work of libraries today, and share personal recommendations for useful tutorials, recommended reading, and communities of practice to help colleagues on their learning journey.\n\n\nWhere do I start?\n\n\nWe have a DS Topic Guide for that! Try Getting Started in DS.\n\n\n\n\nIf only I coud make these work…\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\n\n\n\n\nHelp!\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\n\n\n\n\nThis resource has a dual purpose; it is written as a tool for self-study and quick reference, but also as a guide for individuals and institutions interested in establishing their own local training programmes.\n\n\nAs a self-study resource, we hope it can serve as a useful first point of entry into complex topics in digital scholarship and data science and their application in the library world. It is not meant to be completed in any particular order, rather, learners are invited to jump in and out of individual topic guides as personal curiosity or practical need dictates.\nEach Topic Guide is designed to make it easy to quickly find the information you might need:\n\nIntroduction to the topic\nRelevance to the library sector (case studies/use cases)\nHands-on activities and other self-guided tutorial(s)\nRecommended reading & viewing\nFinding Communities of Practice\n\n\n\n\nThe materials here can also be utilised in group study, as part of a reading group or even a hands-on workshop. See our Topic Guide on how to Start your own training programme for some tips and tricks from British Library Digital Research Team colleagues on utilising the materials referenced here at your own library or within your network.\nDon’t know where to begin? Visit Getting Started in DS",
    "crumbs": [
      "Topic Guides",
      "**Topic Guides**"
    ]
  },
  {
    "objectID": "topicguides.html#how-to-use-ds-topic-guides",
    "href": "topicguides.html#how-to-use-ds-topic-guides",
    "title": "Topic Guides",
    "section": "",
    "text": "This resource has a dual purpose; it is written as a tool for self-study and quick reference, but also as a guide for individuals and institutions interested in establishing their own local training programmes.\n\n\nAs a self-study resource, we hope it can serve as a useful first point of entry into complex topics in digital scholarship and data science and their application in the library world. It is not meant to be completed in any particular order, rather, learners are invited to jump in and out of individual topic guides as personal curiosity or practical need dictates.\nEach Topic Guide is designed to make it easy to quickly find the information you might need:\n\nIntroduction to the topic\nRelevance to the library sector (case studies/use cases)\nHands-on activities and other self-guided tutorial(s)\nRecommended reading & viewing\nFinding Communities of Practice\n\n\n\n\nThe materials here can also be utilised in group study, as part of a reading group or even a hands-on workshop. See our Topic Guide on how to Start your own training programme for some tips and tricks from British Library Digital Research Team colleagues on utilising the materials referenced here at your own library or within your network.\nDon’t know where to begin? Visit Getting Started in DS",
    "crumbs": [
      "Topic Guides",
      "**Topic Guides**"
    ]
  },
  {
    "objectID": "computer-vision.html",
    "href": "computer-vision.html",
    "title": "Computer Vision",
    "section": "",
    "text": "Computer Vision is a field of Artificial Intelligence (AI) that uses Deep Learning models to teach machines to “see” and interpret images and videos. At its core, Computer Vision involves functions like processing, detection, and analysis of visual data. It uses algorithms to extract meaningful information from images and enables machines to identify objects, recognise patterns, and make decisions based on what they see. Computer Vision powers many of today’s technologies—from facial recognition and object detection to autonomous vehicles, augmented reality, and Video Assistant Referees (VAR) in football. It’s also integral to tools used in cultural heritage and information management, helping libraries automatically tag, classify, and organize vast visual collections such as photographs, manuscripts, and artworks.\nDeep Learning uses neural networks to model complex patterns within data. These networks, much like the human brain, consist of interconnected layers of artificial neurons. During training, these models process massive datasets of images, enabling them to progressively identify features and refine their predictions with increasing accuracy. For instance, a network might initially learn to detect edges and shapes. As training progresses, it learns to identify more complex features such as curves, textures, and ultimately, the entire object itself. This continual process of feature extraction and refinement enables Deep Learning models to achieve remarkable accuracy.\nConvolutional Neural Networks (CNN) have become a cornerstone of image recognition as these neural networks are particularly well-suited for processing grid-like data, such as images. CNNs use convolutional layers that extract local features from the input image (edges, corners, etc.), which are then combined to form more complex representations in subsequent layers, allowing the network to learn hierarchical representations of the image content.\n Image retrieved from https://www.pinecone.io/learn/series/image-search/cnn/#What-Makes-a-CNN.\n\n\n\nImage processing - Preparation of input images to enhance them, e.g., noise reduction, sharpening, filtering, extraction of colour, etc.\nObject detection - Identifying specific objects within an image or video (e.g. R-CNN, SSD).\nScene understanding - Analysing the overall context of an image.\nObject classification - Assigning specific objects to a category or class (e.g. AlexNet, VGG).\nImage segmentation - Dividing an image into distinct regions or objects (e.g. U-Net, Mask R-CNN).\nMotion analysis - Tracking the movement of objects within a video (e.g. optical flow).\n\n\n\n\n\nBiases in training data - For example, facial recognition systems trained predominantly on lighter-skinned people struggle to accurately recognise darker-skinned people.\nPrivacy and ethics - The usage of visual and biometric data raises questions regarding privacy and ethics, especially when dealing with black-box models.\nGeneralisation - Models struggle to generalise objects or environments they were not trained on.\nOcclusions - Often objects in an image are partially blocked by other objects, making it difficult for algorithms to correctly identify them. Advanced models, like Mask R-CNN, can help with missing details, but it requires large amounts of training data with occlusion examples.\nFine-graining - Distinguishing between classes or very similar categories is difficult. For instance, it’s relatively easy to differentiate between a cat and a dog, but it is challenging to distinguish dog breeds.\nLighting - Changes in lighting conditions can alter the appearance of objects in an image.\nResource needs - Training and running Deep Learning models require a lot of computational resources.\n\n\n\n\nComputer Vision can be a useful tool in addressing the challenges posed by vast and diverse collections, enabling libraries to improve services and access to knowledge at scale.\n\nImage Classification and Cataloguing - Libraries manage large digital libraries containing very different material. Computer Vision and automated cataloging automate the classification of the collections, thus reducing the manual labor required and improving discoverability.\nObject Detection and Description - Models can automatically detect features and items in the material, this is useful for creating detailed metadata in large collections. Libraries can also use object detection to improve accessibility, by generating descriptive metadata for images and videos.\nAutomatic Text Recognition - Optical Character Recognition (OCR) and Handwritten Text Recognition (HTR) helps libraries to make printed texts and handwritten documents digital. Thus, libraries use OCR/HTR to create searchable digital collections of books, manuscripts, newspapers, etc. OCR/HTR also supports accessibility, enabling text-to-speech conversions for visually impaired users.\nVisual Search and Content-Based Retrieval - Computer Vision improves search functionality in digital libraries through visual search. This allows users to upload an image and retrieve visually similar items from the collection. Content-based image retrieval also supports researchers by enabling them to find works that are similar.",
    "crumbs": [
      "Topic Guides",
      "Computer Vision"
    ]
  },
  {
    "objectID": "computer-vision.html#introduction",
    "href": "computer-vision.html#introduction",
    "title": "Computer Vision",
    "section": "",
    "text": "Computer Vision is a field of Artificial Intelligence (AI) that uses Deep Learning models to teach machines to “see” and interpret images and videos. At its core, Computer Vision involves functions like processing, detection, and analysis of visual data. It uses algorithms to extract meaningful information from images and enables machines to identify objects, recognise patterns, and make decisions based on what they see. Computer Vision powers many of today’s technologies—from facial recognition and object detection to autonomous vehicles, augmented reality, and Video Assistant Referees (VAR) in football. It’s also integral to tools used in cultural heritage and information management, helping libraries automatically tag, classify, and organize vast visual collections such as photographs, manuscripts, and artworks.\nDeep Learning uses neural networks to model complex patterns within data. These networks, much like the human brain, consist of interconnected layers of artificial neurons. During training, these models process massive datasets of images, enabling them to progressively identify features and refine their predictions with increasing accuracy. For instance, a network might initially learn to detect edges and shapes. As training progresses, it learns to identify more complex features such as curves, textures, and ultimately, the entire object itself. This continual process of feature extraction and refinement enables Deep Learning models to achieve remarkable accuracy.\nConvolutional Neural Networks (CNN) have become a cornerstone of image recognition as these neural networks are particularly well-suited for processing grid-like data, such as images. CNNs use convolutional layers that extract local features from the input image (edges, corners, etc.), which are then combined to form more complex representations in subsequent layers, allowing the network to learn hierarchical representations of the image content.\n Image retrieved from https://www.pinecone.io/learn/series/image-search/cnn/#What-Makes-a-CNN.\n\n\n\nImage processing - Preparation of input images to enhance them, e.g., noise reduction, sharpening, filtering, extraction of colour, etc.\nObject detection - Identifying specific objects within an image or video (e.g. R-CNN, SSD).\nScene understanding - Analysing the overall context of an image.\nObject classification - Assigning specific objects to a category or class (e.g. AlexNet, VGG).\nImage segmentation - Dividing an image into distinct regions or objects (e.g. U-Net, Mask R-CNN).\nMotion analysis - Tracking the movement of objects within a video (e.g. optical flow).\n\n\n\n\n\nBiases in training data - For example, facial recognition systems trained predominantly on lighter-skinned people struggle to accurately recognise darker-skinned people.\nPrivacy and ethics - The usage of visual and biometric data raises questions regarding privacy and ethics, especially when dealing with black-box models.\nGeneralisation - Models struggle to generalise objects or environments they were not trained on.\nOcclusions - Often objects in an image are partially blocked by other objects, making it difficult for algorithms to correctly identify them. Advanced models, like Mask R-CNN, can help with missing details, but it requires large amounts of training data with occlusion examples.\nFine-graining - Distinguishing between classes or very similar categories is difficult. For instance, it’s relatively easy to differentiate between a cat and a dog, but it is challenging to distinguish dog breeds.\nLighting - Changes in lighting conditions can alter the appearance of objects in an image.\nResource needs - Training and running Deep Learning models require a lot of computational resources.\n\n\n\n\nComputer Vision can be a useful tool in addressing the challenges posed by vast and diverse collections, enabling libraries to improve services and access to knowledge at scale.\n\nImage Classification and Cataloguing - Libraries manage large digital libraries containing very different material. Computer Vision and automated cataloging automate the classification of the collections, thus reducing the manual labor required and improving discoverability.\nObject Detection and Description - Models can automatically detect features and items in the material, this is useful for creating detailed metadata in large collections. Libraries can also use object detection to improve accessibility, by generating descriptive metadata for images and videos.\nAutomatic Text Recognition - Optical Character Recognition (OCR) and Handwritten Text Recognition (HTR) helps libraries to make printed texts and handwritten documents digital. Thus, libraries use OCR/HTR to create searchable digital collections of books, manuscripts, newspapers, etc. OCR/HTR also supports accessibility, enabling text-to-speech conversions for visually impaired users.\nVisual Search and Content-Based Retrieval - Computer Vision improves search functionality in digital libraries through visual search. This allows users to upload an image and retrieve visually similar items from the collection. Content-based image retrieval also supports researchers by enabling them to find works that are similar.",
    "crumbs": [
      "Topic Guides",
      "Computer Vision"
    ]
  },
  {
    "objectID": "computer-vision.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "computer-vision.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Computer Vision",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nHaving explored the foundational concepts of Computer Vision, let’s now delve into how it is being applied in the library sector. We will take a look at some key applications within areas like collection enrichment, access and delivery, and accessibility.\n\nEnrichment of collections\nComputer Vision is transforming the enrichment of library collections by automating the classification and annotation of visual materials. With the help of object detection and image recognition, it identifies objects, patterns, and relationships within images, enabling the creation of richer metadata and contextual information. This not only improves discoverability, allowing users to locate materials more easily, but also allows for deeper insights into the historical, artistic, or cultural significance of the materials.\nA case study conducted by the University of Calgary’s Libraries and Cultural Resources explored the use of Sheeko, a metadata generation software, to create descriptive metadata from images using pre-trained models. The study concluded that, with moderate human intervention, both the descriptions and titles generated by the pre-trained models were usable. Similarly, Saint George on a Bike which was a project led by Europeana, aimed to improve the classification and metadata of cultural heritage objects, by using CNNs for object detection to identify and create semantic context for the images. These examples demonstrate the potential of combining automated AI-powered tools with human expertise to improve metadata creation, while enriching cultural heritage collections and making them more accessible and meaningful for diverse audiences.\n\n\n\nAccess and delivery\nSimilarly, Computer Vision is improving access, delivery, and research within library collections by enabling more intuitive and sophisticated ways to explore and analyse visual data. Advances in image recognition and retrieval allow users to discover connections, find similar materials, and engage with collections in entirely new ways, improving both the efficiency and depth of exploration.\nFor instance, imgs.ai is a text-based visual search engine designed for digital collections. It uses approximate k-NN algorithms and integrates the OpenAI CLIP model to link textual queries with visually similar content, offering new ways to navigate collections. Similarly, MAKEN, a service developed by the National Library of Norway, uses object detection to identify and retrieve similar images from the library’s digital collection. Thus providing users with yet another possibility of engaging with visual material. These examples show how Computer Vision is making it easier to access digital collections, helping users to find connections, and discover materials quickly and effortlessly.\n PixPlot is a project by the digital humanities lab of Yale University Library, it uses CNNs to cluster similar images.\n\n\nAccessibility\nComputer Vision significantly improves accessibility in libraries by working alongside technologies like OCR and HTR. OCR/HTR convert printed and handwritten texts into machine-readable formats, enabling text-to-speech applications for users with visual impairments. Read more about OCR and HTR from Automatic Text Recognition (OCR/HTR) topic guide.\nBeyond text, Computer Vision enables creation of descriptive metadata for visual materials, such as photographs, maps, or videos, thus providing richer contextual information that benefits users with cognitive or visual disabilities. For instance, a visually impaired user might hear a description of a painting that includes details about the objects and people depicted. This richer contextual information ensures that libraries remain inclusive spaces where all users can access and engage with collections in a meaningful way.\n\n\nSemantic segmentation of maps\nSemantic segmentation in an exciting frontier where Computer Vision can be of use for identifying and categorising distinct elements within maps, such as roads, buildings, water bodies, and land use regions. By using CNNs and U-Net architectures, models partition maps into meaningful segments, allowing for detailed analysis and improved usability. This supports historical research, geographic analysis, and so on, by extracting structured data from cartographic sources.\nFor example, JADIS, developed by the National Library of France, it is a tool designed for semantically segmented, georeferenced, and geocoded maps of Paris.\n\nSimilarly, MapReader, an outcome of the Living with the Machines project, offers a Computer Vision pipeline for analysing large collections of historical maps.",
    "crumbs": [
      "Topic Guides",
      "Computer Vision"
    ]
  },
  {
    "objectID": "computer-vision.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "computer-vision.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Computer Vision",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nFor those new to the field, Google Colab Notebooks offer a good starting point with plenty of notebooks on Computer Vision, one great example being this collection.\nThe TensorFlow Playground allows users to experiment with neural network architectures in an interactive way, offering insight into the understanding of how those work.\nGLAM Workbench is a hands-on guide that focuses on applying Computer Vision to GLAM collections, mainly from New Zealand and Australia. It explores how digitised images can be analysed to identify patterns and improve the accessibility of digital heritage.\nIf a more humanities oriented point of view is desired, then Computer Vision related tutorials can also be found at the Programming Historian.\nThe Social Sciences & Humanities Open Marketplace has quite a lot of useful links to free Computer Vision tutorials and training materials worth a look at (search: computer vision).\nFor practical coding experience, versatile tutorials for using open-source tools can be found at OpenCV and RealPython and PyTorch are handy.\nTop universities, for example Harvard, Stanford and Helsinki, offer, sometimes free, online courses as do sites like Coursera, FutureLearn, and Udemy.",
    "crumbs": [
      "Topic Guides",
      "Computer Vision"
    ]
  },
  {
    "objectID": "computer-vision.html#recommended-readingviewing",
    "href": "computer-vision.html#recommended-readingviewing",
    "title": "Computer Vision",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nFor those looking to get a deeper understanding of Computer Vision, some key resources include:\n\nArnold, T., Tilton, L. (2023). Distant viewing: Computational Exploration of Digital Images. The MIT Press. https://doi.org/10.7551/mitpress/14046.001.0001\nKrizhevsky, A., Sutskever, I., Hinton, G. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems 25 (NIPS 2012). Retrieved from https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\nSzeliski, T. (2022). Computer Vision: Algorithms and Applications. Springer. https://doi.org/10.1007/978-3-030-34372-9\nStanford University’s course CS231n Convolutional Neural Networks for Visual Recognition 2016 and 2017 lectures are up on YouTube.",
    "crumbs": [
      "Topic Guides",
      "Computer Vision"
    ]
  },
  {
    "objectID": "computer-vision.html#finding-communities-of-practice",
    "href": "computer-vision.html#finding-communities-of-practice",
    "title": "Computer Vision",
    "section": "Finding Communities of Practice",
    "text": "Finding Communities of Practice\nThere are many international communities online where one can join and start to learn more about applications of Computer Vision in libraries and seek support for your own aspirations in this area. AI4LAM, CENL network group ‘AI in libraries’, IFLA Artificial Intelligence Special Interest Group and the IIIF AI/ML community group are great places to find and meet others working with Computer Vision in libraries. There are also conferences and events like Fantastic Futures which offer opportunities to learn from and meet like-minded people. The Slack workspaces of AI4LAM or IIIF are open to all and really helpful spaces for asking questions of the community.",
    "crumbs": [
      "Topic Guides",
      "Computer Vision"
    ]
  }
]